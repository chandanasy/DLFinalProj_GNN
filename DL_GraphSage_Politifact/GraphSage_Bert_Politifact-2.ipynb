{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcea1c0",
   "metadata": {},
   "source": [
    "# Model-1 \n",
    "GraphSage + Bert embeddings using default Parameters and 3 Layer MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69343fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
      "Requirement already satisfied: torch-scatter in ./opt/anaconda3/lib/python3.9/site-packages (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
      "Requirement already satisfied: torch-sparse in ./opt/anaconda3/lib/python3.9/site-packages (0.6.17)\n",
      "Requirement already satisfied: scipy in ./opt/anaconda3/lib/python3.9/site-packages (from torch-sparse) (1.9.1)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.18.5 in ./opt/anaconda3/lib/python3.9/site-packages (from scipy->torch-sparse) (1.21.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch-geometric in ./opt/anaconda3/lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in ./opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.21.5)\n",
      "Requirement already satisfied: psutil>=5.8.0 in ./opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (5.9.0)\n",
      "Requirement already satisfied: requests in ./opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (2.28.1)\n",
      "Requirement already satisfied: tqdm in ./opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (4.64.1)\n",
      "Requirement already satisfied: scikit-learn in ./opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.0.2)\n",
      "Requirement already satisfied: pyparsing in ./opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: scipy in ./opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.9.1)\n",
      "Requirement already satisfied: jinja2 in ./opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch-geometric) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (1.26.11)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in ./opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b5118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import UPFD #importing the UPFD Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b98e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"bert\",split=\"test\")\n",
    "train_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"bert\", split=\"train\")\n",
    "val_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"bert\", split=\"val\")\n",
    "train_data_pol = train_data_pol + val_data_pol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19ee5279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politifact Dataset\n",
      "Train Samples:  93\n",
      "Test Samples:  221\n"
     ]
    }
   ],
   "source": [
    "print(\"Politifact Dataset\")\n",
    "print(\"Train Samples: \", len(train_data_pol))\n",
    "print(\"Test Samples: \", len(test_data_pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79be24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  8,  8,  8, 16, 16, 16, 16, 16, 16,\n",
       "         24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
       "         24, 24, 24, 24, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 60],\n",
       "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "         55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pol[0].edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8588f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "train_loader = DataLoader(train_data_pol, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_data_pol, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a12c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LeakyReLU, Softmax, Linear, SELU,Dropout\n",
    "from torch_geometric.nn import SAGEConv, global_max_pool, GATv2Conv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a285b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model architecture \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
    "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
    "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
    "        \n",
    "        self.full1 = Linear(hidden_channels[2],hidden_channels[3])\n",
    "        self.full2 = Linear(hidden_channels[3],hidden_channels[4])\n",
    "        self.full3 = Linear(hidden_channels[4],hidden_channels[5])\n",
    "\n",
    "        self.softmax = Linear(hidden_channels[5],out_channels)\n",
    "\n",
    "        #droupouts\n",
    "        self.dp1 = Dropout(0.2)\n",
    "        self.dp2 = Dropout(0.2)\n",
    "        self.dp3 = Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = self.conv1(x, edge_index).relu()\n",
    "        h = self.conv2(h, edge_index).relu()\n",
    "        h = self.conv3(h, edge_index).relu()\n",
    "\n",
    "        h = global_max_pool(h,batch)\n",
    "\n",
    "        h = self.full1(h).relu()\n",
    "        h = self.dp1(h)\n",
    "        h = self.full2(h).relu()\n",
    "        h = self.dp2(h)\n",
    "        h = self.full3(h).relu()\n",
    "        h = self.dp3(h)\n",
    "        \n",
    "        h = self.softmax(h)\n",
    "\n",
    "        return torch.sigmoid(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0aa1e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score, f1_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23de7dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(test_data_pol.num_features,[512,512,512,256,256,256],1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "lossff = torch.nn.BCELoss()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10eabbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining test and training functions\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        all_preds.append(torch.reshape(out, (-1,)))\n",
    "        all_labels.append(data.y.float())\n",
    "    # print(all_preds)\n",
    "    accuracy, f1 = metrics(all_preds, all_labels)\n",
    "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
    "\n",
    "\n",
    "def metrics(preds, gts):\n",
    "    preds = torch.round(torch.cat(preds))\n",
    "    gts = torch.cat(gts)\n",
    "    # print(preds.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cc640c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 |  TrainLoss: 0.69264 | TestLoss: 0.69346 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 01 |  TrainLoss: 0.69194 | TestLoss: 0.69348 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 02 |  TrainLoss: 0.69249 | TestLoss: 0.69359 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 03 |  TrainLoss: 0.69183 | TestLoss: 0.69353 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 04 |  TrainLoss: 0.69163 | TestLoss: 0.69334 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 05 |  TrainLoss: 0.69025 | TestLoss: 0.69319 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 06 |  TrainLoss: 0.69117 | TestLoss: 0.69297 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 07 |  TrainLoss: 0.68969 | TestLoss: 0.69280 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 08 |  TrainLoss: 0.68937 | TestLoss: 0.69254 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 09 |  TrainLoss: 0.68865 | TestLoss: 0.69224 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 10 |  TrainLoss: 0.69090 | TestLoss: 0.69175 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 11 |  TrainLoss: 0.68817 | TestLoss: 0.69111 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 12 |  TrainLoss: 0.68571 | TestLoss: 0.69037 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 13 |  TrainLoss: 0.68557 | TestLoss: 0.68956 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 14 |  TrainLoss: 0.68560 | TestLoss: 0.68870 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 15 |  TrainLoss: 0.68217 | TestLoss: 0.68780 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 16 |  TrainLoss: 0.68158 | TestLoss: 0.68682 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 17 |  TrainLoss: 0.67949 | TestLoss: 0.68571 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 18 |  TrainLoss: 0.67824 | TestLoss: 0.68416 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 19 |  TrainLoss: 0.67737 | TestLoss: 0.68222 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 20 |  TrainLoss: 0.67382 | TestLoss: 0.67998 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 21 |  TrainLoss: 0.67498 | TestLoss: 0.67748 | TestAcc: 0.49774 | TestF1: 0.03\n",
      "Epoch: 22 |  TrainLoss: 0.66835 | TestLoss: 0.67471 | TestAcc: 0.51584 | TestF1: 0.10\n",
      "Epoch: 23 |  TrainLoss: 0.66508 | TestLoss: 0.67158 | TestAcc: 0.56109 | TestF1: 0.25\n",
      "Epoch: 24 |  TrainLoss: 0.65863 | TestLoss: 0.66798 | TestAcc: 0.58824 | TestF1: 0.34\n",
      "Epoch: 25 |  TrainLoss: 0.65465 | TestLoss: 0.66412 | TestAcc: 0.59276 | TestF1: 0.35\n",
      "Epoch: 26 |  TrainLoss: 0.65042 | TestLoss: 0.65974 | TestAcc: 0.61538 | TestF1: 0.41\n",
      "Epoch: 27 |  TrainLoss: 0.64375 | TestLoss: 0.65496 | TestAcc: 0.63801 | TestF1: 0.46\n",
      "Epoch: 28 |  TrainLoss: 0.63615 | TestLoss: 0.64960 | TestAcc: 0.64706 | TestF1: 0.48\n",
      "Epoch: 29 |  TrainLoss: 0.62863 | TestLoss: 0.64402 | TestAcc: 0.64706 | TestF1: 0.48\n",
      "Epoch: 30 |  TrainLoss: 0.62401 | TestLoss: 0.63693 | TestAcc: 0.67421 | TestF1: 0.55\n",
      "Epoch: 31 |  TrainLoss: 0.60884 | TestLoss: 0.62966 | TestAcc: 0.66968 | TestF1: 0.56\n",
      "Epoch: 32 |  TrainLoss: 0.59694 | TestLoss: 0.62324 | TestAcc: 0.66968 | TestF1: 0.54\n",
      "Epoch: 33 |  TrainLoss: 0.58314 | TestLoss: 0.61460 | TestAcc: 0.67421 | TestF1: 0.57\n",
      "Epoch: 34 |  TrainLoss: 0.57730 | TestLoss: 0.60318 | TestAcc: 0.71041 | TestF1: 0.64\n",
      "Epoch: 35 |  TrainLoss: 0.55796 | TestLoss: 0.59317 | TestAcc: 0.72398 | TestF1: 0.66\n",
      "Epoch: 36 |  TrainLoss: 0.54001 | TestLoss: 0.58631 | TestAcc: 0.70136 | TestF1: 0.62\n",
      "Epoch: 37 |  TrainLoss: 0.52338 | TestLoss: 0.57182 | TestAcc: 0.72851 | TestF1: 0.67\n",
      "Epoch: 38 |  TrainLoss: 0.50586 | TestLoss: 0.55836 | TestAcc: 0.75113 | TestF1: 0.71\n",
      "Epoch: 39 |  TrainLoss: 0.48231 | TestLoss: 0.54804 | TestAcc: 0.75566 | TestF1: 0.71\n",
      "Epoch: 40 |  TrainLoss: 0.46269 | TestLoss: 0.53307 | TestAcc: 0.77376 | TestF1: 0.74\n",
      "Epoch: 41 |  TrainLoss: 0.43836 | TestLoss: 0.51820 | TestAcc: 0.78733 | TestF1: 0.76\n",
      "Epoch: 42 |  TrainLoss: 0.40277 | TestLoss: 0.51053 | TestAcc: 0.77828 | TestF1: 0.75\n",
      "Epoch: 43 |  TrainLoss: 0.37647 | TestLoss: 0.49606 | TestAcc: 0.79638 | TestF1: 0.78\n",
      "Epoch: 44 |  TrainLoss: 0.35805 | TestLoss: 0.47420 | TestAcc: 0.80995 | TestF1: 0.79\n",
      "Epoch: 45 |  TrainLoss: 0.33496 | TestLoss: 0.47060 | TestAcc: 0.80543 | TestF1: 0.79\n",
      "Epoch: 46 |  TrainLoss: 0.30297 | TestLoss: 0.44970 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 47 |  TrainLoss: 0.27877 | TestLoss: 0.44038 | TestAcc: 0.81900 | TestF1: 0.81\n",
      "Epoch: 48 |  TrainLoss: 0.24451 | TestLoss: 0.43814 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 49 |  TrainLoss: 0.21290 | TestLoss: 0.42832 | TestAcc: 0.83258 | TestF1: 0.83\n",
      "Epoch: 50 |  TrainLoss: 0.19863 | TestLoss: 0.44080 | TestAcc: 0.81900 | TestF1: 0.81\n",
      "Epoch: 51 |  TrainLoss: 0.15375 | TestLoss: 0.43763 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 52 |  TrainLoss: 0.14361 | TestLoss: 0.41303 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 53 |  TrainLoss: 0.12748 | TestLoss: 0.46620 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 54 |  TrainLoss: 0.09516 | TestLoss: 0.47545 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 55 |  TrainLoss: 0.08295 | TestLoss: 0.46102 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 56 |  TrainLoss: 0.06709 | TestLoss: 0.46482 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 57 |  TrainLoss: 0.05313 | TestLoss: 0.54687 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 58 |  TrainLoss: 0.04656 | TestLoss: 0.55802 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 59 |  TrainLoss: 0.03888 | TestLoss: 0.52775 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 60 |  TrainLoss: 0.03187 | TestLoss: 0.56779 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 61 |  TrainLoss: 0.01843 | TestLoss: 0.63896 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 62 |  TrainLoss: 0.02937 | TestLoss: 0.59960 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 63 |  TrainLoss: 0.01533 | TestLoss: 0.61404 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 64 |  TrainLoss: 0.01779 | TestLoss: 0.67664 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 65 |  TrainLoss: 0.00818 | TestLoss: 0.73718 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 66 |  TrainLoss: 0.00999 | TestLoss: 0.75831 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 67 |  TrainLoss: 0.00825 | TestLoss: 0.73432 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 68 |  TrainLoss: 0.00572 | TestLoss: 0.74713 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 69 |  TrainLoss: 0.00506 | TestLoss: 0.78104 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 70 |  TrainLoss: 0.00248 | TestLoss: 0.82205 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 71 |  TrainLoss: 0.00280 | TestLoss: 0.84583 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 72 |  TrainLoss: 0.00247 | TestLoss: 0.86280 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 73 |  TrainLoss: 0.00200 | TestLoss: 0.87538 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 74 |  TrainLoss: 0.00156 | TestLoss: 0.88702 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 75 |  TrainLoss: 0.00133 | TestLoss: 0.89370 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 76 |  TrainLoss: 0.00202 | TestLoss: 0.89255 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 77 |  TrainLoss: 0.00073 | TestLoss: 0.89350 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 78 |  TrainLoss: 0.00127 | TestLoss: 0.89937 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 79 |  TrainLoss: 0.00071 | TestLoss: 0.90820 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 80 |  TrainLoss: 0.00068 | TestLoss: 0.91870 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 81 |  TrainLoss: 0.00232 | TestLoss: 0.95843 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 82 |  TrainLoss: 0.00077 | TestLoss: 1.00602 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 83 |  TrainLoss: 0.00100 | TestLoss: 1.04505 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 84 |  TrainLoss: 0.00062 | TestLoss: 1.07739 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 85 |  TrainLoss: 0.00045 | TestLoss: 1.10248 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 86 |  TrainLoss: 0.00069 | TestLoss: 1.10937 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 87 |  TrainLoss: 0.00085 | TestLoss: 1.09770 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 88 |  TrainLoss: 0.00046 | TestLoss: 1.08361 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 89 |  TrainLoss: 0.00035 | TestLoss: 1.06855 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 90 |  TrainLoss: 0.00044 | TestLoss: 1.06040 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 91 |  TrainLoss: 0.00033 | TestLoss: 1.05419 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 92 |  TrainLoss: 0.00051 | TestLoss: 1.05600 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 93 |  TrainLoss: 0.00021 | TestLoss: 1.05677 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 94 |  TrainLoss: 0.00039 | TestLoss: 1.05991 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 95 |  TrainLoss: 0.00030 | TestLoss: 1.06406 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 96 |  TrainLoss: 0.00156 | TestLoss: 1.06152 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 97 |  TrainLoss: 0.00035 | TestLoss: 1.06215 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 98 |  TrainLoss: 0.00025 | TestLoss: 1.06457 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 99 |  TrainLoss: 0.00025 | TestLoss: 1.06480 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 100 |  TrainLoss: 0.00027 | TestLoss: 1.06597 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 101 |  TrainLoss: 0.00037 | TestLoss: 1.06915 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 102 |  TrainLoss: 0.00023 | TestLoss: 1.07270 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 103 |  TrainLoss: 0.00018 | TestLoss: 1.07509 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 104 |  TrainLoss: 0.00026 | TestLoss: 1.07582 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 105 |  TrainLoss: 0.00033 | TestLoss: 1.08131 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 106 |  TrainLoss: 0.00045 | TestLoss: 1.09298 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 107 |  TrainLoss: 0.00030 | TestLoss: 1.10530 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 108 |  TrainLoss: 0.00015 | TestLoss: 1.11753 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 109 |  TrainLoss: 0.00034 | TestLoss: 1.12426 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 110 |  TrainLoss: 0.00056 | TestLoss: 1.11899 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 111 |  TrainLoss: 0.00024 | TestLoss: 1.11555 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 112 |  TrainLoss: 0.00026 | TestLoss: 1.10879 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 113 |  TrainLoss: 0.00042 | TestLoss: 1.09879 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 114 |  TrainLoss: 0.00021 | TestLoss: 1.09057 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 115 |  TrainLoss: 0.00032 | TestLoss: 1.08615 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 116 |  TrainLoss: 0.00022 | TestLoss: 1.08239 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 117 |  TrainLoss: 0.00012 | TestLoss: 1.07931 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 118 |  TrainLoss: 0.00061 | TestLoss: 1.08911 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 119 |  TrainLoss: 0.00015 | TestLoss: 1.09897 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 120 |  TrainLoss: 0.00059 | TestLoss: 1.10033 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 121 |  TrainLoss: 0.00029 | TestLoss: 1.10342 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 122 |  TrainLoss: 0.00015 | TestLoss: 1.10645 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 123 |  TrainLoss: 0.00022 | TestLoss: 1.10999 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 124 |  TrainLoss: 0.00021 | TestLoss: 1.11391 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 125 |  TrainLoss: 0.00016 | TestLoss: 1.11651 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 126 |  TrainLoss: 0.00014 | TestLoss: 1.11859 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 127 |  TrainLoss: 0.00020 | TestLoss: 1.11880 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 128 |  TrainLoss: 0.00017 | TestLoss: 1.11961 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 129 |  TrainLoss: 0.00023 | TestLoss: 1.11973 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 130 |  TrainLoss: 0.00017 | TestLoss: 1.12186 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 131 |  TrainLoss: 0.00012 | TestLoss: 1.12369 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 132 |  TrainLoss: 0.00016 | TestLoss: 1.12689 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 133 |  TrainLoss: 0.00014 | TestLoss: 1.13074 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 134 |  TrainLoss: 0.00012 | TestLoss: 1.13243 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 135 |  TrainLoss: 0.00025 | TestLoss: 1.13072 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 136 |  TrainLoss: 0.00017 | TestLoss: 1.12750 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 137 |  TrainLoss: 0.00010 | TestLoss: 1.12414 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 138 |  TrainLoss: 0.00012 | TestLoss: 1.12122 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 139 |  TrainLoss: 0.00009 | TestLoss: 1.11827 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 140 |  TrainLoss: 0.00014 | TestLoss: 1.11665 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 141 |  TrainLoss: 0.00012 | TestLoss: 1.11513 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 142 |  TrainLoss: 0.00016 | TestLoss: 1.11572 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 143 |  TrainLoss: 0.00014 | TestLoss: 1.11793 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 144 |  TrainLoss: 0.00010 | TestLoss: 1.12045 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 145 |  TrainLoss: 0.00017 | TestLoss: 1.12130 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 146 |  TrainLoss: 0.00013 | TestLoss: 1.12246 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 147 |  TrainLoss: 0.00021 | TestLoss: 1.12670 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 148 |  TrainLoss: 0.00014 | TestLoss: 1.13073 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 149 |  TrainLoss: 0.00012 | TestLoss: 1.13487 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 150 |  TrainLoss: 0.00084 | TestLoss: 1.16552 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 151 |  TrainLoss: 0.00010 | TestLoss: 1.19565 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 152 |  TrainLoss: 0.00007 | TestLoss: 1.22427 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 153 |  TrainLoss: 0.00015 | TestLoss: 1.25110 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 154 |  TrainLoss: 0.00013 | TestLoss: 1.27369 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 155 |  TrainLoss: 0.00020 | TestLoss: 1.28843 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 156 |  TrainLoss: 0.00023 | TestLoss: 1.29381 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 157 |  TrainLoss: 0.00040 | TestLoss: 1.28386 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 158 |  TrainLoss: 0.00038 | TestLoss: 1.26114 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 159 |  TrainLoss: 0.00027 | TestLoss: 1.23274 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 160 |  TrainLoss: 0.00036 | TestLoss: 1.19541 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 161 |  TrainLoss: 0.00011 | TestLoss: 1.16497 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 162 |  TrainLoss: 0.00018 | TestLoss: 1.13859 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 163 |  TrainLoss: 0.00011 | TestLoss: 1.12034 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 164 |  TrainLoss: 0.00018 | TestLoss: 1.10980 | TestAcc: 0.83710 | TestF1: 0.84\n",
      "Epoch: 165 |  TrainLoss: 0.00026 | TestLoss: 1.10637 | TestAcc: 0.83710 | TestF1: 0.84\n",
      "Epoch: 166 |  TrainLoss: 0.00012 | TestLoss: 1.10492 | TestAcc: 0.83258 | TestF1: 0.84\n",
      "Epoch: 167 |  TrainLoss: 0.00014 | TestLoss: 1.10582 | TestAcc: 0.83258 | TestF1: 0.84\n",
      "Epoch: 168 |  TrainLoss: 0.00010 | TestLoss: 1.10744 | TestAcc: 0.83258 | TestF1: 0.84\n",
      "Epoch: 169 |  TrainLoss: 0.00021 | TestLoss: 1.11270 | TestAcc: 0.83710 | TestF1: 0.84\n",
      "Epoch: 170 |  TrainLoss: 0.00024 | TestLoss: 1.11999 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 171 |  TrainLoss: 0.00017 | TestLoss: 1.12934 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 172 |  TrainLoss: 0.00024 | TestLoss: 1.14215 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 173 |  TrainLoss: 0.00013 | TestLoss: 1.15653 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 174 |  TrainLoss: 0.00010 | TestLoss: 1.17145 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 175 |  TrainLoss: 0.00007 | TestLoss: 1.18546 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 176 |  TrainLoss: 0.00006 | TestLoss: 1.19920 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 177 |  TrainLoss: 0.00019 | TestLoss: 1.21455 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 178 |  TrainLoss: 0.00013 | TestLoss: 1.22620 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 179 |  TrainLoss: 0.00006 | TestLoss: 1.23679 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 180 |  TrainLoss: 0.00018 | TestLoss: 1.24262 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 181 |  TrainLoss: 0.00017 | TestLoss: 1.24482 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 182 |  TrainLoss: 0.00021 | TestLoss: 1.24054 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 183 |  TrainLoss: 0.00008 | TestLoss: 1.23545 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 184 |  TrainLoss: 0.00017 | TestLoss: 1.22816 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 185 |  TrainLoss: 0.00009 | TestLoss: 1.22014 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 186 |  TrainLoss: 0.00014 | TestLoss: 1.21550 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 187 |  TrainLoss: 0.00014 | TestLoss: 1.21047 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 188 |  TrainLoss: 0.00010 | TestLoss: 1.20487 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 189 |  TrainLoss: 0.00012 | TestLoss: 1.19935 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 190 |  TrainLoss: 0.00019 | TestLoss: 1.19223 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 191 |  TrainLoss: 0.00006 | TestLoss: 1.18544 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 192 |  TrainLoss: 0.00023 | TestLoss: 1.17955 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 193 |  TrainLoss: 0.00008 | TestLoss: 1.17442 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 194 |  TrainLoss: 0.00011 | TestLoss: 1.17114 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 195 |  TrainLoss: 0.00011 | TestLoss: 1.17071 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 196 |  TrainLoss: 0.00011 | TestLoss: 1.17221 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 197 |  TrainLoss: 0.00013 | TestLoss: 1.17268 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 198 |  TrainLoss: 0.00008 | TestLoss: 1.17419 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 199 |  TrainLoss: 0.00009 | TestLoss: 1.17643 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 200 |  TrainLoss: 0.00008 | TestLoss: 1.17890 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 201 |  TrainLoss: 0.00014 | TestLoss: 1.18088 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 202 |  TrainLoss: 0.00007 | TestLoss: 1.18342 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 203 |  TrainLoss: 0.00009 | TestLoss: 1.18683 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 204 |  TrainLoss: 0.00010 | TestLoss: 1.19144 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 205 |  TrainLoss: 0.00008 | TestLoss: 1.19650 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 206 |  TrainLoss: 0.00006 | TestLoss: 1.20058 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 207 |  TrainLoss: 0.00007 | TestLoss: 1.20392 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 208 |  TrainLoss: 0.00005 | TestLoss: 1.20706 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 209 |  TrainLoss: 0.00007 | TestLoss: 1.21002 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 210 |  TrainLoss: 0.00009 | TestLoss: 1.21165 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 211 |  TrainLoss: 0.00009 | TestLoss: 1.21214 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 212 |  TrainLoss: 0.00157 | TestLoss: 1.29797 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 213 |  TrainLoss: 0.00019 | TestLoss: 1.38593 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 214 |  TrainLoss: 0.00021 | TestLoss: 1.46812 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 215 |  TrainLoss: 0.00051 | TestLoss: 1.51518 | TestAcc: 0.81900 | TestF1: 0.81\n",
      "Epoch: 216 |  TrainLoss: 0.00062 | TestLoss: 1.51351 | TestAcc: 0.81900 | TestF1: 0.81\n",
      "Epoch: 217 |  TrainLoss: 0.00045 | TestLoss: 1.47723 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 218 |  TrainLoss: 0.00061 | TestLoss: 1.40488 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 219 |  TrainLoss: 0.00016 | TestLoss: 1.33814 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 220 |  TrainLoss: 0.00010 | TestLoss: 1.28207 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 221 |  TrainLoss: 0.00012 | TestLoss: 1.23492 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 222 |  TrainLoss: 0.00011 | TestLoss: 1.19827 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 223 |  TrainLoss: 0.00007 | TestLoss: 1.17271 | TestAcc: 0.83710 | TestF1: 0.84\n",
      "Epoch: 224 |  TrainLoss: 0.00012 | TestLoss: 1.15771 | TestAcc: 0.83710 | TestF1: 0.84\n",
      "Epoch: 225 |  TrainLoss: 0.00016 | TestLoss: 1.15123 | TestAcc: 0.84163 | TestF1: 0.85\n",
      "Epoch: 226 |  TrainLoss: 0.00009 | TestLoss: 1.14753 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 227 |  TrainLoss: 0.00012 | TestLoss: 1.14620 | TestAcc: 0.84163 | TestF1: 0.85\n",
      "Epoch: 228 |  TrainLoss: 0.00015 | TestLoss: 1.14653 | TestAcc: 0.84163 | TestF1: 0.85\n",
      "Epoch: 229 |  TrainLoss: 0.00021 | TestLoss: 1.14906 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 230 |  TrainLoss: 0.00017 | TestLoss: 1.15443 | TestAcc: 0.84163 | TestF1: 0.85\n",
      "Epoch: 231 |  TrainLoss: 0.00009 | TestLoss: 1.16193 | TestAcc: 0.83710 | TestF1: 0.84\n",
      "Epoch: 232 |  TrainLoss: 0.00005 | TestLoss: 1.17040 | TestAcc: 0.83258 | TestF1: 0.84\n",
      "Epoch: 233 |  TrainLoss: 0.00006 | TestLoss: 1.18014 | TestAcc: 0.83258 | TestF1: 0.84\n",
      "Epoch: 234 |  TrainLoss: 0.00008 | TestLoss: 1.19126 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 235 |  TrainLoss: 0.00009 | TestLoss: 1.20407 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 236 |  TrainLoss: 0.00008 | TestLoss: 1.21752 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 237 |  TrainLoss: 0.00008 | TestLoss: 1.22939 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 238 |  TrainLoss: 0.00012 | TestLoss: 1.24332 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 239 |  TrainLoss: 0.00007 | TestLoss: 1.25527 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 240 |  TrainLoss: 0.00005 | TestLoss: 1.26601 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 241 |  TrainLoss: 0.00003 | TestLoss: 1.27580 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 242 |  TrainLoss: 0.00006 | TestLoss: 1.28449 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 243 |  TrainLoss: 0.00006 | TestLoss: 1.29225 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 244 |  TrainLoss: 0.00009 | TestLoss: 1.29735 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 245 |  TrainLoss: 0.00014 | TestLoss: 1.29705 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 246 |  TrainLoss: 0.00005 | TestLoss: 1.29626 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 247 |  TrainLoss: 0.00005 | TestLoss: 1.29493 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 248 |  TrainLoss: 0.00007 | TestLoss: 1.29173 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 249 |  TrainLoss: 0.00004 | TestLoss: 1.28844 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 250 |  TrainLoss: 0.00010 | TestLoss: 1.28267 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 251 |  TrainLoss: 0.00006 | TestLoss: 1.27586 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 252 |  TrainLoss: 0.00005 | TestLoss: 1.26916 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 253 |  TrainLoss: 0.00004 | TestLoss: 1.26333 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 254 |  TrainLoss: 0.00005 | TestLoss: 1.25808 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 255 |  TrainLoss: 0.00006 | TestLoss: 1.25274 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 256 |  TrainLoss: 0.00005 | TestLoss: 1.24760 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 257 |  TrainLoss: 0.00004 | TestLoss: 1.24280 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 258 |  TrainLoss: 0.00006 | TestLoss: 1.24020 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 259 |  TrainLoss: 0.00005 | TestLoss: 1.23790 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 260 |  TrainLoss: 0.00003 | TestLoss: 1.23591 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 261 |  TrainLoss: 0.00005 | TestLoss: 1.23372 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 262 |  TrainLoss: 0.00005 | TestLoss: 1.23222 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 263 |  TrainLoss: 0.00005 | TestLoss: 1.23155 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 264 |  TrainLoss: 0.00008 | TestLoss: 1.23258 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 265 |  TrainLoss: 0.00005 | TestLoss: 1.23283 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 266 |  TrainLoss: 0.00008 | TestLoss: 1.23429 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 267 |  TrainLoss: 0.00013 | TestLoss: 1.23994 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 268 |  TrainLoss: 0.00003 | TestLoss: 1.24521 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 269 |  TrainLoss: 0.00006 | TestLoss: 1.25019 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 270 |  TrainLoss: 0.00006 | TestLoss: 1.25389 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 271 |  TrainLoss: 0.00003 | TestLoss: 1.25717 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 272 |  TrainLoss: 0.00004 | TestLoss: 1.25968 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 273 |  TrainLoss: 0.00004 | TestLoss: 1.26160 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 274 |  TrainLoss: 0.00008 | TestLoss: 1.26512 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 275 |  TrainLoss: 0.00005 | TestLoss: 1.26752 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 276 |  TrainLoss: 0.00006 | TestLoss: 1.26970 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 277 |  TrainLoss: 0.00008 | TestLoss: 1.26966 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 278 |  TrainLoss: 0.00007 | TestLoss: 1.27165 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 279 |  TrainLoss: 0.00007 | TestLoss: 1.27217 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 280 |  TrainLoss: 0.00006 | TestLoss: 1.27153 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 281 |  TrainLoss: 0.00004 | TestLoss: 1.27058 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 282 |  TrainLoss: 0.00003 | TestLoss: 1.26947 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 283 |  TrainLoss: 0.00007 | TestLoss: 1.26870 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 284 |  TrainLoss: 0.00003 | TestLoss: 1.26769 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 285 |  TrainLoss: 0.00003 | TestLoss: 1.26662 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 286 |  TrainLoss: 0.00003 | TestLoss: 1.26576 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 287 |  TrainLoss: 0.00007 | TestLoss: 1.26413 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 288 |  TrainLoss: 0.00011 | TestLoss: 1.26819 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 289 |  TrainLoss: 0.00007 | TestLoss: 1.27129 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 290 |  TrainLoss: 0.00003 | TestLoss: 1.27372 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 291 |  TrainLoss: 0.00004 | TestLoss: 1.27651 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 292 |  TrainLoss: 0.00005 | TestLoss: 1.27812 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 293 |  TrainLoss: 0.00009 | TestLoss: 1.27690 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 294 |  TrainLoss: 0.00004 | TestLoss: 1.27545 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 295 |  TrainLoss: 0.00012 | TestLoss: 1.27873 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 296 |  TrainLoss: 0.00003 | TestLoss: 1.28196 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 297 |  TrainLoss: 0.00006 | TestLoss: 1.28492 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 298 |  TrainLoss: 0.00004 | TestLoss: 1.28790 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 299 |  TrainLoss: 0.00005 | TestLoss: 1.29017 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 300 |  TrainLoss: 0.00004 | TestLoss: 1.29151 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 301 |  TrainLoss: 0.00004 | TestLoss: 1.29253 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 302 |  TrainLoss: 0.00007 | TestLoss: 1.29391 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 303 |  TrainLoss: 0.00006 | TestLoss: 1.29416 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 304 |  TrainLoss: 0.00005 | TestLoss: 1.29332 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 305 |  TrainLoss: 0.00004 | TestLoss: 1.29181 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 306 |  TrainLoss: 0.00003 | TestLoss: 1.28989 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 307 |  TrainLoss: 0.00010 | TestLoss: 1.28491 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 308 |  TrainLoss: 0.00007 | TestLoss: 1.28039 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 309 |  TrainLoss: 0.00004 | TestLoss: 1.27619 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 310 |  TrainLoss: 0.00003 | TestLoss: 1.27232 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 311 |  TrainLoss: 0.00005 | TestLoss: 1.26886 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 312 |  TrainLoss: 0.00002 | TestLoss: 1.26581 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 313 |  TrainLoss: 0.00005 | TestLoss: 1.26279 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 314 |  TrainLoss: 0.00003 | TestLoss: 1.26002 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 315 |  TrainLoss: 0.00004 | TestLoss: 1.25722 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 316 |  TrainLoss: 0.00006 | TestLoss: 1.25436 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 317 |  TrainLoss: 0.00004 | TestLoss: 1.25142 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 318 |  TrainLoss: 0.00006 | TestLoss: 1.24910 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 319 |  TrainLoss: 0.00004 | TestLoss: 1.24635 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 320 |  TrainLoss: 0.00006 | TestLoss: 1.24605 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 321 |  TrainLoss: 0.00004 | TestLoss: 1.24627 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 322 |  TrainLoss: 0.00005 | TestLoss: 1.24664 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 323 |  TrainLoss: 0.00006 | TestLoss: 1.24817 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 324 |  TrainLoss: 0.00003 | TestLoss: 1.24945 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 325 |  TrainLoss: 0.00001 | TestLoss: 1.25090 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 326 |  TrainLoss: 0.00005 | TestLoss: 1.25398 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 327 |  TrainLoss: 0.00003 | TestLoss: 1.25752 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 328 |  TrainLoss: 0.00002 | TestLoss: 1.26080 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 329 |  TrainLoss: 0.00005 | TestLoss: 1.26444 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 330 |  TrainLoss: 0.00002 | TestLoss: 1.26806 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 331 |  TrainLoss: 0.00003 | TestLoss: 1.27115 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 332 |  TrainLoss: 0.00003 | TestLoss: 1.27408 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 333 |  TrainLoss: 0.00003 | TestLoss: 1.27723 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 334 |  TrainLoss: 0.00006 | TestLoss: 1.27960 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 335 |  TrainLoss: 0.00005 | TestLoss: 1.28158 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 336 |  TrainLoss: 0.00004 | TestLoss: 1.28337 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 337 |  TrainLoss: 0.00007 | TestLoss: 1.28403 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 338 |  TrainLoss: 0.00007 | TestLoss: 1.28475 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 339 |  TrainLoss: 0.00005 | TestLoss: 1.28561 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 340 |  TrainLoss: 0.00010 | TestLoss: 1.29136 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 341 |  TrainLoss: 0.00004 | TestLoss: 1.29596 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 342 |  TrainLoss: 0.00004 | TestLoss: 1.29949 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 343 |  TrainLoss: 0.00006 | TestLoss: 1.30261 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 344 |  TrainLoss: 0.00004 | TestLoss: 1.30587 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 345 |  TrainLoss: 0.00005 | TestLoss: 1.30865 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 346 |  TrainLoss: 0.00005 | TestLoss: 1.31034 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 347 |  TrainLoss: 0.00005 | TestLoss: 1.31045 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 348 |  TrainLoss: 0.00004 | TestLoss: 1.31143 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 349 |  TrainLoss: 0.00004 | TestLoss: 1.31123 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 350 |  TrainLoss: 0.00003 | TestLoss: 1.31065 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 351 |  TrainLoss: 0.00004 | TestLoss: 1.31071 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 352 |  TrainLoss: 0.00003 | TestLoss: 1.31065 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 353 |  TrainLoss: 0.00004 | TestLoss: 1.30963 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 354 |  TrainLoss: 0.00003 | TestLoss: 1.30840 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 355 |  TrainLoss: 0.00002 | TestLoss: 1.30674 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 356 |  TrainLoss: 0.00007 | TestLoss: 1.30666 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 357 |  TrainLoss: 0.00005 | TestLoss: 1.30806 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 358 |  TrainLoss: 0.00002 | TestLoss: 1.30949 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 359 |  TrainLoss: 0.00003 | TestLoss: 1.31035 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 360 |  TrainLoss: 0.00002 | TestLoss: 1.31104 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 361 |  TrainLoss: 0.00007 | TestLoss: 1.31034 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 362 |  TrainLoss: 0.00004 | TestLoss: 1.30910 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 363 |  TrainLoss: 0.00004 | TestLoss: 1.30710 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 364 |  TrainLoss: 0.00006 | TestLoss: 1.30451 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 365 |  TrainLoss: 0.00003 | TestLoss: 1.30160 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 366 |  TrainLoss: 0.00004 | TestLoss: 1.30004 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 367 |  TrainLoss: 0.00002 | TestLoss: 1.29814 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 368 |  TrainLoss: 0.00004 | TestLoss: 1.29606 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 369 |  TrainLoss: 0.00003 | TestLoss: 1.29396 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 370 |  TrainLoss: 0.00002 | TestLoss: 1.29210 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 371 |  TrainLoss: 0.00003 | TestLoss: 1.29105 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 372 |  TrainLoss: 0.00006 | TestLoss: 1.29206 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 373 |  TrainLoss: 0.00018 | TestLoss: 1.30339 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 374 |  TrainLoss: 0.00003 | TestLoss: 1.31416 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 375 |  TrainLoss: 0.00002 | TestLoss: 1.32469 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 376 |  TrainLoss: 0.00003 | TestLoss: 1.33390 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 377 |  TrainLoss: 0.00004 | TestLoss: 1.34122 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 378 |  TrainLoss: 0.00002 | TestLoss: 1.34718 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 379 |  TrainLoss: 0.00005 | TestLoss: 1.35206 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 380 |  TrainLoss: 0.00006 | TestLoss: 1.35413 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 381 |  TrainLoss: 0.00002 | TestLoss: 1.35549 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 382 |  TrainLoss: 0.00003 | TestLoss: 1.35624 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 383 |  TrainLoss: 0.00004 | TestLoss: 1.35674 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 384 |  TrainLoss: 0.00004 | TestLoss: 1.35592 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 385 |  TrainLoss: 0.00003 | TestLoss: 1.35440 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 386 |  TrainLoss: 0.00005 | TestLoss: 1.35151 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 387 |  TrainLoss: 0.00003 | TestLoss: 1.34815 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 388 |  TrainLoss: 0.00003 | TestLoss: 1.34473 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 389 |  TrainLoss: 0.00003 | TestLoss: 1.34164 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 390 |  TrainLoss: 0.00003 | TestLoss: 1.33899 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 391 |  TrainLoss: 0.00004 | TestLoss: 1.33648 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 392 |  TrainLoss: 0.00002 | TestLoss: 1.33397 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 393 |  TrainLoss: 0.00002 | TestLoss: 1.33189 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 394 |  TrainLoss: 0.00003 | TestLoss: 1.32877 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 395 |  TrainLoss: 0.00002 | TestLoss: 1.32575 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 396 |  TrainLoss: 0.00002 | TestLoss: 1.32308 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 397 |  TrainLoss: 0.00003 | TestLoss: 1.32006 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 398 |  TrainLoss: 0.00004 | TestLoss: 1.31700 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 399 |  TrainLoss: 0.00004 | TestLoss: 1.31470 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 400 |  TrainLoss: 0.00003 | TestLoss: 1.31283 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 401 |  TrainLoss: 0.00002 | TestLoss: 1.31151 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 402 |  TrainLoss: 0.00002 | TestLoss: 1.31046 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 403 |  TrainLoss: 0.00006 | TestLoss: 1.31248 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 404 |  TrainLoss: 0.00008 | TestLoss: 1.31909 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 405 |  TrainLoss: 0.00002 | TestLoss: 1.32503 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 406 |  TrainLoss: 0.00003 | TestLoss: 1.33130 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 407 |  TrainLoss: 0.00004 | TestLoss: 1.33764 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 408 |  TrainLoss: 0.00002 | TestLoss: 1.34338 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 409 |  TrainLoss: 0.00007 | TestLoss: 1.34638 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 410 |  TrainLoss: 0.00003 | TestLoss: 1.34928 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 411 |  TrainLoss: 0.00003 | TestLoss: 1.35153 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 412 |  TrainLoss: 0.00005 | TestLoss: 1.35306 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 413 |  TrainLoss: 0.00003 | TestLoss: 1.35379 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 414 |  TrainLoss: 0.00002 | TestLoss: 1.35397 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 415 |  TrainLoss: 0.00003 | TestLoss: 1.35382 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 416 |  TrainLoss: 0.00004 | TestLoss: 1.35242 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 417 |  TrainLoss: 0.00002 | TestLoss: 1.35096 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 418 |  TrainLoss: 0.00006 | TestLoss: 1.34799 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 419 |  TrainLoss: 0.00002 | TestLoss: 1.34449 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 420 |  TrainLoss: 0.00003 | TestLoss: 1.34107 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 421 |  TrainLoss: 0.00002 | TestLoss: 1.33757 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 422 |  TrainLoss: 0.00002 | TestLoss: 1.33456 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 423 |  TrainLoss: 0.00002 | TestLoss: 1.33151 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 424 |  TrainLoss: 0.00003 | TestLoss: 1.33004 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 425 |  TrainLoss: 0.00004 | TestLoss: 1.32853 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 426 |  TrainLoss: 0.00002 | TestLoss: 1.32711 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 427 |  TrainLoss: 0.00001 | TestLoss: 1.32523 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 428 |  TrainLoss: 0.00004 | TestLoss: 1.32362 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 429 |  TrainLoss: 0.00005 | TestLoss: 1.32459 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 430 |  TrainLoss: 0.00003 | TestLoss: 1.32655 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 431 |  TrainLoss: 0.00002 | TestLoss: 1.32847 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 432 |  TrainLoss: 0.00003 | TestLoss: 1.33123 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 433 |  TrainLoss: 0.00002 | TestLoss: 1.33331 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 434 |  TrainLoss: 0.00003 | TestLoss: 1.33449 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 435 |  TrainLoss: 0.00002 | TestLoss: 1.33575 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 436 |  TrainLoss: 0.00002 | TestLoss: 1.33685 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 437 |  TrainLoss: 0.00003 | TestLoss: 1.33772 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 438 |  TrainLoss: 0.00002 | TestLoss: 1.33905 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 439 |  TrainLoss: 0.00008 | TestLoss: 1.33711 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 440 |  TrainLoss: 0.00001 | TestLoss: 1.33468 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 441 |  TrainLoss: 0.00002 | TestLoss: 1.33265 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 442 |  TrainLoss: 0.00003 | TestLoss: 1.33064 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 443 |  TrainLoss: 0.00002 | TestLoss: 1.32982 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 444 |  TrainLoss: 0.00002 | TestLoss: 1.32829 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 445 |  TrainLoss: 0.00002 | TestLoss: 1.32702 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 446 |  TrainLoss: 0.00002 | TestLoss: 1.32632 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 447 |  TrainLoss: 0.00002 | TestLoss: 1.32628 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 448 |  TrainLoss: 0.00002 | TestLoss: 1.32667 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 449 |  TrainLoss: 0.00002 | TestLoss: 1.32684 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 450 |  TrainLoss: 0.00001 | TestLoss: 1.32689 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 451 |  TrainLoss: 0.00002 | TestLoss: 1.32744 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 452 |  TrainLoss: 0.00001 | TestLoss: 1.32801 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 453 |  TrainLoss: 0.00002 | TestLoss: 1.32900 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 454 |  TrainLoss: 0.00002 | TestLoss: 1.32960 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 455 |  TrainLoss: 0.00003 | TestLoss: 1.33121 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 456 |  TrainLoss: 0.00006 | TestLoss: 1.33357 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 457 |  TrainLoss: 0.00005 | TestLoss: 1.33800 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 458 |  TrainLoss: 0.00003 | TestLoss: 1.34262 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 459 |  TrainLoss: 0.00001 | TestLoss: 1.34711 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 460 |  TrainLoss: 0.00003 | TestLoss: 1.35178 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 461 |  TrainLoss: 0.00001 | TestLoss: 1.35645 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 462 |  TrainLoss: 0.00003 | TestLoss: 1.36049 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 463 |  TrainLoss: 0.00003 | TestLoss: 1.36498 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 464 |  TrainLoss: 0.00006 | TestLoss: 1.36617 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 465 |  TrainLoss: 0.00002 | TestLoss: 1.36690 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 466 |  TrainLoss: 0.00002 | TestLoss: 1.36763 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 467 |  TrainLoss: 0.00002 | TestLoss: 1.36757 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 468 |  TrainLoss: 0.00003 | TestLoss: 1.36669 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 469 |  TrainLoss: 0.00002 | TestLoss: 1.36565 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 470 |  TrainLoss: 0.00004 | TestLoss: 1.36678 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 471 |  TrainLoss: 0.00003 | TestLoss: 1.36738 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 472 |  TrainLoss: 0.00002 | TestLoss: 1.36748 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 473 |  TrainLoss: 0.00002 | TestLoss: 1.36729 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 474 |  TrainLoss: 0.00002 | TestLoss: 1.36665 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 475 |  TrainLoss: 0.00001 | TestLoss: 1.36632 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 476 |  TrainLoss: 0.00001 | TestLoss: 1.36608 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 477 |  TrainLoss: 0.00002 | TestLoss: 1.36612 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 478 |  TrainLoss: 0.00001 | TestLoss: 1.36613 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 479 |  TrainLoss: 0.00002 | TestLoss: 1.36649 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 480 |  TrainLoss: 0.00006 | TestLoss: 1.36929 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 481 |  TrainLoss: 0.00001 | TestLoss: 1.37157 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 482 |  TrainLoss: 0.00002 | TestLoss: 1.37358 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 483 |  TrainLoss: 0.00001 | TestLoss: 1.37531 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 484 |  TrainLoss: 0.00002 | TestLoss: 1.37698 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 485 |  TrainLoss: 0.00005 | TestLoss: 1.37869 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 486 |  TrainLoss: 0.00001 | TestLoss: 1.38027 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 487 |  TrainLoss: 0.00002 | TestLoss: 1.38109 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 488 |  TrainLoss: 0.00001 | TestLoss: 1.38159 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 489 |  TrainLoss: 0.00002 | TestLoss: 1.38194 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 490 |  TrainLoss: 0.00001 | TestLoss: 1.38215 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 491 |  TrainLoss: 0.00001 | TestLoss: 1.38239 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 492 |  TrainLoss: 0.00002 | TestLoss: 1.38194 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 493 |  TrainLoss: 0.00003 | TestLoss: 1.38223 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 494 |  TrainLoss: 0.00002 | TestLoss: 1.38215 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 495 |  TrainLoss: 0.00002 | TestLoss: 1.38140 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 496 |  TrainLoss: 0.00002 | TestLoss: 1.38013 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 497 |  TrainLoss: 0.00002 | TestLoss: 1.37889 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 498 |  TrainLoss: 0.00002 | TestLoss: 1.37848 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 499 |  TrainLoss: 0.00002 | TestLoss: 1.37787 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Best WLoss: 0.00203 | Best Epoch: 52\n"
     ]
    }
   ],
   "source": [
    "wloss = []\n",
    "weighted_loss = 0\n",
    "exp_param = 0.8\n",
    "best_test_loss = float('inf')  #initialize with a large value\n",
    "\n",
    "#without dropout training results\n",
    "for epoch in range(500):\n",
    "  train_loss = train(epoch)\n",
    "  test_loss, test_acc, test_f1 = test(epoch)\n",
    "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
    "  \n",
    "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
    "  \n",
    "  if test_loss < best_test_loss:\n",
    "    best_test_loss = test_loss  #update the best test loss\n",
    "\n",
    "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
    "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
    "\n",
    "# Print the best values\n",
    "best_wloss = min(wloss)\n",
    "best_epoch = wloss.index(best_wloss)\n",
    "print(f'Best WLoss: {best_wloss:.5f} | Best Epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326dede9",
   "metadata": {},
   "source": [
    "# Model 2: \n",
    "GraphSage + Bert Embeddings with hyperparameters as defined by the paper and 3 Layer MLP\n",
    "\n",
    "(Embedding size = 128, batch size= 128, l2 Regularization = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f714830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, embedding_size, batch_size, l2_reg_weight):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.batch_size = batch_size\n",
    "        self.l2_reg_weight = l2_reg_weight\n",
    "        \n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
    "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
    "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
    "        \n",
    "        self.full1 = Linear(hidden_channels[2], hidden_channels[3])\n",
    "        self.full2 = Linear(hidden_channels[3], hidden_channels[4])\n",
    "        self.full3 = Linear(hidden_channels[4], hidden_channels[5])\n",
    "\n",
    "        self.softmax = Linear(hidden_channels[5], out_channels)\n",
    "\n",
    "        # Dropouts\n",
    "        self.dp1 = Dropout(0.2)\n",
    "        self.dp2 = Dropout(0.2)\n",
    "        self.dp3 = Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = self.conv1(x, edge_index).relu()\n",
    "        h = self.conv2(h, edge_index).relu()\n",
    "        h = self.conv3(h, edge_index).relu()\n",
    "\n",
    "        h = global_max_pool(h, batch)\n",
    "\n",
    "        h = self.full1(h).relu()\n",
    "        h = self.dp1(h)\n",
    "        h = self.full2(h).relu()\n",
    "        h = self.dp2(h)\n",
    "        h = self.full3(h).relu()\n",
    "        h = self.dp3(h)\n",
    "        \n",
    "        h = self.softmax(h)\n",
    "\n",
    "        return torch.sigmoid(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d226b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(test_data_pol.num_features,[512,512,512,256,256,256],1, 128, 128, 0.001).to(device) #defining embedding size=128, batch size=128 and l2 regularization weight= 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "lossff = torch.nn.BCELoss()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bfcf3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        all_preds.append(torch.reshape(out, (-1,)))\n",
    "        all_labels.append(data.y.float())\n",
    "    # print(all_preds)\n",
    "    accuracy, f1 = metrics(all_preds, all_labels)\n",
    "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
    "\n",
    "\n",
    "def metrics(preds, gts):\n",
    "    preds = torch.round(torch.cat(preds))\n",
    "    gts = torch.cat(gts)\n",
    "    # print(preds.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80982443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 |  TrainLoss: 0.69616 | TestLoss: 0.69280 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 01 |  TrainLoss: 0.69535 | TestLoss: 0.69268 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 02 |  TrainLoss: 0.69425 | TestLoss: 0.69263 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 03 |  TrainLoss: 0.69481 | TestLoss: 0.69261 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 04 |  TrainLoss: 0.69466 | TestLoss: 0.69255 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 05 |  TrainLoss: 0.69441 | TestLoss: 0.69250 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 06 |  TrainLoss: 0.69342 | TestLoss: 0.69246 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 07 |  TrainLoss: 0.69260 | TestLoss: 0.69237 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 08 |  TrainLoss: 0.69307 | TestLoss: 0.69228 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 09 |  TrainLoss: 0.69403 | TestLoss: 0.69215 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 10 |  TrainLoss: 0.69286 | TestLoss: 0.69201 | TestAcc: 0.56561 | TestF1: 0.70\n",
      "Epoch: 11 |  TrainLoss: 0.69227 | TestLoss: 0.69186 | TestAcc: 0.72398 | TestF1: 0.68\n",
      "Epoch: 12 |  TrainLoss: 0.69075 | TestLoss: 0.69165 | TestAcc: 0.60633 | TestF1: 0.39\n",
      "Epoch: 13 |  TrainLoss: 0.69038 | TestLoss: 0.69141 | TestAcc: 0.51584 | TestF1: 0.10\n",
      "Epoch: 14 |  TrainLoss: 0.69110 | TestLoss: 0.69114 | TestAcc: 0.49774 | TestF1: 0.03\n",
      "Epoch: 15 |  TrainLoss: 0.69138 | TestLoss: 0.69078 | TestAcc: 0.49774 | TestF1: 0.03\n",
      "Epoch: 16 |  TrainLoss: 0.68867 | TestLoss: 0.69040 | TestAcc: 0.49774 | TestF1: 0.03\n",
      "Epoch: 17 |  TrainLoss: 0.68908 | TestLoss: 0.68996 | TestAcc: 0.49774 | TestF1: 0.03\n",
      "Epoch: 18 |  TrainLoss: 0.68916 | TestLoss: 0.68941 | TestAcc: 0.49774 | TestF1: 0.03\n",
      "Epoch: 19 |  TrainLoss: 0.68669 | TestLoss: 0.68870 | TestAcc: 0.49774 | TestF1: 0.03\n",
      "Epoch: 20 |  TrainLoss: 0.68373 | TestLoss: 0.68780 | TestAcc: 0.51131 | TestF1: 0.08\n",
      "Epoch: 21 |  TrainLoss: 0.68526 | TestLoss: 0.68672 | TestAcc: 0.53846 | TestF1: 0.18\n",
      "Epoch: 22 |  TrainLoss: 0.67988 | TestLoss: 0.68551 | TestAcc: 0.58371 | TestF1: 0.32\n",
      "Epoch: 23 |  TrainLoss: 0.68040 | TestLoss: 0.68427 | TestAcc: 0.57919 | TestF1: 0.31\n",
      "Epoch: 24 |  TrainLoss: 0.67787 | TestLoss: 0.68292 | TestAcc: 0.57919 | TestF1: 0.31\n",
      "Epoch: 25 |  TrainLoss: 0.67734 | TestLoss: 0.68138 | TestAcc: 0.57919 | TestF1: 0.31\n",
      "Epoch: 26 |  TrainLoss: 0.67309 | TestLoss: 0.67965 | TestAcc: 0.57919 | TestF1: 0.31\n",
      "Epoch: 27 |  TrainLoss: 0.66993 | TestLoss: 0.67769 | TestAcc: 0.57919 | TestF1: 0.31\n",
      "Epoch: 28 |  TrainLoss: 0.66945 | TestLoss: 0.67519 | TestAcc: 0.58824 | TestF1: 0.34\n",
      "Epoch: 29 |  TrainLoss: 0.66404 | TestLoss: 0.67198 | TestAcc: 0.60181 | TestF1: 0.37\n",
      "Epoch: 30 |  TrainLoss: 0.66251 | TestLoss: 0.66825 | TestAcc: 0.65158 | TestF1: 0.50\n",
      "Epoch: 31 |  TrainLoss: 0.65445 | TestLoss: 0.66428 | TestAcc: 0.65611 | TestF1: 0.51\n",
      "Epoch: 32 |  TrainLoss: 0.64610 | TestLoss: 0.66023 | TestAcc: 0.66063 | TestF1: 0.52\n",
      "Epoch: 33 |  TrainLoss: 0.64581 | TestLoss: 0.65576 | TestAcc: 0.66063 | TestF1: 0.52\n",
      "Epoch: 34 |  TrainLoss: 0.63440 | TestLoss: 0.65032 | TestAcc: 0.65611 | TestF1: 0.51\n",
      "Epoch: 35 |  TrainLoss: 0.62498 | TestLoss: 0.64360 | TestAcc: 0.66516 | TestF1: 0.54\n",
      "Epoch: 36 |  TrainLoss: 0.61211 | TestLoss: 0.63609 | TestAcc: 0.68326 | TestF1: 0.58\n",
      "Epoch: 37 |  TrainLoss: 0.61082 | TestLoss: 0.62773 | TestAcc: 0.70136 | TestF1: 0.62\n",
      "Epoch: 38 |  TrainLoss: 0.59432 | TestLoss: 0.61910 | TestAcc: 0.69683 | TestF1: 0.61\n",
      "Epoch: 39 |  TrainLoss: 0.57868 | TestLoss: 0.61103 | TestAcc: 0.69231 | TestF1: 0.60\n",
      "Epoch: 40 |  TrainLoss: 0.56451 | TestLoss: 0.60031 | TestAcc: 0.69683 | TestF1: 0.61\n",
      "Epoch: 41 |  TrainLoss: 0.54486 | TestLoss: 0.58627 | TestAcc: 0.74208 | TestF1: 0.70\n",
      "Epoch: 42 |  TrainLoss: 0.52992 | TestLoss: 0.57229 | TestAcc: 0.76471 | TestF1: 0.73\n",
      "Epoch: 43 |  TrainLoss: 0.51083 | TestLoss: 0.56082 | TestAcc: 0.76018 | TestF1: 0.72\n",
      "Epoch: 44 |  TrainLoss: 0.47949 | TestLoss: 0.54936 | TestAcc: 0.75566 | TestF1: 0.72\n",
      "Epoch: 45 |  TrainLoss: 0.46678 | TestLoss: 0.53154 | TestAcc: 0.78281 | TestF1: 0.76\n",
      "Epoch: 46 |  TrainLoss: 0.43190 | TestLoss: 0.51504 | TestAcc: 0.79186 | TestF1: 0.77\n",
      "Epoch: 47 |  TrainLoss: 0.40693 | TestLoss: 0.50358 | TestAcc: 0.78733 | TestF1: 0.76\n",
      "Epoch: 48 |  TrainLoss: 0.37006 | TestLoss: 0.48690 | TestAcc: 0.79638 | TestF1: 0.78\n",
      "Epoch: 49 |  TrainLoss: 0.34480 | TestLoss: 0.47362 | TestAcc: 0.80090 | TestF1: 0.78\n",
      "Epoch: 50 |  TrainLoss: 0.32263 | TestLoss: 0.47095 | TestAcc: 0.78733 | TestF1: 0.76\n",
      "Epoch: 51 |  TrainLoss: 0.29195 | TestLoss: 0.43871 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 52 |  TrainLoss: 0.25917 | TestLoss: 0.43275 | TestAcc: 0.82353 | TestF1: 0.82\n",
      "Epoch: 53 |  TrainLoss: 0.24686 | TestLoss: 0.45259 | TestAcc: 0.80543 | TestF1: 0.79\n",
      "Epoch: 54 |  TrainLoss: 0.20652 | TestLoss: 0.43395 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 55 |  TrainLoss: 0.17853 | TestLoss: 0.43153 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 56 |  TrainLoss: 0.16279 | TestLoss: 0.42921 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 57 |  TrainLoss: 0.13558 | TestLoss: 0.45643 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 58 |  TrainLoss: 0.11893 | TestLoss: 0.44177 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 59 |  TrainLoss: 0.10923 | TestLoss: 0.44236 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 60 |  TrainLoss: 0.10166 | TestLoss: 0.49424 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 61 |  TrainLoss: 0.06609 | TestLoss: 0.49920 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 62 |  TrainLoss: 0.05681 | TestLoss: 0.49229 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 63 |  TrainLoss: 0.04479 | TestLoss: 0.52700 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 64 |  TrainLoss: 0.04937 | TestLoss: 0.60276 | TestAcc: 0.83258 | TestF1: 0.82\n",
      "Epoch: 65 |  TrainLoss: 0.03693 | TestLoss: 0.58082 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 66 |  TrainLoss: 0.02251 | TestLoss: 0.57692 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 67 |  TrainLoss: 0.02786 | TestLoss: 0.62708 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 68 |  TrainLoss: 0.01529 | TestLoss: 0.70269 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 69 |  TrainLoss: 0.01244 | TestLoss: 0.71619 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 70 |  TrainLoss: 0.01399 | TestLoss: 0.67163 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 71 |  TrainLoss: 0.00877 | TestLoss: 0.65637 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 72 |  TrainLoss: 0.01295 | TestLoss: 0.68098 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 73 |  TrainLoss: 0.00803 | TestLoss: 0.74107 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 74 |  TrainLoss: 0.00456 | TestLoss: 0.81779 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 75 |  TrainLoss: 0.00415 | TestLoss: 0.89387 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 76 |  TrainLoss: 0.00435 | TestLoss: 0.91741 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 77 |  TrainLoss: 0.00970 | TestLoss: 0.82893 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 78 |  TrainLoss: 0.00226 | TestLoss: 0.78844 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 79 |  TrainLoss: 0.00408 | TestLoss: 0.79405 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 80 |  TrainLoss: 0.00413 | TestLoss: 0.80996 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 81 |  TrainLoss: 0.00147 | TestLoss: 0.83103 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 82 |  TrainLoss: 0.00136 | TestLoss: 0.86060 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 83 |  TrainLoss: 0.00108 | TestLoss: 0.89955 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 84 |  TrainLoss: 0.00112 | TestLoss: 0.94220 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 85 |  TrainLoss: 0.00142 | TestLoss: 0.98865 | TestAcc: 0.83258 | TestF1: 0.82\n",
      "Epoch: 86 |  TrainLoss: 0.00114 | TestLoss: 1.02806 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 87 |  TrainLoss: 0.00118 | TestLoss: 1.04926 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 88 |  TrainLoss: 0.00080 | TestLoss: 1.05904 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 89 |  TrainLoss: 0.00084 | TestLoss: 1.05641 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 90 |  TrainLoss: 0.00163 | TestLoss: 1.02337 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 91 |  TrainLoss: 0.00115 | TestLoss: 0.99059 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 92 |  TrainLoss: 0.00066 | TestLoss: 0.96357 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 93 |  TrainLoss: 0.00045 | TestLoss: 0.94544 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 94 |  TrainLoss: 0.00063 | TestLoss: 0.93787 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 95 |  TrainLoss: 0.00144 | TestLoss: 0.94292 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 96 |  TrainLoss: 0.00086 | TestLoss: 0.95257 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 97 |  TrainLoss: 0.00112 | TestLoss: 0.97194 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 98 |  TrainLoss: 0.00021 | TestLoss: 0.99394 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 99 |  TrainLoss: 0.00087 | TestLoss: 1.00793 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 100 |  TrainLoss: 0.00054 | TestLoss: 1.02366 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 101 |  TrainLoss: 0.00030 | TestLoss: 1.03994 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 102 |  TrainLoss: 0.00050 | TestLoss: 1.05714 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 103 |  TrainLoss: 0.00096 | TestLoss: 1.05903 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 104 |  TrainLoss: 0.00033 | TestLoss: 1.06039 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 105 |  TrainLoss: 0.00025 | TestLoss: 1.06015 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 106 |  TrainLoss: 0.00128 | TestLoss: 1.04358 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 107 |  TrainLoss: 0.00027 | TestLoss: 1.02946 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 108 |  TrainLoss: 0.00024 | TestLoss: 1.01823 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 109 |  TrainLoss: 0.00023 | TestLoss: 1.00937 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 110 |  TrainLoss: 0.00035 | TestLoss: 1.00436 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 111 |  TrainLoss: 0.00039 | TestLoss: 1.00339 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 112 |  TrainLoss: 0.00033 | TestLoss: 1.00492 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 113 |  TrainLoss: 0.00080 | TestLoss: 1.01485 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 114 |  TrainLoss: 0.00020 | TestLoss: 1.02568 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 115 |  TrainLoss: 0.00018 | TestLoss: 1.03700 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 116 |  TrainLoss: 0.00029 | TestLoss: 1.04987 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 117 |  TrainLoss: 0.00036 | TestLoss: 1.06233 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 118 |  TrainLoss: 0.00027 | TestLoss: 1.07161 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 119 |  TrainLoss: 0.00018 | TestLoss: 1.08183 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 120 |  TrainLoss: 0.00019 | TestLoss: 1.09195 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 121 |  TrainLoss: 0.00023 | TestLoss: 1.09988 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 122 |  TrainLoss: 0.00039 | TestLoss: 1.10205 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 123 |  TrainLoss: 0.00032 | TestLoss: 1.10034 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 124 |  TrainLoss: 0.00018 | TestLoss: 1.09910 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 125 |  TrainLoss: 0.00124 | TestLoss: 1.07471 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 126 |  TrainLoss: 0.00016 | TestLoss: 1.05513 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 127 |  TrainLoss: 0.00017 | TestLoss: 1.03998 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 128 |  TrainLoss: 0.00041 | TestLoss: 1.03301 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 129 |  TrainLoss: 0.00021 | TestLoss: 1.02820 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 130 |  TrainLoss: 0.00018 | TestLoss: 1.02586 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 131 |  TrainLoss: 0.00027 | TestLoss: 1.02515 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 132 |  TrainLoss: 0.00016 | TestLoss: 1.02540 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 133 |  TrainLoss: 0.00021 | TestLoss: 1.02722 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 134 |  TrainLoss: 0.00035 | TestLoss: 1.03153 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 135 |  TrainLoss: 0.00012 | TestLoss: 1.03606 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 136 |  TrainLoss: 0.00020 | TestLoss: 1.04188 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 137 |  TrainLoss: 0.00024 | TestLoss: 1.04950 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 138 |  TrainLoss: 0.00022 | TestLoss: 1.05902 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 139 |  TrainLoss: 0.00013 | TestLoss: 1.06932 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 140 |  TrainLoss: 0.00020 | TestLoss: 1.08206 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 141 |  TrainLoss: 0.00015 | TestLoss: 1.09560 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 142 |  TrainLoss: 0.00024 | TestLoss: 1.10947 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 143 |  TrainLoss: 0.00020 | TestLoss: 1.12156 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 144 |  TrainLoss: 0.00021 | TestLoss: 1.13021 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 145 |  TrainLoss: 0.00013 | TestLoss: 1.13751 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 146 |  TrainLoss: 0.00018 | TestLoss: 1.14355 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 147 |  TrainLoss: 0.00023 | TestLoss: 1.14628 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 148 |  TrainLoss: 0.00018 | TestLoss: 1.14814 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 149 |  TrainLoss: 0.00009 | TestLoss: 1.14925 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 150 |  TrainLoss: 0.00013 | TestLoss: 1.14917 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 151 |  TrainLoss: 0.00016 | TestLoss: 1.14691 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 152 |  TrainLoss: 0.00028 | TestLoss: 1.14700 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 153 |  TrainLoss: 0.00015 | TestLoss: 1.14583 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 154 |  TrainLoss: 0.00014 | TestLoss: 1.14378 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 155 |  TrainLoss: 0.00030 | TestLoss: 1.14051 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 156 |  TrainLoss: 0.00016 | TestLoss: 1.13674 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 157 |  TrainLoss: 0.00010 | TestLoss: 1.13320 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 158 |  TrainLoss: 0.00018 | TestLoss: 1.12900 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 159 |  TrainLoss: 0.00014 | TestLoss: 1.12505 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 160 |  TrainLoss: 0.00007 | TestLoss: 1.12141 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 161 |  TrainLoss: 0.00024 | TestLoss: 1.11825 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 162 |  TrainLoss: 0.00013 | TestLoss: 1.11510 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 163 |  TrainLoss: 0.00011 | TestLoss: 1.11251 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 164 |  TrainLoss: 0.00014 | TestLoss: 1.11027 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 165 |  TrainLoss: 0.00016 | TestLoss: 1.11004 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 166 |  TrainLoss: 0.00008 | TestLoss: 1.10969 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 167 |  TrainLoss: 0.00013 | TestLoss: 1.10980 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 168 |  TrainLoss: 0.00010 | TestLoss: 1.11093 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 169 |  TrainLoss: 0.00013 | TestLoss: 1.11151 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 170 |  TrainLoss: 0.00012 | TestLoss: 1.11277 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 171 |  TrainLoss: 0.00011 | TestLoss: 1.11455 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 172 |  TrainLoss: 0.00025 | TestLoss: 1.12041 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 173 |  TrainLoss: 0.00037 | TestLoss: 1.13279 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 174 |  TrainLoss: 0.00007 | TestLoss: 1.14428 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 175 |  TrainLoss: 0.00009 | TestLoss: 1.15553 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 176 |  TrainLoss: 0.00013 | TestLoss: 1.16465 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 177 |  TrainLoss: 0.00010 | TestLoss: 1.17321 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 178 |  TrainLoss: 0.00008 | TestLoss: 1.18048 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 179 |  TrainLoss: 0.00007 | TestLoss: 1.18647 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 180 |  TrainLoss: 0.00013 | TestLoss: 1.19300 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 181 |  TrainLoss: 0.00011 | TestLoss: 1.19741 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 182 |  TrainLoss: 0.00014 | TestLoss: 1.19966 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 183 |  TrainLoss: 0.00015 | TestLoss: 1.19908 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 184 |  TrainLoss: 0.00013 | TestLoss: 1.19514 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 185 |  TrainLoss: 0.00015 | TestLoss: 1.19095 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 186 |  TrainLoss: 0.00017 | TestLoss: 1.18454 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 187 |  TrainLoss: 0.00009 | TestLoss: 1.17851 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 188 |  TrainLoss: 0.00009 | TestLoss: 1.17285 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 189 |  TrainLoss: 0.00012 | TestLoss: 1.16726 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 190 |  TrainLoss: 0.00019 | TestLoss: 1.16016 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 191 |  TrainLoss: 0.00007 | TestLoss: 1.15406 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 192 |  TrainLoss: 0.00009 | TestLoss: 1.14881 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 193 |  TrainLoss: 0.00010 | TestLoss: 1.14404 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 194 |  TrainLoss: 0.00008 | TestLoss: 1.13931 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 195 |  TrainLoss: 0.00017 | TestLoss: 1.13881 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 196 |  TrainLoss: 0.00007 | TestLoss: 1.13846 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 197 |  TrainLoss: 0.00012 | TestLoss: 1.13725 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 198 |  TrainLoss: 0.00011 | TestLoss: 1.13732 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 199 |  TrainLoss: 0.00017 | TestLoss: 1.14152 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 200 |  TrainLoss: 0.00005 | TestLoss: 1.14535 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 201 |  TrainLoss: 0.00012 | TestLoss: 1.15025 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 202 |  TrainLoss: 0.00011 | TestLoss: 1.15389 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 203 |  TrainLoss: 0.00008 | TestLoss: 1.15671 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 204 |  TrainLoss: 0.00011 | TestLoss: 1.15896 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 205 |  TrainLoss: 0.00009 | TestLoss: 1.16031 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 206 |  TrainLoss: 0.00007 | TestLoss: 1.16113 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 207 |  TrainLoss: 0.00010 | TestLoss: 1.16276 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 208 |  TrainLoss: 0.00010 | TestLoss: 1.16494 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 209 |  TrainLoss: 0.00008 | TestLoss: 1.16691 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 210 |  TrainLoss: 0.00008 | TestLoss: 1.16906 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 211 |  TrainLoss: 0.00016 | TestLoss: 1.17205 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 212 |  TrainLoss: 0.00011 | TestLoss: 1.17470 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 213 |  TrainLoss: 0.00008 | TestLoss: 1.17683 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 214 |  TrainLoss: 0.00009 | TestLoss: 1.18098 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 215 |  TrainLoss: 0.00006 | TestLoss: 1.18422 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 216 |  TrainLoss: 0.00008 | TestLoss: 1.18681 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 217 |  TrainLoss: 0.00011 | TestLoss: 1.18807 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 218 |  TrainLoss: 0.00009 | TestLoss: 1.19033 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 219 |  TrainLoss: 0.00011 | TestLoss: 1.19202 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 220 |  TrainLoss: 0.00006 | TestLoss: 1.19346 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 221 |  TrainLoss: 0.00007 | TestLoss: 1.19503 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 222 |  TrainLoss: 0.00006 | TestLoss: 1.19627 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 223 |  TrainLoss: 0.00007 | TestLoss: 1.19648 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 224 |  TrainLoss: 0.00003 | TestLoss: 1.19685 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 225 |  TrainLoss: 0.00011 | TestLoss: 1.19750 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 226 |  TrainLoss: 0.00008 | TestLoss: 1.19706 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 227 |  TrainLoss: 0.00007 | TestLoss: 1.19642 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 228 |  TrainLoss: 0.00008 | TestLoss: 1.19565 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 229 |  TrainLoss: 0.00006 | TestLoss: 1.19590 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 230 |  TrainLoss: 0.00007 | TestLoss: 1.19563 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 231 |  TrainLoss: 0.00007 | TestLoss: 1.19505 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 232 |  TrainLoss: 0.00010 | TestLoss: 1.19514 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 233 |  TrainLoss: 0.00005 | TestLoss: 1.19537 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 234 |  TrainLoss: 0.00007 | TestLoss: 1.19431 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 235 |  TrainLoss: 0.00013 | TestLoss: 1.19552 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 236 |  TrainLoss: 0.00013 | TestLoss: 1.19564 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 237 |  TrainLoss: 0.00007 | TestLoss: 1.19512 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 238 |  TrainLoss: 0.00011 | TestLoss: 1.19334 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 239 |  TrainLoss: 0.00015 | TestLoss: 1.19630 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 240 |  TrainLoss: 0.00004 | TestLoss: 1.19949 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 241 |  TrainLoss: 0.00006 | TestLoss: 1.20260 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 242 |  TrainLoss: 0.00005 | TestLoss: 1.20650 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 243 |  TrainLoss: 0.00011 | TestLoss: 1.20901 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 244 |  TrainLoss: 0.00007 | TestLoss: 1.21051 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 245 |  TrainLoss: 0.00005 | TestLoss: 1.21195 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 246 |  TrainLoss: 0.00009 | TestLoss: 1.21175 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 247 |  TrainLoss: 0.00013 | TestLoss: 1.21136 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 248 |  TrainLoss: 0.00005 | TestLoss: 1.21077 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 249 |  TrainLoss: 0.00009 | TestLoss: 1.20973 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 250 |  TrainLoss: 0.00007 | TestLoss: 1.20922 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 251 |  TrainLoss: 0.00006 | TestLoss: 1.20861 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 252 |  TrainLoss: 0.00004 | TestLoss: 1.20792 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 253 |  TrainLoss: 0.00006 | TestLoss: 1.20754 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 254 |  TrainLoss: 0.00006 | TestLoss: 1.20746 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 255 |  TrainLoss: 0.00005 | TestLoss: 1.20665 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 256 |  TrainLoss: 0.00004 | TestLoss: 1.20596 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 257 |  TrainLoss: 0.00010 | TestLoss: 1.20673 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 258 |  TrainLoss: 0.00006 | TestLoss: 1.20816 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 259 |  TrainLoss: 0.00005 | TestLoss: 1.21066 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 260 |  TrainLoss: 0.00006 | TestLoss: 1.21358 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 261 |  TrainLoss: 0.00003 | TestLoss: 1.21604 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 262 |  TrainLoss: 0.00005 | TestLoss: 1.21902 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 263 |  TrainLoss: 0.00005 | TestLoss: 1.22175 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 264 |  TrainLoss: 0.00006 | TestLoss: 1.22405 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 265 |  TrainLoss: 0.00009 | TestLoss: 1.22464 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 266 |  TrainLoss: 0.00006 | TestLoss: 1.22497 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 267 |  TrainLoss: 0.00004 | TestLoss: 1.22517 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 268 |  TrainLoss: 0.00006 | TestLoss: 1.22529 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 269 |  TrainLoss: 0.00004 | TestLoss: 1.22620 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 270 |  TrainLoss: 0.00008 | TestLoss: 1.22741 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 271 |  TrainLoss: 0.00004 | TestLoss: 1.22837 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 272 |  TrainLoss: 0.00003 | TestLoss: 1.22956 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 273 |  TrainLoss: 0.00005 | TestLoss: 1.22998 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 274 |  TrainLoss: 0.00004 | TestLoss: 1.23033 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 275 |  TrainLoss: 0.00007 | TestLoss: 1.23158 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 276 |  TrainLoss: 0.00005 | TestLoss: 1.23228 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 277 |  TrainLoss: 0.00003 | TestLoss: 1.23295 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 278 |  TrainLoss: 0.00002 | TestLoss: 1.23376 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 279 |  TrainLoss: 0.00004 | TestLoss: 1.23428 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 280 |  TrainLoss: 0.00005 | TestLoss: 1.23447 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 281 |  TrainLoss: 0.00007 | TestLoss: 1.23602 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 282 |  TrainLoss: 0.00004 | TestLoss: 1.23700 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 283 |  TrainLoss: 0.00003 | TestLoss: 1.23791 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 284 |  TrainLoss: 0.00004 | TestLoss: 1.23839 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 285 |  TrainLoss: 0.00005 | TestLoss: 1.23928 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 286 |  TrainLoss: 0.00007 | TestLoss: 1.24003 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 287 |  TrainLoss: 0.00004 | TestLoss: 1.24050 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 288 |  TrainLoss: 0.00004 | TestLoss: 1.24141 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 289 |  TrainLoss: 0.00003 | TestLoss: 1.24241 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 290 |  TrainLoss: 0.00004 | TestLoss: 1.24392 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 291 |  TrainLoss: 0.00005 | TestLoss: 1.24651 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 292 |  TrainLoss: 0.00003 | TestLoss: 1.24854 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 293 |  TrainLoss: 0.00004 | TestLoss: 1.25077 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 294 |  TrainLoss: 0.00005 | TestLoss: 1.25313 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 295 |  TrainLoss: 0.00003 | TestLoss: 1.25545 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 296 |  TrainLoss: 0.00007 | TestLoss: 1.25853 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 297 |  TrainLoss: 0.00004 | TestLoss: 1.26174 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 298 |  TrainLoss: 0.00004 | TestLoss: 1.26429 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 299 |  TrainLoss: 0.00004 | TestLoss: 1.26727 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 300 |  TrainLoss: 0.00005 | TestLoss: 1.27080 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 301 |  TrainLoss: 0.00004 | TestLoss: 1.27344 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 302 |  TrainLoss: 0.00008 | TestLoss: 1.27544 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 303 |  TrainLoss: 0.00005 | TestLoss: 1.27627 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 304 |  TrainLoss: 0.00004 | TestLoss: 1.27770 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 305 |  TrainLoss: 0.00003 | TestLoss: 1.27887 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 306 |  TrainLoss: 0.00002 | TestLoss: 1.28022 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 307 |  TrainLoss: 0.00004 | TestLoss: 1.28037 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 308 |  TrainLoss: 0.00003 | TestLoss: 1.27993 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 309 |  TrainLoss: 0.00003 | TestLoss: 1.27955 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 310 |  TrainLoss: 0.00007 | TestLoss: 1.27719 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 311 |  TrainLoss: 0.00009 | TestLoss: 1.27556 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 312 |  TrainLoss: 0.00002 | TestLoss: 1.27419 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 313 |  TrainLoss: 0.00004 | TestLoss: 1.27278 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 314 |  TrainLoss: 0.00004 | TestLoss: 1.27157 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 315 |  TrainLoss: 0.00003 | TestLoss: 1.27102 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 316 |  TrainLoss: 0.00003 | TestLoss: 1.27017 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 317 |  TrainLoss: 0.00002 | TestLoss: 1.26932 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 318 |  TrainLoss: 0.00003 | TestLoss: 1.26924 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 319 |  TrainLoss: 0.00005 | TestLoss: 1.26836 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 320 |  TrainLoss: 0.00003 | TestLoss: 1.26802 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 321 |  TrainLoss: 0.00006 | TestLoss: 1.26946 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 322 |  TrainLoss: 0.00002 | TestLoss: 1.27064 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 323 |  TrainLoss: 0.00001 | TestLoss: 1.27189 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 324 |  TrainLoss: 0.00002 | TestLoss: 1.27301 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 325 |  TrainLoss: 0.00004 | TestLoss: 1.27450 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 326 |  TrainLoss: 0.00004 | TestLoss: 1.27593 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 327 |  TrainLoss: 0.00006 | TestLoss: 1.27961 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 328 |  TrainLoss: 0.00002 | TestLoss: 1.28330 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 329 |  TrainLoss: 0.00008 | TestLoss: 1.28827 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 330 |  TrainLoss: 0.00006 | TestLoss: 1.29054 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 331 |  TrainLoss: 0.00007 | TestLoss: 1.29606 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 332 |  TrainLoss: 0.00004 | TestLoss: 1.30135 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 333 |  TrainLoss: 0.00004 | TestLoss: 1.30627 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 334 |  TrainLoss: 0.00003 | TestLoss: 1.31131 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 335 |  TrainLoss: 0.00002 | TestLoss: 1.31578 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 336 |  TrainLoss: 0.00002 | TestLoss: 1.31966 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 337 |  TrainLoss: 0.00004 | TestLoss: 1.32399 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 338 |  TrainLoss: 0.00002 | TestLoss: 1.32745 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 339 |  TrainLoss: 0.00004 | TestLoss: 1.33083 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 340 |  TrainLoss: 0.00002 | TestLoss: 1.33397 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 341 |  TrainLoss: 0.00005 | TestLoss: 1.33377 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 342 |  TrainLoss: 0.00004 | TestLoss: 1.33267 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 343 |  TrainLoss: 0.00003 | TestLoss: 1.32992 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 344 |  TrainLoss: 0.00003 | TestLoss: 1.32721 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 345 |  TrainLoss: 0.00005 | TestLoss: 1.32389 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 346 |  TrainLoss: 0.00012 | TestLoss: 1.31490 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 347 |  TrainLoss: 0.00003 | TestLoss: 1.30725 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 348 |  TrainLoss: 0.00003 | TestLoss: 1.30015 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 349 |  TrainLoss: 0.00003 | TestLoss: 1.29377 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 350 |  TrainLoss: 0.00002 | TestLoss: 1.28849 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 351 |  TrainLoss: 0.00003 | TestLoss: 1.28327 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 352 |  TrainLoss: 0.00002 | TestLoss: 1.27873 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 353 |  TrainLoss: 0.00002 | TestLoss: 1.27540 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 354 |  TrainLoss: 0.00004 | TestLoss: 1.27356 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 355 |  TrainLoss: 0.00002 | TestLoss: 1.27292 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 356 |  TrainLoss: 0.00006 | TestLoss: 1.27439 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 357 |  TrainLoss: 0.00003 | TestLoss: 1.27671 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 358 |  TrainLoss: 0.00003 | TestLoss: 1.27943 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 359 |  TrainLoss: 0.00003 | TestLoss: 1.28251 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 360 |  TrainLoss: 0.00003 | TestLoss: 1.28621 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 361 |  TrainLoss: 0.00002 | TestLoss: 1.29016 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 362 |  TrainLoss: 0.00002 | TestLoss: 1.29427 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 363 |  TrainLoss: 0.00001 | TestLoss: 1.29817 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 364 |  TrainLoss: 0.00003 | TestLoss: 1.30232 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 365 |  TrainLoss: 0.00002 | TestLoss: 1.30661 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 366 |  TrainLoss: 0.00003 | TestLoss: 1.31093 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 367 |  TrainLoss: 0.00003 | TestLoss: 1.31520 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 368 |  TrainLoss: 0.00003 | TestLoss: 1.31869 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 369 |  TrainLoss: 0.00002 | TestLoss: 1.32213 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 370 |  TrainLoss: 0.00002 | TestLoss: 1.32522 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 371 |  TrainLoss: 0.00003 | TestLoss: 1.32645 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 372 |  TrainLoss: 0.00004 | TestLoss: 1.32623 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 373 |  TrainLoss: 0.00002 | TestLoss: 1.32523 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 374 |  TrainLoss: 0.00002 | TestLoss: 1.32477 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 375 |  TrainLoss: 0.00002 | TestLoss: 1.32343 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 376 |  TrainLoss: 0.00002 | TestLoss: 1.32259 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 377 |  TrainLoss: 0.00002 | TestLoss: 1.32154 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 378 |  TrainLoss: 0.00002 | TestLoss: 1.32007 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 379 |  TrainLoss: 0.00001 | TestLoss: 1.31897 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 380 |  TrainLoss: 0.00003 | TestLoss: 1.31951 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 381 |  TrainLoss: 0.00003 | TestLoss: 1.31904 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 382 |  TrainLoss: 0.00002 | TestLoss: 1.31920 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 383 |  TrainLoss: 0.00004 | TestLoss: 1.32110 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 384 |  TrainLoss: 0.00002 | TestLoss: 1.32283 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 385 |  TrainLoss: 0.00002 | TestLoss: 1.32493 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 386 |  TrainLoss: 0.00004 | TestLoss: 1.32763 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 387 |  TrainLoss: 0.00003 | TestLoss: 1.32959 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 388 |  TrainLoss: 0.00001 | TestLoss: 1.33122 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 389 |  TrainLoss: 0.00002 | TestLoss: 1.33229 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 390 |  TrainLoss: 0.00003 | TestLoss: 1.33338 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 391 |  TrainLoss: 0.00002 | TestLoss: 1.33385 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 392 |  TrainLoss: 0.00001 | TestLoss: 1.33450 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 393 |  TrainLoss: 0.00003 | TestLoss: 1.33493 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 394 |  TrainLoss: 0.00004 | TestLoss: 1.33521 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 395 |  TrainLoss: 0.00001 | TestLoss: 1.33548 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 396 |  TrainLoss: 0.00002 | TestLoss: 1.33623 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 397 |  TrainLoss: 0.00003 | TestLoss: 1.33669 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 398 |  TrainLoss: 0.00001 | TestLoss: 1.33833 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 399 |  TrainLoss: 0.00003 | TestLoss: 1.34006 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 400 |  TrainLoss: 0.00001 | TestLoss: 1.34194 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 401 |  TrainLoss: 0.00002 | TestLoss: 1.34320 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 402 |  TrainLoss: 0.00002 | TestLoss: 1.34424 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 403 |  TrainLoss: 0.00002 | TestLoss: 1.34477 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 404 |  TrainLoss: 0.00003 | TestLoss: 1.34434 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 405 |  TrainLoss: 0.00002 | TestLoss: 1.34395 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 406 |  TrainLoss: 0.00002 | TestLoss: 1.34308 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 407 |  TrainLoss: 0.00003 | TestLoss: 1.34341 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 408 |  TrainLoss: 0.00003 | TestLoss: 1.34517 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 409 |  TrainLoss: 0.00002 | TestLoss: 1.34749 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 410 |  TrainLoss: 0.00001 | TestLoss: 1.34975 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 411 |  TrainLoss: 0.00005 | TestLoss: 1.35208 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 412 |  TrainLoss: 0.00002 | TestLoss: 1.35366 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 413 |  TrainLoss: 0.00002 | TestLoss: 1.35656 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 414 |  TrainLoss: 0.00003 | TestLoss: 1.35748 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 415 |  TrainLoss: 0.00002 | TestLoss: 1.35752 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 416 |  TrainLoss: 0.00002 | TestLoss: 1.35861 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 417 |  TrainLoss: 0.00004 | TestLoss: 1.36063 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 418 |  TrainLoss: 0.00002 | TestLoss: 1.36113 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 419 |  TrainLoss: 0.00002 | TestLoss: 1.36192 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 420 |  TrainLoss: 0.00002 | TestLoss: 1.36203 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 421 |  TrainLoss: 0.00002 | TestLoss: 1.36258 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 422 |  TrainLoss: 0.00003 | TestLoss: 1.36367 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 423 |  TrainLoss: 0.00002 | TestLoss: 1.36407 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 424 |  TrainLoss: 0.00001 | TestLoss: 1.36427 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 425 |  TrainLoss: 0.00001 | TestLoss: 1.36485 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 426 |  TrainLoss: 0.00001 | TestLoss: 1.36530 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 427 |  TrainLoss: 0.00001 | TestLoss: 1.36547 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 428 |  TrainLoss: 0.00005 | TestLoss: 1.36265 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 429 |  TrainLoss: 0.00003 | TestLoss: 1.35966 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 430 |  TrainLoss: 0.00002 | TestLoss: 1.35808 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 431 |  TrainLoss: 0.00002 | TestLoss: 1.35670 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 432 |  TrainLoss: 0.00001 | TestLoss: 1.35510 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 433 |  TrainLoss: 0.00001 | TestLoss: 1.35353 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 434 |  TrainLoss: 0.00001 | TestLoss: 1.35415 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 435 |  TrainLoss: 0.00001 | TestLoss: 1.35429 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 436 |  TrainLoss: 0.00002 | TestLoss: 1.35442 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 437 |  TrainLoss: 0.00004 | TestLoss: 1.35837 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 438 |  TrainLoss: 0.00002 | TestLoss: 1.36170 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 439 |  TrainLoss: 0.00002 | TestLoss: 1.36446 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 440 |  TrainLoss: 0.00002 | TestLoss: 1.36641 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 441 |  TrainLoss: 0.00003 | TestLoss: 1.36958 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 442 |  TrainLoss: 0.00001 | TestLoss: 1.37287 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 443 |  TrainLoss: 0.00002 | TestLoss: 1.37519 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 444 |  TrainLoss: 0.00001 | TestLoss: 1.37775 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 445 |  TrainLoss: 0.00001 | TestLoss: 1.37916 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 446 |  TrainLoss: 0.00002 | TestLoss: 1.37974 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 447 |  TrainLoss: 0.00001 | TestLoss: 1.37900 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 448 |  TrainLoss: 0.00002 | TestLoss: 1.37835 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 449 |  TrainLoss: 0.00001 | TestLoss: 1.37850 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 450 |  TrainLoss: 0.00002 | TestLoss: 1.37870 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 451 |  TrainLoss: 0.00003 | TestLoss: 1.37883 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 452 |  TrainLoss: 0.00002 | TestLoss: 1.37884 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 453 |  TrainLoss: 0.00001 | TestLoss: 1.37918 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 454 |  TrainLoss: 0.00001 | TestLoss: 1.37945 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 455 |  TrainLoss: 0.00002 | TestLoss: 1.38004 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 456 |  TrainLoss: 0.00003 | TestLoss: 1.38332 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 457 |  TrainLoss: 0.00005 | TestLoss: 1.38916 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 458 |  TrainLoss: 0.00001 | TestLoss: 1.39449 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 459 |  TrainLoss: 0.00002 | TestLoss: 1.39910 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 460 |  TrainLoss: 0.00001 | TestLoss: 1.40349 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 461 |  TrainLoss: 0.00003 | TestLoss: 1.40721 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 462 |  TrainLoss: 0.00001 | TestLoss: 1.41042 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 463 |  TrainLoss: 0.00002 | TestLoss: 1.41170 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 464 |  TrainLoss: 0.00001 | TestLoss: 1.41319 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 465 |  TrainLoss: 0.00002 | TestLoss: 1.41407 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 466 |  TrainLoss: 0.00001 | TestLoss: 1.41459 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 467 |  TrainLoss: 0.00001 | TestLoss: 1.41464 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 468 |  TrainLoss: 0.00002 | TestLoss: 1.41500 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 469 |  TrainLoss: 0.00010 | TestLoss: 1.40412 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 470 |  TrainLoss: 0.00002 | TestLoss: 1.39438 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 471 |  TrainLoss: 0.00002 | TestLoss: 1.38671 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 472 |  TrainLoss: 0.00002 | TestLoss: 1.38060 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 473 |  TrainLoss: 0.00007 | TestLoss: 1.38163 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 474 |  TrainLoss: 0.00001 | TestLoss: 1.38273 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 475 |  TrainLoss: 0.00002 | TestLoss: 1.38468 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 476 |  TrainLoss: 0.00003 | TestLoss: 1.38558 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 477 |  TrainLoss: 0.00001 | TestLoss: 1.38627 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 478 |  TrainLoss: 0.00001 | TestLoss: 1.38725 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 479 |  TrainLoss: 0.00002 | TestLoss: 1.38741 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 480 |  TrainLoss: 0.00003 | TestLoss: 1.38790 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 481 |  TrainLoss: 0.00001 | TestLoss: 1.38825 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 482 |  TrainLoss: 0.00002 | TestLoss: 1.38990 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 483 |  TrainLoss: 0.00002 | TestLoss: 1.39108 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 484 |  TrainLoss: 0.00001 | TestLoss: 1.39163 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 485 |  TrainLoss: 0.00003 | TestLoss: 1.39205 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 486 |  TrainLoss: 0.00001 | TestLoss: 1.39318 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 487 |  TrainLoss: 0.00002 | TestLoss: 1.39351 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 488 |  TrainLoss: 0.00002 | TestLoss: 1.39393 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 489 |  TrainLoss: 0.00001 | TestLoss: 1.39484 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 490 |  TrainLoss: 0.00001 | TestLoss: 1.39509 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 491 |  TrainLoss: 0.00003 | TestLoss: 1.39379 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 492 |  TrainLoss: 0.00002 | TestLoss: 1.39421 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 493 |  TrainLoss: 0.00002 | TestLoss: 1.39514 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 494 |  TrainLoss: 0.00002 | TestLoss: 1.39637 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 495 |  TrainLoss: 0.00001 | TestLoss: 1.39694 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 496 |  TrainLoss: 0.00001 | TestLoss: 1.39759 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 497 |  TrainLoss: 0.00001 | TestLoss: 1.39866 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 498 |  TrainLoss: 0.00001 | TestLoss: 1.40174 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 499 |  TrainLoss: 0.00001 | TestLoss: 1.40428 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Best WLoss: 0.00204 | Best Epoch: 59\n"
     ]
    }
   ],
   "source": [
    "wloss = []\n",
    "weighted_loss = 0\n",
    "exp_param = 0.8\n",
    "best_test_loss = float('inf')  # Initialize with a large value\n",
    "\n",
    "# Without dropout training results\n",
    "for epoch in range(500):\n",
    "  train_loss = train(epoch)\n",
    "  test_loss, test_acc, test_f1 = test(epoch)\n",
    "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
    "  \n",
    "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
    "  \n",
    "  if test_loss < best_test_loss:\n",
    "    best_test_loss = test_loss  # Update the best test loss\n",
    "\n",
    "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
    "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
    "\n",
    "# Print the best values\n",
    "best_wloss = min(wloss)\n",
    "best_epoch = wloss.index(best_wloss)\n",
    "print(f'Best WLoss: {best_wloss:.5f} | Best Epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54747d4c",
   "metadata": {},
   "source": [
    "# Model 3: \n",
    "GraphSage + Bert Embeddings with 2 Layer MLP as defined by the paper. (This is the model implemented in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8d253d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, embedding_size, batch_size, l2_reg_weight):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.batch_size = batch_size\n",
    "        self.l2_reg_weight = l2_reg_weight\n",
    "        \n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
    "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
    "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
    "        \n",
    "        self.full1 = Linear(hidden_channels[2], hidden_channels[3])\n",
    "        self.full2 = Linear(hidden_channels[3], hidden_channels[4])\n",
    "        self.softmax = Linear(hidden_channels[4], out_channels)\n",
    "\n",
    "        # Dropouts\n",
    "        self.dp1 = Dropout(0.2)\n",
    "        self.dp2 = Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = self.conv1(x, edge_index).relu()\n",
    "        h = self.conv2(h, edge_index).relu()\n",
    "        h = self.conv3(h, edge_index).relu()\n",
    "\n",
    "        h = global_max_pool(h, batch)\n",
    "\n",
    "        h = self.full1(h).relu()\n",
    "        h = self.dp1(h)\n",
    "        h = self.full2(h).relu()\n",
    "        h = self.dp2(h)\n",
    "        \n",
    "        h = self.softmax(h)\n",
    "\n",
    "        return torch.sigmoid(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39e7fc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(test_data_pol.num_features,[512,512,512,256,256,256],1, 128, 128, 0.001).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "lossff = torch.nn.BCELoss()\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "132ce3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        all_preds.append(torch.reshape(out, (-1,)))\n",
    "        all_labels.append(data.y.float())\n",
    "    # print(all_preds)\n",
    "    accuracy, f1 = metrics(all_preds, all_labels)\n",
    "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
    "\n",
    "\n",
    "def metrics(preds, gts):\n",
    "    preds = torch.round(torch.cat(preds))\n",
    "    gts = torch.cat(gts)\n",
    "    # print(preds.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0307c1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 |  TrainLoss: 0.69671 | TestLoss: 0.69282 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 01 |  TrainLoss: 0.69413 | TestLoss: 0.69280 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 02 |  TrainLoss: 0.69507 | TestLoss: 0.69285 | TestAcc: 0.57014 | TestF1: 0.69\n",
      "Epoch: 03 |  TrainLoss: 0.69253 | TestLoss: 0.69307 | TestAcc: 0.49774 | TestF1: 0.03\n",
      "Epoch: 04 |  TrainLoss: 0.69297 | TestLoss: 0.69333 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 05 |  TrainLoss: 0.68861 | TestLoss: 0.69350 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 06 |  TrainLoss: 0.68912 | TestLoss: 0.69365 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 07 |  TrainLoss: 0.68609 | TestLoss: 0.69376 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 08 |  TrainLoss: 0.68814 | TestLoss: 0.69375 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 09 |  TrainLoss: 0.69032 | TestLoss: 0.69369 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 10 |  TrainLoss: 0.69106 | TestLoss: 0.69328 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 11 |  TrainLoss: 0.68663 | TestLoss: 0.69253 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 12 |  TrainLoss: 0.68456 | TestLoss: 0.69158 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 13 |  TrainLoss: 0.68396 | TestLoss: 0.69030 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 14 |  TrainLoss: 0.67797 | TestLoss: 0.68900 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 15 |  TrainLoss: 0.68028 | TestLoss: 0.68746 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 16 |  TrainLoss: 0.67849 | TestLoss: 0.68578 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 17 |  TrainLoss: 0.67797 | TestLoss: 0.68406 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 18 |  TrainLoss: 0.67057 | TestLoss: 0.68237 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 19 |  TrainLoss: 0.67342 | TestLoss: 0.68046 | TestAcc: 0.49321 | TestF1: 0.02\n",
      "Epoch: 20 |  TrainLoss: 0.66777 | TestLoss: 0.67817 | TestAcc: 0.50226 | TestF1: 0.05\n",
      "Epoch: 21 |  TrainLoss: 0.66631 | TestLoss: 0.67522 | TestAcc: 0.52489 | TestF1: 0.13\n",
      "Epoch: 22 |  TrainLoss: 0.66184 | TestLoss: 0.67184 | TestAcc: 0.56561 | TestF1: 0.26\n",
      "Epoch: 23 |  TrainLoss: 0.65483 | TestLoss: 0.66789 | TestAcc: 0.59276 | TestF1: 0.35\n",
      "Epoch: 24 |  TrainLoss: 0.64850 | TestLoss: 0.66344 | TestAcc: 0.64253 | TestF1: 0.48\n",
      "Epoch: 25 |  TrainLoss: 0.64415 | TestLoss: 0.65832 | TestAcc: 0.66516 | TestF1: 0.53\n",
      "Epoch: 26 |  TrainLoss: 0.63985 | TestLoss: 0.65247 | TestAcc: 0.69683 | TestF1: 0.60\n",
      "Epoch: 27 |  TrainLoss: 0.62846 | TestLoss: 0.64631 | TestAcc: 0.70588 | TestF1: 0.63\n",
      "Epoch: 28 |  TrainLoss: 0.62406 | TestLoss: 0.63959 | TestAcc: 0.70588 | TestF1: 0.63\n",
      "Epoch: 29 |  TrainLoss: 0.61153 | TestLoss: 0.63233 | TestAcc: 0.68326 | TestF1: 0.59\n",
      "Epoch: 30 |  TrainLoss: 0.60725 | TestLoss: 0.62335 | TestAcc: 0.72398 | TestF1: 0.66\n",
      "Epoch: 31 |  TrainLoss: 0.58644 | TestLoss: 0.61411 | TestAcc: 0.75113 | TestF1: 0.71\n",
      "Epoch: 32 |  TrainLoss: 0.57927 | TestLoss: 0.60384 | TestAcc: 0.77376 | TestF1: 0.74\n",
      "Epoch: 33 |  TrainLoss: 0.56446 | TestLoss: 0.59254 | TestAcc: 0.78733 | TestF1: 0.76\n",
      "Epoch: 34 |  TrainLoss: 0.54316 | TestLoss: 0.58154 | TestAcc: 0.78281 | TestF1: 0.76\n",
      "Epoch: 35 |  TrainLoss: 0.52177 | TestLoss: 0.57014 | TestAcc: 0.78281 | TestF1: 0.76\n",
      "Epoch: 36 |  TrainLoss: 0.50841 | TestLoss: 0.55763 | TestAcc: 0.78281 | TestF1: 0.76\n",
      "Epoch: 37 |  TrainLoss: 0.48587 | TestLoss: 0.54138 | TestAcc: 0.80995 | TestF1: 0.79\n",
      "Epoch: 38 |  TrainLoss: 0.46575 | TestLoss: 0.52808 | TestAcc: 0.80543 | TestF1: 0.79\n",
      "Epoch: 39 |  TrainLoss: 0.43821 | TestLoss: 0.51409 | TestAcc: 0.80543 | TestF1: 0.79\n",
      "Epoch: 40 |  TrainLoss: 0.42535 | TestLoss: 0.49734 | TestAcc: 0.80995 | TestF1: 0.80\n",
      "Epoch: 41 |  TrainLoss: 0.39122 | TestLoss: 0.48107 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 42 |  TrainLoss: 0.36029 | TestLoss: 0.46933 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 43 |  TrainLoss: 0.33403 | TestLoss: 0.45745 | TestAcc: 0.80995 | TestF1: 0.79\n",
      "Epoch: 44 |  TrainLoss: 0.30294 | TestLoss: 0.43803 | TestAcc: 0.81900 | TestF1: 0.81\n",
      "Epoch: 45 |  TrainLoss: 0.27660 | TestLoss: 0.42565 | TestAcc: 0.83258 | TestF1: 0.83\n",
      "Epoch: 46 |  TrainLoss: 0.25897 | TestLoss: 0.42407 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 47 |  TrainLoss: 0.22448 | TestLoss: 0.41869 | TestAcc: 0.81900 | TestF1: 0.81\n",
      "Epoch: 48 |  TrainLoss: 0.20469 | TestLoss: 0.40625 | TestAcc: 0.83258 | TestF1: 0.83\n",
      "Epoch: 49 |  TrainLoss: 0.17191 | TestLoss: 0.40592 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 50 |  TrainLoss: 0.16198 | TestLoss: 0.40994 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 51 |  TrainLoss: 0.13308 | TestLoss: 0.42576 | TestAcc: 0.84163 | TestF1: 0.83\n",
      "Epoch: 52 |  TrainLoss: 0.12190 | TestLoss: 0.41048 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 53 |  TrainLoss: 0.10395 | TestLoss: 0.42924 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 54 |  TrainLoss: 0.08369 | TestLoss: 0.47046 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 55 |  TrainLoss: 0.07792 | TestLoss: 0.45252 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 56 |  TrainLoss: 0.06143 | TestLoss: 0.44763 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 57 |  TrainLoss: 0.06043 | TestLoss: 0.48279 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 58 |  TrainLoss: 0.04427 | TestLoss: 0.55581 | TestAcc: 0.80995 | TestF1: 0.79\n",
      "Epoch: 59 |  TrainLoss: 0.03968 | TestLoss: 0.53422 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 60 |  TrainLoss: 0.04036 | TestLoss: 0.49966 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 61 |  TrainLoss: 0.02637 | TestLoss: 0.51495 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 62 |  TrainLoss: 0.04106 | TestLoss: 0.54255 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 63 |  TrainLoss: 0.01435 | TestLoss: 0.63614 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 64 |  TrainLoss: 0.02922 | TestLoss: 0.63783 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 65 |  TrainLoss: 0.02429 | TestLoss: 0.58229 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 66 |  TrainLoss: 0.01043 | TestLoss: 0.57214 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 67 |  TrainLoss: 0.01815 | TestLoss: 0.58474 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 68 |  TrainLoss: 0.01776 | TestLoss: 0.62548 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 69 |  TrainLoss: 0.00681 | TestLoss: 0.69830 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 70 |  TrainLoss: 0.00890 | TestLoss: 0.75174 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 71 |  TrainLoss: 0.01606 | TestLoss: 0.72164 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 72 |  TrainLoss: 0.00683 | TestLoss: 0.68018 | TestAcc: 0.84163 | TestF1: 0.83\n",
      "Epoch: 73 |  TrainLoss: 0.00351 | TestLoss: 0.65597 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 74 |  TrainLoss: 0.00385 | TestLoss: 0.65189 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 75 |  TrainLoss: 0.00713 | TestLoss: 0.65928 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 76 |  TrainLoss: 0.00507 | TestLoss: 0.67045 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 77 |  TrainLoss: 0.00412 | TestLoss: 0.69026 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 78 |  TrainLoss: 0.00273 | TestLoss: 0.72170 | TestAcc: 0.83258 | TestF1: 0.83\n",
      "Epoch: 79 |  TrainLoss: 0.00202 | TestLoss: 0.75742 | TestAcc: 0.84163 | TestF1: 0.83\n",
      "Epoch: 80 |  TrainLoss: 0.00177 | TestLoss: 0.79067 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 81 |  TrainLoss: 0.00240 | TestLoss: 0.81433 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 82 |  TrainLoss: 0.00261 | TestLoss: 0.82110 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 83 |  TrainLoss: 0.00268 | TestLoss: 0.81191 | TestAcc: 0.84163 | TestF1: 0.83\n",
      "Epoch: 84 |  TrainLoss: 0.00197 | TestLoss: 0.79766 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 85 |  TrainLoss: 0.00176 | TestLoss: 0.78153 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 86 |  TrainLoss: 0.00113 | TestLoss: 0.76822 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 87 |  TrainLoss: 0.00159 | TestLoss: 0.75931 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 88 |  TrainLoss: 0.00132 | TestLoss: 0.75518 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 89 |  TrainLoss: 0.00155 | TestLoss: 0.75519 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 90 |  TrainLoss: 0.00111 | TestLoss: 0.75758 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 91 |  TrainLoss: 0.00126 | TestLoss: 0.76228 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 92 |  TrainLoss: 0.00132 | TestLoss: 0.76942 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 93 |  TrainLoss: 0.00099 | TestLoss: 0.77754 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 94 |  TrainLoss: 0.00093 | TestLoss: 0.78741 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 95 |  TrainLoss: 0.00062 | TestLoss: 0.79802 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 96 |  TrainLoss: 0.00068 | TestLoss: 0.80930 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 97 |  TrainLoss: 0.00077 | TestLoss: 0.82028 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 98 |  TrainLoss: 0.00067 | TestLoss: 0.82985 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 99 |  TrainLoss: 0.00063 | TestLoss: 0.83785 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 100 |  TrainLoss: 0.00097 | TestLoss: 0.84488 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 101 |  TrainLoss: 0.00079 | TestLoss: 0.84943 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 102 |  TrainLoss: 0.00075 | TestLoss: 0.85062 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 103 |  TrainLoss: 0.00064 | TestLoss: 0.85162 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 104 |  TrainLoss: 0.00110 | TestLoss: 0.84924 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 105 |  TrainLoss: 0.00056 | TestLoss: 0.84733 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 106 |  TrainLoss: 0.00068 | TestLoss: 0.84420 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 107 |  TrainLoss: 0.00053 | TestLoss: 0.84103 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 108 |  TrainLoss: 0.00077 | TestLoss: 0.83743 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 109 |  TrainLoss: 0.00073 | TestLoss: 0.83409 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 110 |  TrainLoss: 0.00055 | TestLoss: 0.83186 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 111 |  TrainLoss: 0.00054 | TestLoss: 0.83011 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 112 |  TrainLoss: 0.00074 | TestLoss: 0.82967 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 113 |  TrainLoss: 0.00063 | TestLoss: 0.83016 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 114 |  TrainLoss: 0.00075 | TestLoss: 0.83354 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 115 |  TrainLoss: 0.00052 | TestLoss: 0.83680 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 116 |  TrainLoss: 0.00050 | TestLoss: 0.84087 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 117 |  TrainLoss: 0.00048 | TestLoss: 0.84532 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 118 |  TrainLoss: 0.00056 | TestLoss: 0.85003 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 119 |  TrainLoss: 0.00062 | TestLoss: 0.85285 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 120 |  TrainLoss: 0.00056 | TestLoss: 0.85505 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 121 |  TrainLoss: 0.00048 | TestLoss: 0.85769 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 122 |  TrainLoss: 0.00039 | TestLoss: 0.86056 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 123 |  TrainLoss: 0.00059 | TestLoss: 0.86290 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 124 |  TrainLoss: 0.00040 | TestLoss: 0.86527 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 125 |  TrainLoss: 0.00045 | TestLoss: 0.86715 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 126 |  TrainLoss: 0.00041 | TestLoss: 0.86890 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 127 |  TrainLoss: 0.00062 | TestLoss: 0.86921 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 128 |  TrainLoss: 0.00051 | TestLoss: 0.86955 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 129 |  TrainLoss: 0.00041 | TestLoss: 0.86948 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 130 |  TrainLoss: 0.00045 | TestLoss: 0.87025 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 131 |  TrainLoss: 0.00038 | TestLoss: 0.87109 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 132 |  TrainLoss: 0.00040 | TestLoss: 0.87282 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 133 |  TrainLoss: 0.00037 | TestLoss: 0.87450 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 134 |  TrainLoss: 0.00032 | TestLoss: 0.87569 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 135 |  TrainLoss: 0.00042 | TestLoss: 0.87718 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 136 |  TrainLoss: 0.00041 | TestLoss: 0.87949 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 137 |  TrainLoss: 0.00040 | TestLoss: 0.88294 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 138 |  TrainLoss: 0.00031 | TestLoss: 0.88634 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 139 |  TrainLoss: 0.00034 | TestLoss: 0.88896 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 140 |  TrainLoss: 0.00036 | TestLoss: 0.89103 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 141 |  TrainLoss: 0.00025 | TestLoss: 0.89282 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 142 |  TrainLoss: 0.00027 | TestLoss: 0.89471 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 143 |  TrainLoss: 0.00034 | TestLoss: 0.89583 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 144 |  TrainLoss: 0.00036 | TestLoss: 0.89708 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 145 |  TrainLoss: 0.00026 | TestLoss: 0.89851 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 146 |  TrainLoss: 0.00034 | TestLoss: 0.90026 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 147 |  TrainLoss: 0.00033 | TestLoss: 0.90140 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 148 |  TrainLoss: 0.00029 | TestLoss: 0.90289 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 149 |  TrainLoss: 0.00033 | TestLoss: 0.90405 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 150 |  TrainLoss: 0.00053 | TestLoss: 0.90429 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 151 |  TrainLoss: 0.00044 | TestLoss: 0.90586 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 152 |  TrainLoss: 0.00031 | TestLoss: 0.90696 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 153 |  TrainLoss: 0.00035 | TestLoss: 0.90907 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 154 |  TrainLoss: 0.00026 | TestLoss: 0.91106 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 155 |  TrainLoss: 0.00030 | TestLoss: 0.91292 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 156 |  TrainLoss: 0.00047 | TestLoss: 0.91475 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 157 |  TrainLoss: 0.00027 | TestLoss: 0.91677 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 158 |  TrainLoss: 0.00021 | TestLoss: 0.91866 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 159 |  TrainLoss: 0.00029 | TestLoss: 0.91966 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 160 |  TrainLoss: 0.00021 | TestLoss: 0.92043 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 161 |  TrainLoss: 0.00031 | TestLoss: 0.91992 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 162 |  TrainLoss: 0.00039 | TestLoss: 0.91960 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 163 |  TrainLoss: 0.00030 | TestLoss: 0.91967 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 164 |  TrainLoss: 0.00024 | TestLoss: 0.91996 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 165 |  TrainLoss: 0.00033 | TestLoss: 0.91932 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 166 |  TrainLoss: 0.00025 | TestLoss: 0.91882 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 167 |  TrainLoss: 0.00023 | TestLoss: 0.91773 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 168 |  TrainLoss: 0.00025 | TestLoss: 0.91758 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 169 |  TrainLoss: 0.00018 | TestLoss: 0.91779 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 170 |  TrainLoss: 0.00021 | TestLoss: 0.91775 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 171 |  TrainLoss: 0.00027 | TestLoss: 0.91879 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 172 |  TrainLoss: 0.00038 | TestLoss: 0.92126 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 173 |  TrainLoss: 0.00026 | TestLoss: 0.92401 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 174 |  TrainLoss: 0.00040 | TestLoss: 0.92900 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 175 |  TrainLoss: 0.00030 | TestLoss: 0.93497 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 176 |  TrainLoss: 0.00030 | TestLoss: 0.94006 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 177 |  TrainLoss: 0.00034 | TestLoss: 0.94516 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 178 |  TrainLoss: 0.00031 | TestLoss: 0.94947 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 179 |  TrainLoss: 0.00032 | TestLoss: 0.95356 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 180 |  TrainLoss: 0.00027 | TestLoss: 0.95551 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 181 |  TrainLoss: 0.00026 | TestLoss: 0.95672 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 182 |  TrainLoss: 0.00026 | TestLoss: 0.95684 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 183 |  TrainLoss: 0.00022 | TestLoss: 0.95687 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 184 |  TrainLoss: 0.00029 | TestLoss: 0.95637 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 185 |  TrainLoss: 0.00023 | TestLoss: 0.95551 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 186 |  TrainLoss: 0.00019 | TestLoss: 0.95389 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 187 |  TrainLoss: 0.00026 | TestLoss: 0.95242 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 188 |  TrainLoss: 0.00021 | TestLoss: 0.95102 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 189 |  TrainLoss: 0.00030 | TestLoss: 0.94914 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 190 |  TrainLoss: 0.00027 | TestLoss: 0.94954 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 191 |  TrainLoss: 0.00023 | TestLoss: 0.95046 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 192 |  TrainLoss: 0.00022 | TestLoss: 0.95185 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 193 |  TrainLoss: 0.00016 | TestLoss: 0.95314 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 194 |  TrainLoss: 0.00026 | TestLoss: 0.95546 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 195 |  TrainLoss: 0.00020 | TestLoss: 0.95785 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 196 |  TrainLoss: 0.00022 | TestLoss: 0.96006 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 197 |  TrainLoss: 0.00019 | TestLoss: 0.96189 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 198 |  TrainLoss: 0.00018 | TestLoss: 0.96327 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 199 |  TrainLoss: 0.00023 | TestLoss: 0.96508 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 200 |  TrainLoss: 0.00015 | TestLoss: 0.96701 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 201 |  TrainLoss: 0.00014 | TestLoss: 0.96859 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 202 |  TrainLoss: 0.00018 | TestLoss: 0.96963 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 203 |  TrainLoss: 0.00015 | TestLoss: 0.97104 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 204 |  TrainLoss: 0.00014 | TestLoss: 0.97223 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 205 |  TrainLoss: 0.00021 | TestLoss: 0.97377 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 206 |  TrainLoss: 0.00022 | TestLoss: 0.97669 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 207 |  TrainLoss: 0.00022 | TestLoss: 0.97780 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 208 |  TrainLoss: 0.00020 | TestLoss: 0.97764 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 209 |  TrainLoss: 0.00017 | TestLoss: 0.97681 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 210 |  TrainLoss: 0.00013 | TestLoss: 0.97535 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 211 |  TrainLoss: 0.00022 | TestLoss: 0.97490 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 212 |  TrainLoss: 0.00019 | TestLoss: 0.97484 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 213 |  TrainLoss: 0.00017 | TestLoss: 0.97454 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 214 |  TrainLoss: 0.00014 | TestLoss: 0.97431 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 215 |  TrainLoss: 0.00018 | TestLoss: 0.97535 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 216 |  TrainLoss: 0.00017 | TestLoss: 0.97683 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 217 |  TrainLoss: 0.00019 | TestLoss: 0.97839 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 218 |  TrainLoss: 0.00020 | TestLoss: 0.97985 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 219 |  TrainLoss: 0.00012 | TestLoss: 0.98115 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 220 |  TrainLoss: 0.00017 | TestLoss: 0.98241 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 221 |  TrainLoss: 0.00011 | TestLoss: 0.98384 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 222 |  TrainLoss: 0.00017 | TestLoss: 0.98417 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 223 |  TrainLoss: 0.00018 | TestLoss: 0.98557 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 224 |  TrainLoss: 0.00019 | TestLoss: 0.98752 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 225 |  TrainLoss: 0.00013 | TestLoss: 0.98887 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 226 |  TrainLoss: 0.00014 | TestLoss: 0.98983 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 227 |  TrainLoss: 0.00011 | TestLoss: 0.99119 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 228 |  TrainLoss: 0.00014 | TestLoss: 0.99324 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 229 |  TrainLoss: 0.00014 | TestLoss: 0.99448 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 230 |  TrainLoss: 0.00022 | TestLoss: 0.99633 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 231 |  TrainLoss: 0.00013 | TestLoss: 0.99733 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 232 |  TrainLoss: 0.00015 | TestLoss: 0.99950 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 233 |  TrainLoss: 0.00025 | TestLoss: 1.00373 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 234 |  TrainLoss: 0.00017 | TestLoss: 1.00791 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 235 |  TrainLoss: 0.00011 | TestLoss: 1.01115 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 236 |  TrainLoss: 0.00009 | TestLoss: 1.01469 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 237 |  TrainLoss: 0.00014 | TestLoss: 1.01762 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 238 |  TrainLoss: 0.00020 | TestLoss: 1.02016 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 239 |  TrainLoss: 0.00012 | TestLoss: 1.02236 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 240 |  TrainLoss: 0.00016 | TestLoss: 1.02384 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 241 |  TrainLoss: 0.00012 | TestLoss: 1.02496 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 242 |  TrainLoss: 0.00016 | TestLoss: 1.02540 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 243 |  TrainLoss: 0.00012 | TestLoss: 1.02524 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 244 |  TrainLoss: 0.00014 | TestLoss: 1.02477 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 245 |  TrainLoss: 0.00019 | TestLoss: 1.02365 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 246 |  TrainLoss: 0.00010 | TestLoss: 1.02282 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 247 |  TrainLoss: 0.00010 | TestLoss: 1.02199 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 248 |  TrainLoss: 0.00015 | TestLoss: 1.02160 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 249 |  TrainLoss: 0.00012 | TestLoss: 1.02132 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 250 |  TrainLoss: 0.00010 | TestLoss: 1.02121 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 251 |  TrainLoss: 0.00011 | TestLoss: 1.02164 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 252 |  TrainLoss: 0.00012 | TestLoss: 1.02115 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 253 |  TrainLoss: 0.00016 | TestLoss: 1.02145 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 254 |  TrainLoss: 0.00019 | TestLoss: 1.01997 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 255 |  TrainLoss: 0.00009 | TestLoss: 1.01837 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 256 |  TrainLoss: 0.00010 | TestLoss: 1.01675 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 257 |  TrainLoss: 0.00008 | TestLoss: 1.01493 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 258 |  TrainLoss: 0.00011 | TestLoss: 1.01310 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 259 |  TrainLoss: 0.00013 | TestLoss: 1.01399 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 260 |  TrainLoss: 0.00008 | TestLoss: 1.01511 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 261 |  TrainLoss: 0.00012 | TestLoss: 1.01718 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 262 |  TrainLoss: 0.00009 | TestLoss: 1.01916 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 263 |  TrainLoss: 0.00010 | TestLoss: 1.02245 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 264 |  TrainLoss: 0.00006 | TestLoss: 1.02560 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 265 |  TrainLoss: 0.00014 | TestLoss: 1.02966 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 266 |  TrainLoss: 0.00008 | TestLoss: 1.03331 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 267 |  TrainLoss: 0.00010 | TestLoss: 1.03747 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 268 |  TrainLoss: 0.00011 | TestLoss: 1.04196 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 269 |  TrainLoss: 0.00008 | TestLoss: 1.04582 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 270 |  TrainLoss: 0.00011 | TestLoss: 1.04806 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 271 |  TrainLoss: 0.00011 | TestLoss: 1.04918 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 272 |  TrainLoss: 0.00008 | TestLoss: 1.04988 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 273 |  TrainLoss: 0.00009 | TestLoss: 1.05054 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 274 |  TrainLoss: 0.00009 | TestLoss: 1.05096 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 275 |  TrainLoss: 0.00007 | TestLoss: 1.05123 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 276 |  TrainLoss: 0.00011 | TestLoss: 1.05212 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 277 |  TrainLoss: 0.00008 | TestLoss: 1.05286 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 278 |  TrainLoss: 0.00010 | TestLoss: 1.05308 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 279 |  TrainLoss: 0.00006 | TestLoss: 1.05354 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 280 |  TrainLoss: 0.00008 | TestLoss: 1.05470 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 281 |  TrainLoss: 0.00008 | TestLoss: 1.05650 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 282 |  TrainLoss: 0.00009 | TestLoss: 1.05793 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 283 |  TrainLoss: 0.00007 | TestLoss: 1.05954 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 284 |  TrainLoss: 0.00008 | TestLoss: 1.06027 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 285 |  TrainLoss: 0.00007 | TestLoss: 1.06033 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 286 |  TrainLoss: 0.00005 | TestLoss: 1.06012 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 287 |  TrainLoss: 0.00007 | TestLoss: 1.06071 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 288 |  TrainLoss: 0.00006 | TestLoss: 1.06115 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 289 |  TrainLoss: 0.00008 | TestLoss: 1.06290 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 290 |  TrainLoss: 0.00009 | TestLoss: 1.06458 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 291 |  TrainLoss: 0.00007 | TestLoss: 1.06636 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 292 |  TrainLoss: 0.00008 | TestLoss: 1.06871 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 293 |  TrainLoss: 0.00006 | TestLoss: 1.07050 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 294 |  TrainLoss: 0.00007 | TestLoss: 1.07193 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 295 |  TrainLoss: 0.00006 | TestLoss: 1.07383 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 296 |  TrainLoss: 0.00012 | TestLoss: 1.07605 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 297 |  TrainLoss: 0.00006 | TestLoss: 1.07789 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 298 |  TrainLoss: 0.00006 | TestLoss: 1.07931 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 299 |  TrainLoss: 0.00009 | TestLoss: 1.07925 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 300 |  TrainLoss: 0.00009 | TestLoss: 1.08037 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 301 |  TrainLoss: 0.00006 | TestLoss: 1.08128 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 302 |  TrainLoss: 0.00008 | TestLoss: 1.08157 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 303 |  TrainLoss: 0.00006 | TestLoss: 1.08219 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 304 |  TrainLoss: 0.00006 | TestLoss: 1.08211 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 305 |  TrainLoss: 0.00004 | TestLoss: 1.08277 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 306 |  TrainLoss: 0.00007 | TestLoss: 1.08307 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 307 |  TrainLoss: 0.00006 | TestLoss: 1.08326 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 308 |  TrainLoss: 0.00005 | TestLoss: 1.08452 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 309 |  TrainLoss: 0.00004 | TestLoss: 1.08569 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 310 |  TrainLoss: 0.00004 | TestLoss: 1.08712 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 311 |  TrainLoss: 0.00005 | TestLoss: 1.08895 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 312 |  TrainLoss: 0.00007 | TestLoss: 1.09080 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 313 |  TrainLoss: 0.00005 | TestLoss: 1.09159 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 314 |  TrainLoss: 0.00006 | TestLoss: 1.09189 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 315 |  TrainLoss: 0.00004 | TestLoss: 1.09231 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 316 |  TrainLoss: 0.00004 | TestLoss: 1.09377 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 317 |  TrainLoss: 0.00006 | TestLoss: 1.09504 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 318 |  TrainLoss: 0.00005 | TestLoss: 1.09578 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 319 |  TrainLoss: 0.00006 | TestLoss: 1.09700 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 320 |  TrainLoss: 0.00006 | TestLoss: 1.09950 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 321 |  TrainLoss: 0.00005 | TestLoss: 1.10221 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 322 |  TrainLoss: 0.00004 | TestLoss: 1.10498 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 323 |  TrainLoss: 0.00005 | TestLoss: 1.10727 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 324 |  TrainLoss: 0.00004 | TestLoss: 1.10875 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 325 |  TrainLoss: 0.00004 | TestLoss: 1.10995 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 326 |  TrainLoss: 0.00005 | TestLoss: 1.11191 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 327 |  TrainLoss: 0.00005 | TestLoss: 1.11519 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 328 |  TrainLoss: 0.00005 | TestLoss: 1.11649 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 329 |  TrainLoss: 0.00004 | TestLoss: 1.11745 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 330 |  TrainLoss: 0.00008 | TestLoss: 1.11876 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 331 |  TrainLoss: 0.00005 | TestLoss: 1.12120 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 332 |  TrainLoss: 0.00004 | TestLoss: 1.12265 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 333 |  TrainLoss: 0.00005 | TestLoss: 1.12489 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 334 |  TrainLoss: 0.00005 | TestLoss: 1.12661 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 335 |  TrainLoss: 0.00005 | TestLoss: 1.12931 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 336 |  TrainLoss: 0.00003 | TestLoss: 1.13104 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 337 |  TrainLoss: 0.00005 | TestLoss: 1.13289 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 338 |  TrainLoss: 0.00013 | TestLoss: 1.13035 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 339 |  TrainLoss: 0.00004 | TestLoss: 1.12821 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 340 |  TrainLoss: 0.00004 | TestLoss: 1.12685 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 341 |  TrainLoss: 0.00005 | TestLoss: 1.12472 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 342 |  TrainLoss: 0.00003 | TestLoss: 1.12311 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 343 |  TrainLoss: 0.00005 | TestLoss: 1.12129 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 344 |  TrainLoss: 0.00005 | TestLoss: 1.11942 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 345 |  TrainLoss: 0.00005 | TestLoss: 1.12019 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 346 |  TrainLoss: 0.00005 | TestLoss: 1.12129 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 347 |  TrainLoss: 0.00004 | TestLoss: 1.12328 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 348 |  TrainLoss: 0.00005 | TestLoss: 1.12633 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 349 |  TrainLoss: 0.00004 | TestLoss: 1.12893 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 350 |  TrainLoss: 0.00004 | TestLoss: 1.13126 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 351 |  TrainLoss: 0.00007 | TestLoss: 1.13543 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 352 |  TrainLoss: 0.00004 | TestLoss: 1.13968 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 353 |  TrainLoss: 0.00005 | TestLoss: 1.14431 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 354 |  TrainLoss: 0.00004 | TestLoss: 1.14877 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 355 |  TrainLoss: 0.00003 | TestLoss: 1.15274 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 356 |  TrainLoss: 0.00003 | TestLoss: 1.15591 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 357 |  TrainLoss: 0.00003 | TestLoss: 1.15867 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 358 |  TrainLoss: 0.00004 | TestLoss: 1.16079 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 359 |  TrainLoss: 0.00002 | TestLoss: 1.16294 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 360 |  TrainLoss: 0.00003 | TestLoss: 1.16461 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 361 |  TrainLoss: 0.00005 | TestLoss: 1.16426 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 362 |  TrainLoss: 0.00003 | TestLoss: 1.16360 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 363 |  TrainLoss: 0.00005 | TestLoss: 1.16357 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 364 |  TrainLoss: 0.00004 | TestLoss: 1.16254 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 365 |  TrainLoss: 0.00003 | TestLoss: 1.16152 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 366 |  TrainLoss: 0.00004 | TestLoss: 1.15927 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 367 |  TrainLoss: 0.00004 | TestLoss: 1.15777 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 368 |  TrainLoss: 0.00005 | TestLoss: 1.15586 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 369 |  TrainLoss: 0.00003 | TestLoss: 1.15570 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 370 |  TrainLoss: 0.00003 | TestLoss: 1.15505 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 371 |  TrainLoss: 0.00003 | TestLoss: 1.15447 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 372 |  TrainLoss: 0.00002 | TestLoss: 1.15381 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 373 |  TrainLoss: 0.00006 | TestLoss: 1.15520 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 374 |  TrainLoss: 0.00005 | TestLoss: 1.15574 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 375 |  TrainLoss: 0.00003 | TestLoss: 1.15659 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 376 |  TrainLoss: 0.00003 | TestLoss: 1.15795 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 377 |  TrainLoss: 0.00003 | TestLoss: 1.15957 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 378 |  TrainLoss: 0.00002 | TestLoss: 1.16149 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 379 |  TrainLoss: 0.00005 | TestLoss: 1.16207 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 380 |  TrainLoss: 0.00002 | TestLoss: 1.16225 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 381 |  TrainLoss: 0.00003 | TestLoss: 1.16366 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 382 |  TrainLoss: 0.00002 | TestLoss: 1.16449 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 383 |  TrainLoss: 0.00004 | TestLoss: 1.16739 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 384 |  TrainLoss: 0.00004 | TestLoss: 1.16993 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 385 |  TrainLoss: 0.00003 | TestLoss: 1.17283 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 386 |  TrainLoss: 0.00003 | TestLoss: 1.17498 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 387 |  TrainLoss: 0.00002 | TestLoss: 1.17698 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 388 |  TrainLoss: 0.00003 | TestLoss: 1.17854 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 389 |  TrainLoss: 0.00003 | TestLoss: 1.18000 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 390 |  TrainLoss: 0.00002 | TestLoss: 1.18093 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 391 |  TrainLoss: 0.00003 | TestLoss: 1.18134 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 392 |  TrainLoss: 0.00003 | TestLoss: 1.18212 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 393 |  TrainLoss: 0.00002 | TestLoss: 1.18241 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 394 |  TrainLoss: 0.00002 | TestLoss: 1.18233 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 395 |  TrainLoss: 0.00003 | TestLoss: 1.18222 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 396 |  TrainLoss: 0.00002 | TestLoss: 1.18158 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 397 |  TrainLoss: 0.00001 | TestLoss: 1.18142 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 398 |  TrainLoss: 0.00002 | TestLoss: 1.18103 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 399 |  TrainLoss: 0.00002 | TestLoss: 1.18075 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 400 |  TrainLoss: 0.00002 | TestLoss: 1.18071 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 401 |  TrainLoss: 0.00003 | TestLoss: 1.18205 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 402 |  TrainLoss: 0.00002 | TestLoss: 1.18355 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 403 |  TrainLoss: 0.00004 | TestLoss: 1.18578 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 404 |  TrainLoss: 0.00002 | TestLoss: 1.18769 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 405 |  TrainLoss: 0.00003 | TestLoss: 1.18949 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 406 |  TrainLoss: 0.00002 | TestLoss: 1.19077 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 407 |  TrainLoss: 0.00002 | TestLoss: 1.19183 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 408 |  TrainLoss: 0.00002 | TestLoss: 1.19435 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 409 |  TrainLoss: 0.00003 | TestLoss: 1.19609 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 410 |  TrainLoss: 0.00002 | TestLoss: 1.19794 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 411 |  TrainLoss: 0.00002 | TestLoss: 1.19940 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 412 |  TrainLoss: 0.00002 | TestLoss: 1.20044 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 413 |  TrainLoss: 0.00001 | TestLoss: 1.20127 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 414 |  TrainLoss: 0.00002 | TestLoss: 1.20204 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 415 |  TrainLoss: 0.00003 | TestLoss: 1.20356 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 416 |  TrainLoss: 0.00002 | TestLoss: 1.20475 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 417 |  TrainLoss: 0.00002 | TestLoss: 1.20584 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 418 |  TrainLoss: 0.00002 | TestLoss: 1.20705 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 419 |  TrainLoss: 0.00002 | TestLoss: 1.20808 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 420 |  TrainLoss: 0.00001 | TestLoss: 1.20904 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 421 |  TrainLoss: 0.00002 | TestLoss: 1.21004 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 422 |  TrainLoss: 0.00002 | TestLoss: 1.21204 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 423 |  TrainLoss: 0.00002 | TestLoss: 1.21266 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 424 |  TrainLoss: 0.00002 | TestLoss: 1.21249 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 425 |  TrainLoss: 0.00003 | TestLoss: 1.21213 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 426 |  TrainLoss: 0.00002 | TestLoss: 1.21226 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 427 |  TrainLoss: 0.00004 | TestLoss: 1.21093 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 428 |  TrainLoss: 0.00002 | TestLoss: 1.21009 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 429 |  TrainLoss: 0.00002 | TestLoss: 1.20938 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 430 |  TrainLoss: 0.00002 | TestLoss: 1.20773 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 431 |  TrainLoss: 0.00001 | TestLoss: 1.20735 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 432 |  TrainLoss: 0.00004 | TestLoss: 1.20524 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 433 |  TrainLoss: 0.00002 | TestLoss: 1.20458 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 434 |  TrainLoss: 0.00003 | TestLoss: 1.20290 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 435 |  TrainLoss: 0.00003 | TestLoss: 1.20248 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 436 |  TrainLoss: 0.00003 | TestLoss: 1.20329 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 437 |  TrainLoss: 0.00002 | TestLoss: 1.20454 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 438 |  TrainLoss: 0.00002 | TestLoss: 1.20561 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 439 |  TrainLoss: 0.00003 | TestLoss: 1.20709 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 440 |  TrainLoss: 0.00001 | TestLoss: 1.20844 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 441 |  TrainLoss: 0.00001 | TestLoss: 1.20985 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 442 |  TrainLoss: 0.00002 | TestLoss: 1.21217 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 443 |  TrainLoss: 0.00001 | TestLoss: 1.21446 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 444 |  TrainLoss: 0.00001 | TestLoss: 1.21681 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 445 |  TrainLoss: 0.00001 | TestLoss: 1.21857 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 446 |  TrainLoss: 0.00002 | TestLoss: 1.22092 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 447 |  TrainLoss: 0.00002 | TestLoss: 1.22326 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 448 |  TrainLoss: 0.00002 | TestLoss: 1.22534 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 449 |  TrainLoss: 0.00002 | TestLoss: 1.22788 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 450 |  TrainLoss: 0.00003 | TestLoss: 1.23218 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 451 |  TrainLoss: 0.00002 | TestLoss: 1.23481 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 452 |  TrainLoss: 0.00001 | TestLoss: 1.23679 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 453 |  TrainLoss: 0.00002 | TestLoss: 1.23914 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 454 |  TrainLoss: 0.00002 | TestLoss: 1.24157 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 455 |  TrainLoss: 0.00002 | TestLoss: 1.24283 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 456 |  TrainLoss: 0.00002 | TestLoss: 1.24278 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 457 |  TrainLoss: 0.00002 | TestLoss: 1.24615 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 458 |  TrainLoss: 0.00001 | TestLoss: 1.24617 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 459 |  TrainLoss: 0.00002 | TestLoss: 1.24631 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 460 |  TrainLoss: 0.00002 | TestLoss: 1.24594 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 461 |  TrainLoss: 0.00002 | TestLoss: 1.24535 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 462 |  TrainLoss: 0.00002 | TestLoss: 1.24368 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 463 |  TrainLoss: 0.00002 | TestLoss: 1.24233 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 464 |  TrainLoss: 0.00002 | TestLoss: 1.24034 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 465 |  TrainLoss: 0.00002 | TestLoss: 1.23791 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 466 |  TrainLoss: 0.00001 | TestLoss: 1.23635 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 467 |  TrainLoss: 0.00003 | TestLoss: 1.23465 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 468 |  TrainLoss: 0.00001 | TestLoss: 1.23270 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 469 |  TrainLoss: 0.00001 | TestLoss: 1.23205 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 470 |  TrainLoss: 0.00001 | TestLoss: 1.23198 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 471 |  TrainLoss: 0.00001 | TestLoss: 1.23173 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 472 |  TrainLoss: 0.00002 | TestLoss: 1.23143 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 473 |  TrainLoss: 0.00003 | TestLoss: 1.23045 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 474 |  TrainLoss: 0.00002 | TestLoss: 1.23056 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 475 |  TrainLoss: 0.00002 | TestLoss: 1.23103 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 476 |  TrainLoss: 0.00002 | TestLoss: 1.23111 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 477 |  TrainLoss: 0.00001 | TestLoss: 1.23145 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 478 |  TrainLoss: 0.00002 | TestLoss: 1.23248 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 479 |  TrainLoss: 0.00002 | TestLoss: 1.23399 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 480 |  TrainLoss: 0.00002 | TestLoss: 1.23535 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 481 |  TrainLoss: 0.00001 | TestLoss: 1.23718 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 482 |  TrainLoss: 0.00001 | TestLoss: 1.23892 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 483 |  TrainLoss: 0.00002 | TestLoss: 1.24138 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 484 |  TrainLoss: 0.00001 | TestLoss: 1.24346 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 485 |  TrainLoss: 0.00001 | TestLoss: 1.24582 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 486 |  TrainLoss: 0.00001 | TestLoss: 1.24776 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 487 |  TrainLoss: 0.00001 | TestLoss: 1.24958 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 488 |  TrainLoss: 0.00001 | TestLoss: 1.25176 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 489 |  TrainLoss: 0.00002 | TestLoss: 1.25418 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 490 |  TrainLoss: 0.00001 | TestLoss: 1.25639 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 491 |  TrainLoss: 0.00001 | TestLoss: 1.25880 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 492 |  TrainLoss: 0.00001 | TestLoss: 1.26057 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 493 |  TrainLoss: 0.00002 | TestLoss: 1.26143 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 494 |  TrainLoss: 0.00002 | TestLoss: 1.26181 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 495 |  TrainLoss: 0.00001 | TestLoss: 1.26192 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 496 |  TrainLoss: 0.00001 | TestLoss: 1.26174 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 497 |  TrainLoss: 0.00001 | TestLoss: 1.26170 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 498 |  TrainLoss: 0.00001 | TestLoss: 1.26174 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 499 |  TrainLoss: 0.00002 | TestLoss: 1.26061 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Best WLoss: 0.00194 | Best Epoch: 52\n"
     ]
    }
   ],
   "source": [
    "wloss = []\n",
    "weighted_loss = 0\n",
    "exp_param = 0.8\n",
    "best_test_loss = float('inf')  # Initialize with a large value\n",
    "\n",
    "# Without dropout training results\n",
    "for epoch in range(500):\n",
    "  train_loss = train(epoch)\n",
    "  test_loss, test_acc, test_f1 = test(epoch)\n",
    "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
    "  \n",
    "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
    "  \n",
    "  if test_loss < best_test_loss:\n",
    "    best_test_loss = test_loss  # Update the best test loss\n",
    "\n",
    "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
    "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
    "\n",
    "# Print the best values\n",
    "best_wloss = min(wloss)\n",
    "best_epoch = wloss.index(best_wloss)\n",
    "print(f'Best WLoss: {best_wloss:.5f} | Best Epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3fc786",
   "metadata": {},
   "source": [
    "# Code 4: \n",
    "GraphSage + Bert Embedding with hyperparameters as defined in the paper and replacemenent of 1 MLP layer with 1 RNN Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4d9c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, embedding_size, batch_size, l2_reg_weight):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.batch_size = batch_size\n",
    "        self.l2_reg_weight = l2_reg_weight\n",
    "        \n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
    "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
    "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_size, hidden_channels[3], batch_first=True)\n",
    "        self.full1 = nn.Linear(hidden_channels[3], hidden_channels[4])\n",
    "        self.softmax = nn.Linear(hidden_channels[4], out_channels)\n",
    "\n",
    "        # Dropouts\n",
    "        self.dp1 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = self.conv1(x, edge_index).relu()\n",
    "        h = self.conv2(h, edge_index).relu()\n",
    "        h = self.conv3(h, edge_index).relu()\n",
    "\n",
    "        h = global_max_pool(h, batch)\n",
    "        \n",
    "        # Reshape the input tensor for RNN\n",
    "        h = h.unsqueeze(0)  # Add a time dimension\n",
    "        h = self.dp1(h)\n",
    "        \n",
    "        # Apply RNN\n",
    "        h, _ = self.rnn(h)\n",
    "        h = h.squeeze(0)  # Remove the time dimension\n",
    "        \n",
    "        h = self.full1(h).relu()\n",
    "        h = self.softmax(h)\n",
    "\n",
    "        return torch.sigmoid(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0570ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        all_preds.append(torch.reshape(out, (-1,)))\n",
    "        all_labels.append(data.y.float())\n",
    "    # print(all_preds)\n",
    "    accuracy, f1 = metrics(all_preds, all_labels)\n",
    "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
    "\n",
    "\n",
    "def metrics(preds, gts):\n",
    "    preds = torch.round(torch.cat(preds))\n",
    "    gts = torch.cat(gts)\n",
    "    # print(preds.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3db208bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 |  TrainLoss: 0.00002 | TestLoss: 1.26138 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 01 |  TrainLoss: 0.00001 | TestLoss: 1.26069 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 02 |  TrainLoss: 0.00002 | TestLoss: 1.26074 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 03 |  TrainLoss: 0.00001 | TestLoss: 1.26097 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 04 |  TrainLoss: 0.00001 | TestLoss: 1.26089 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 05 |  TrainLoss: 0.00001 | TestLoss: 1.26084 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 06 |  TrainLoss: 0.00001 | TestLoss: 1.26034 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 07 |  TrainLoss: 0.00001 | TestLoss: 1.26018 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 08 |  TrainLoss: 0.00001 | TestLoss: 1.25933 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 09 |  TrainLoss: 0.00001 | TestLoss: 1.25968 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 10 |  TrainLoss: 0.00001 | TestLoss: 1.25995 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 11 |  TrainLoss: 0.00001 | TestLoss: 1.26021 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 12 |  TrainLoss: 0.00002 | TestLoss: 1.26018 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 13 |  TrainLoss: 0.00001 | TestLoss: 1.26055 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 14 |  TrainLoss: 0.00001 | TestLoss: 1.26122 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 15 |  TrainLoss: 0.00002 | TestLoss: 1.26091 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 16 |  TrainLoss: 0.00001 | TestLoss: 1.26017 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 17 |  TrainLoss: 0.00001 | TestLoss: 1.25961 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 18 |  TrainLoss: 0.00001 | TestLoss: 1.25945 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 19 |  TrainLoss: 0.00001 | TestLoss: 1.25945 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 20 |  TrainLoss: 0.00001 | TestLoss: 1.26149 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 21 |  TrainLoss: 0.00001 | TestLoss: 1.26250 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 22 |  TrainLoss: 0.00001 | TestLoss: 1.26394 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 23 |  TrainLoss: 0.00001 | TestLoss: 1.26535 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 24 |  TrainLoss: 0.00001 | TestLoss: 1.26734 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 25 |  TrainLoss: 0.00001 | TestLoss: 1.26933 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 26 |  TrainLoss: 0.00002 | TestLoss: 1.27115 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 27 |  TrainLoss: 0.00001 | TestLoss: 1.27367 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 28 |  TrainLoss: 0.00001 | TestLoss: 1.27587 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 29 |  TrainLoss: 0.00001 | TestLoss: 1.27789 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 30 |  TrainLoss: 0.00001 | TestLoss: 1.27939 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 31 |  TrainLoss: 0.00001 | TestLoss: 1.28038 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 32 |  TrainLoss: 0.00001 | TestLoss: 1.28149 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 33 |  TrainLoss: 0.00001 | TestLoss: 1.28248 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 34 |  TrainLoss: 0.00001 | TestLoss: 1.28318 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 35 |  TrainLoss: 0.00001 | TestLoss: 1.28389 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 36 |  TrainLoss: 0.00001 | TestLoss: 1.28553 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 37 |  TrainLoss: 0.00000 | TestLoss: 1.28606 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 38 |  TrainLoss: 0.00001 | TestLoss: 1.28623 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 39 |  TrainLoss: 0.00001 | TestLoss: 1.28643 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 40 |  TrainLoss: 0.00002 | TestLoss: 1.28577 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 41 |  TrainLoss: 0.00002 | TestLoss: 1.28435 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 42 |  TrainLoss: 0.00001 | TestLoss: 1.28304 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 43 |  TrainLoss: 0.00001 | TestLoss: 1.28122 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 44 |  TrainLoss: 0.00001 | TestLoss: 1.28009 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 45 |  TrainLoss: 0.00001 | TestLoss: 1.27915 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 46 |  TrainLoss: 0.00001 | TestLoss: 1.28001 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 47 |  TrainLoss: 0.00002 | TestLoss: 1.28036 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 48 |  TrainLoss: 0.00002 | TestLoss: 1.27941 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 49 |  TrainLoss: 0.00001 | TestLoss: 1.27904 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 50 |  TrainLoss: 0.00001 | TestLoss: 1.27848 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 51 |  TrainLoss: 0.00002 | TestLoss: 1.65754 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 52 |  TrainLoss: 0.00001 | TestLoss: 1.65748 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 53 |  TrainLoss: 0.00003 | TestLoss: 1.65839 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 54 |  TrainLoss: 0.00001 | TestLoss: 1.66004 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 55 |  TrainLoss: 0.00002 | TestLoss: 1.66159 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 56 |  TrainLoss: 0.00001 | TestLoss: 1.66330 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 57 |  TrainLoss: 0.00001 | TestLoss: 1.66505 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 58 |  TrainLoss: 0.00002 | TestLoss: 1.66683 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 59 |  TrainLoss: 0.00002 | TestLoss: 1.66847 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 60 |  TrainLoss: 0.00001 | TestLoss: 1.66994 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 61 |  TrainLoss: 0.00001 | TestLoss: 1.67092 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 62 |  TrainLoss: 0.00001 | TestLoss: 1.67197 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 63 |  TrainLoss: 0.00001 | TestLoss: 1.67310 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 64 |  TrainLoss: 0.00001 | TestLoss: 1.67423 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 65 |  TrainLoss: 0.00001 | TestLoss: 1.67528 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 66 |  TrainLoss: 0.00003 | TestLoss: 1.67924 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 67 |  TrainLoss: 0.00002 | TestLoss: 1.68183 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 68 |  TrainLoss: 0.00001 | TestLoss: 1.68381 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 69 |  TrainLoss: 0.00002 | TestLoss: 1.68409 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 70 |  TrainLoss: 0.00001 | TestLoss: 1.68384 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 71 |  TrainLoss: 0.00001 | TestLoss: 1.68368 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 72 |  TrainLoss: 0.00001 | TestLoss: 1.68339 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 73 |  TrainLoss: 0.00001 | TestLoss: 1.68323 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 74 |  TrainLoss: 0.00001 | TestLoss: 1.68382 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 75 |  TrainLoss: 0.00001 | TestLoss: 1.68470 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 76 |  TrainLoss: 0.00001 | TestLoss: 1.68504 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 77 |  TrainLoss: 0.00001 | TestLoss: 1.68445 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 78 |  TrainLoss: 0.00001 | TestLoss: 1.68429 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 79 |  TrainLoss: 0.00001 | TestLoss: 1.68560 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 80 |  TrainLoss: 0.00001 | TestLoss: 1.68736 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 81 |  TrainLoss: 0.00000 | TestLoss: 1.68748 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 82 |  TrainLoss: 0.00001 | TestLoss: 1.68775 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 83 |  TrainLoss: 0.00001 | TestLoss: 1.68781 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 84 |  TrainLoss: 0.00001 | TestLoss: 1.68778 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 85 |  TrainLoss: 0.00001 | TestLoss: 1.68798 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 86 |  TrainLoss: 0.00001 | TestLoss: 1.68787 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 87 |  TrainLoss: 0.00001 | TestLoss: 1.68853 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 88 |  TrainLoss: 0.00001 | TestLoss: 1.68991 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 89 |  TrainLoss: 0.00001 | TestLoss: 1.69036 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 90 |  TrainLoss: 0.00002 | TestLoss: 1.69011 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 91 |  TrainLoss: 0.00001 | TestLoss: 1.68972 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 92 |  TrainLoss: 0.00001 | TestLoss: 1.68905 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 93 |  TrainLoss: 0.00002 | TestLoss: 1.68865 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 94 |  TrainLoss: 0.00001 | TestLoss: 1.68845 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 95 |  TrainLoss: 0.00001 | TestLoss: 1.68833 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 96 |  TrainLoss: 0.00001 | TestLoss: 1.68863 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 97 |  TrainLoss: 0.00001 | TestLoss: 1.68899 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 98 |  TrainLoss: 0.00001 | TestLoss: 1.68997 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 99 |  TrainLoss: 0.00002 | TestLoss: 1.69031 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 100 |  TrainLoss: 0.00001 | TestLoss: 1.69189 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 101 |  TrainLoss: 0.00001 | TestLoss: 1.69311 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 102 |  TrainLoss: 0.00001 | TestLoss: 1.69433 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 103 |  TrainLoss: 0.00001 | TestLoss: 1.69530 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 104 |  TrainLoss: 0.00000 | TestLoss: 1.69600 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 105 |  TrainLoss: 0.00001 | TestLoss: 1.69746 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 106 |  TrainLoss: 0.00000 | TestLoss: 1.69877 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 107 |  TrainLoss: 0.00001 | TestLoss: 1.69963 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 108 |  TrainLoss: 0.00001 | TestLoss: 1.70065 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 109 |  TrainLoss: 0.00001 | TestLoss: 1.70118 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 110 |  TrainLoss: 0.00001 | TestLoss: 1.70089 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 111 |  TrainLoss: 0.00001 | TestLoss: 1.70072 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 112 |  TrainLoss: 0.00001 | TestLoss: 1.70237 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 113 |  TrainLoss: 0.00001 | TestLoss: 1.70159 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 114 |  TrainLoss: 0.00001 | TestLoss: 1.70174 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 115 |  TrainLoss: 0.00001 | TestLoss: 1.70076 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 116 |  TrainLoss: 0.00000 | TestLoss: 1.70013 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 117 |  TrainLoss: 0.00001 | TestLoss: 1.70072 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 118 |  TrainLoss: 0.00001 | TestLoss: 1.70046 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 119 |  TrainLoss: 0.00001 | TestLoss: 1.69977 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 120 |  TrainLoss: 0.00001 | TestLoss: 1.69897 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 121 |  TrainLoss: 0.00001 | TestLoss: 1.69869 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 122 |  TrainLoss: 0.00001 | TestLoss: 1.69850 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 123 |  TrainLoss: 0.00003 | TestLoss: 1.69438 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 124 |  TrainLoss: 0.00002 | TestLoss: 1.69098 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 125 |  TrainLoss: 0.00001 | TestLoss: 1.68801 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 126 |  TrainLoss: 0.00001 | TestLoss: 1.68579 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 127 |  TrainLoss: 0.00001 | TestLoss: 1.68371 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 128 |  TrainLoss: 0.00001 | TestLoss: 1.68471 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 129 |  TrainLoss: 0.00001 | TestLoss: 1.68533 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 130 |  TrainLoss: 0.00001 | TestLoss: 1.68472 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 131 |  TrainLoss: 0.00001 | TestLoss: 1.68449 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 132 |  TrainLoss: 0.00001 | TestLoss: 1.68442 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 133 |  TrainLoss: 0.00002 | TestLoss: 1.68459 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 134 |  TrainLoss: 0.00001 | TestLoss: 1.68514 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 135 |  TrainLoss: 0.00001 | TestLoss: 1.68601 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 136 |  TrainLoss: 0.00001 | TestLoss: 1.68835 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 137 |  TrainLoss: 0.00001 | TestLoss: 1.69043 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 138 |  TrainLoss: 0.00001 | TestLoss: 1.69075 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 139 |  TrainLoss: 0.00002 | TestLoss: 1.69382 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 140 |  TrainLoss: 0.00000 | TestLoss: 1.69664 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 141 |  TrainLoss: 0.00001 | TestLoss: 1.69668 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 142 |  TrainLoss: 0.00001 | TestLoss: 1.69876 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 143 |  TrainLoss: 0.00001 | TestLoss: 1.70042 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 144 |  TrainLoss: 0.00001 | TestLoss: 1.70237 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 145 |  TrainLoss: 0.00001 | TestLoss: 1.70381 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 146 |  TrainLoss: 0.00001 | TestLoss: 1.70510 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 147 |  TrainLoss: 0.00001 | TestLoss: 1.70626 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 148 |  TrainLoss: 0.00001 | TestLoss: 1.70752 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 149 |  TrainLoss: 0.00001 | TestLoss: 1.70887 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 150 |  TrainLoss: 0.00001 | TestLoss: 1.70969 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 151 |  TrainLoss: 0.00001 | TestLoss: 1.71042 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 152 |  TrainLoss: 0.00002 | TestLoss: 1.71356 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 153 |  TrainLoss: 0.00001 | TestLoss: 1.71386 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 154 |  TrainLoss: 0.00001 | TestLoss: 1.71447 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 155 |  TrainLoss: 0.00001 | TestLoss: 1.71510 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 156 |  TrainLoss: 0.00001 | TestLoss: 1.71576 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 157 |  TrainLoss: 0.00001 | TestLoss: 1.71435 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 158 |  TrainLoss: 0.00001 | TestLoss: 1.71397 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 159 |  TrainLoss: 0.00001 | TestLoss: 1.71325 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 160 |  TrainLoss: 0.00001 | TestLoss: 1.71297 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 161 |  TrainLoss: 0.00001 | TestLoss: 1.71574 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 162 |  TrainLoss: 0.00001 | TestLoss: 1.71516 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 163 |  TrainLoss: 0.00001 | TestLoss: 1.71473 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 164 |  TrainLoss: 0.00000 | TestLoss: 1.71446 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 165 |  TrainLoss: 0.00000 | TestLoss: 1.71433 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 166 |  TrainLoss: 0.00001 | TestLoss: 1.71411 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 167 |  TrainLoss: 0.00001 | TestLoss: 1.71379 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 168 |  TrainLoss: 0.00001 | TestLoss: 1.71382 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 169 |  TrainLoss: 0.00001 | TestLoss: 1.71390 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 170 |  TrainLoss: 0.00001 | TestLoss: 1.71407 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 171 |  TrainLoss: 0.00000 | TestLoss: 1.71410 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 172 |  TrainLoss: 0.00001 | TestLoss: 1.71454 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 173 |  TrainLoss: 0.00002 | TestLoss: 1.71261 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 174 |  TrainLoss: 0.00001 | TestLoss: 1.71055 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 175 |  TrainLoss: 0.00001 | TestLoss: 1.70880 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 176 |  TrainLoss: 0.00001 | TestLoss: 1.70712 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 177 |  TrainLoss: 0.00001 | TestLoss: 1.70612 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 178 |  TrainLoss: 0.00001 | TestLoss: 1.70524 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 179 |  TrainLoss: 0.00001 | TestLoss: 1.70514 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 180 |  TrainLoss: 0.00001 | TestLoss: 1.70557 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 181 |  TrainLoss: 0.00001 | TestLoss: 1.70646 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 182 |  TrainLoss: 0.00001 | TestLoss: 1.70739 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 183 |  TrainLoss: 0.00001 | TestLoss: 1.70927 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 184 |  TrainLoss: 0.00001 | TestLoss: 1.71213 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 185 |  TrainLoss: 0.00000 | TestLoss: 1.71500 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 186 |  TrainLoss: 0.00000 | TestLoss: 1.71777 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 187 |  TrainLoss: 0.00001 | TestLoss: 1.71915 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 188 |  TrainLoss: 0.00000 | TestLoss: 1.72051 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 189 |  TrainLoss: 0.00001 | TestLoss: 1.72274 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 190 |  TrainLoss: 0.00001 | TestLoss: 1.72506 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 191 |  TrainLoss: 0.00001 | TestLoss: 1.72738 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 192 |  TrainLoss: 0.00002 | TestLoss: 1.72805 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 193 |  TrainLoss: 0.00001 | TestLoss: 1.72915 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 194 |  TrainLoss: 0.00000 | TestLoss: 1.73025 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 195 |  TrainLoss: 0.00001 | TestLoss: 1.73200 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 196 |  TrainLoss: 0.00001 | TestLoss: 1.73243 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 197 |  TrainLoss: 0.00001 | TestLoss: 1.73264 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 198 |  TrainLoss: 0.00001 | TestLoss: 1.73209 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 199 |  TrainLoss: 0.00001 | TestLoss: 1.73185 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 200 |  TrainLoss: 0.00001 | TestLoss: 1.73144 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 201 |  TrainLoss: 0.00000 | TestLoss: 1.73140 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 202 |  TrainLoss: 0.00000 | TestLoss: 1.73134 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 203 |  TrainLoss: 0.00002 | TestLoss: 1.72971 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 204 |  TrainLoss: 0.00001 | TestLoss: 1.72741 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 205 |  TrainLoss: 0.00000 | TestLoss: 1.72607 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 206 |  TrainLoss: 0.00001 | TestLoss: 1.72547 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 207 |  TrainLoss: 0.00000 | TestLoss: 1.72531 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 208 |  TrainLoss: 0.00001 | TestLoss: 1.72579 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 209 |  TrainLoss: 0.00001 | TestLoss: 1.73007 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 210 |  TrainLoss: 0.00000 | TestLoss: 1.73095 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 211 |  TrainLoss: 0.00001 | TestLoss: 1.73186 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 212 |  TrainLoss: 0.00001 | TestLoss: 1.73243 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 213 |  TrainLoss: 0.00001 | TestLoss: 1.73131 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 214 |  TrainLoss: 0.00001 | TestLoss: 1.73009 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 215 |  TrainLoss: 0.00001 | TestLoss: 1.72945 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 216 |  TrainLoss: 0.00000 | TestLoss: 1.72909 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 217 |  TrainLoss: 0.00001 | TestLoss: 1.72934 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 218 |  TrainLoss: 0.00001 | TestLoss: 1.72999 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 219 |  TrainLoss: 0.00000 | TestLoss: 1.73091 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 220 |  TrainLoss: 0.00000 | TestLoss: 1.73194 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 221 |  TrainLoss: 0.00000 | TestLoss: 1.73337 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 222 |  TrainLoss: 0.00000 | TestLoss: 1.73435 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 223 |  TrainLoss: 0.00000 | TestLoss: 1.73617 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 224 |  TrainLoss: 0.00001 | TestLoss: 1.73763 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 225 |  TrainLoss: 0.00000 | TestLoss: 1.73933 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 226 |  TrainLoss: 0.00000 | TestLoss: 1.74106 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 227 |  TrainLoss: 0.00001 | TestLoss: 1.74277 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 228 |  TrainLoss: 0.00001 | TestLoss: 1.74219 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 229 |  TrainLoss: 0.00001 | TestLoss: 1.74229 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 230 |  TrainLoss: 0.00001 | TestLoss: 1.74292 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 231 |  TrainLoss: 0.00001 | TestLoss: 1.74356 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 232 |  TrainLoss: 0.00000 | TestLoss: 1.74487 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 233 |  TrainLoss: 0.00001 | TestLoss: 1.74475 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 234 |  TrainLoss: 0.00001 | TestLoss: 1.74624 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 235 |  TrainLoss: 0.00000 | TestLoss: 1.74643 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 236 |  TrainLoss: 0.00000 | TestLoss: 1.74670 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 237 |  TrainLoss: 0.00001 | TestLoss: 1.74668 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 238 |  TrainLoss: 0.00000 | TestLoss: 1.74683 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 239 |  TrainLoss: 0.00001 | TestLoss: 1.74678 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 240 |  TrainLoss: 0.00002 | TestLoss: 1.74342 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 241 |  TrainLoss: 0.00001 | TestLoss: 1.74125 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 242 |  TrainLoss: 0.00001 | TestLoss: 1.73967 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 243 |  TrainLoss: 0.00000 | TestLoss: 1.74153 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 244 |  TrainLoss: 0.00000 | TestLoss: 1.74079 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 245 |  TrainLoss: 0.00001 | TestLoss: 1.74006 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 246 |  TrainLoss: 0.00001 | TestLoss: 1.73983 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 247 |  TrainLoss: 0.00000 | TestLoss: 1.73990 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 248 |  TrainLoss: 0.00000 | TestLoss: 1.74054 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 249 |  TrainLoss: 0.00001 | TestLoss: 1.74114 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 250 |  TrainLoss: 0.00001 | TestLoss: 1.74253 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 251 |  TrainLoss: 0.00000 | TestLoss: 1.74418 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 252 |  TrainLoss: 0.00000 | TestLoss: 1.74598 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 253 |  TrainLoss: 0.00000 | TestLoss: 1.74715 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 254 |  TrainLoss: 0.00000 | TestLoss: 1.74855 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 255 |  TrainLoss: 0.00001 | TestLoss: 1.74978 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 256 |  TrainLoss: 0.00000 | TestLoss: 1.75092 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 257 |  TrainLoss: 0.00000 | TestLoss: 1.75238 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 258 |  TrainLoss: 0.00000 | TestLoss: 1.75371 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 259 |  TrainLoss: 0.00000 | TestLoss: 1.75477 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 260 |  TrainLoss: 0.00001 | TestLoss: 1.75619 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 261 |  TrainLoss: 0.00000 | TestLoss: 1.75742 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 262 |  TrainLoss: 0.00000 | TestLoss: 1.75584 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 263 |  TrainLoss: 0.00000 | TestLoss: 1.75692 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 264 |  TrainLoss: 0.00001 | TestLoss: 1.75789 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 265 |  TrainLoss: 0.00000 | TestLoss: 1.75870 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 266 |  TrainLoss: 0.00000 | TestLoss: 1.75978 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 267 |  TrainLoss: 0.00001 | TestLoss: 1.76020 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 268 |  TrainLoss: 0.00000 | TestLoss: 1.76383 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 269 |  TrainLoss: 0.00001 | TestLoss: 1.76443 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 270 |  TrainLoss: 0.00001 | TestLoss: 1.76441 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 271 |  TrainLoss: 0.00000 | TestLoss: 1.76508 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 272 |  TrainLoss: 0.00000 | TestLoss: 1.76587 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 273 |  TrainLoss: 0.00000 | TestLoss: 1.76629 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 274 |  TrainLoss: 0.00001 | TestLoss: 1.76574 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 275 |  TrainLoss: 0.00001 | TestLoss: 1.76606 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 276 |  TrainLoss: 0.00000 | TestLoss: 1.76599 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 277 |  TrainLoss: 0.00000 | TestLoss: 1.76583 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 278 |  TrainLoss: 0.00001 | TestLoss: 1.76639 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 279 |  TrainLoss: 0.00001 | TestLoss: 1.76785 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 280 |  TrainLoss: 0.00001 | TestLoss: 1.76906 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 281 |  TrainLoss: 0.00000 | TestLoss: 1.76986 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 282 |  TrainLoss: 0.00000 | TestLoss: 1.77073 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 283 |  TrainLoss: 0.00000 | TestLoss: 1.77143 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 284 |  TrainLoss: 0.00000 | TestLoss: 1.77192 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 285 |  TrainLoss: 0.00000 | TestLoss: 1.77270 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 286 |  TrainLoss: 0.00000 | TestLoss: 1.77327 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 287 |  TrainLoss: 0.00001 | TestLoss: 1.77106 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 288 |  TrainLoss: 0.00000 | TestLoss: 1.76866 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 289 |  TrainLoss: 0.00000 | TestLoss: 1.76676 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 290 |  TrainLoss: 0.00000 | TestLoss: 1.76482 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 291 |  TrainLoss: 0.00000 | TestLoss: 1.76328 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 292 |  TrainLoss: 0.00001 | TestLoss: 1.76312 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 293 |  TrainLoss: 0.00000 | TestLoss: 1.76274 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 294 |  TrainLoss: 0.00000 | TestLoss: 1.76267 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 295 |  TrainLoss: 0.00000 | TestLoss: 1.76264 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 296 |  TrainLoss: 0.00000 | TestLoss: 1.76238 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 297 |  TrainLoss: 0.00000 | TestLoss: 1.76233 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 298 |  TrainLoss: 0.00000 | TestLoss: 1.76245 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 299 |  TrainLoss: 0.00000 | TestLoss: 1.76302 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 300 |  TrainLoss: 0.00000 | TestLoss: 1.76350 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 301 |  TrainLoss: 0.00000 | TestLoss: 1.76375 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 302 |  TrainLoss: 0.00000 | TestLoss: 1.76412 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 303 |  TrainLoss: 0.00000 | TestLoss: 1.76493 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 304 |  TrainLoss: 0.00000 | TestLoss: 1.76588 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 305 |  TrainLoss: 0.00001 | TestLoss: 1.76820 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 306 |  TrainLoss: 0.00001 | TestLoss: 1.76983 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 307 |  TrainLoss: 0.00000 | TestLoss: 1.77148 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 308 |  TrainLoss: 0.00000 | TestLoss: 1.77250 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 309 |  TrainLoss: 0.00000 | TestLoss: 1.77358 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 310 |  TrainLoss: 0.00001 | TestLoss: 1.77382 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 311 |  TrainLoss: 0.00000 | TestLoss: 1.77443 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 312 |  TrainLoss: 0.00001 | TestLoss: 1.77416 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 313 |  TrainLoss: 0.00000 | TestLoss: 1.77374 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 314 |  TrainLoss: 0.00000 | TestLoss: 1.77336 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 315 |  TrainLoss: 0.00001 | TestLoss: 1.77303 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 316 |  TrainLoss: 0.00000 | TestLoss: 1.77316 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 317 |  TrainLoss: 0.00001 | TestLoss: 1.77294 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 318 |  TrainLoss: 0.00000 | TestLoss: 1.77319 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 319 |  TrainLoss: 0.00000 | TestLoss: 1.77396 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 320 |  TrainLoss: 0.00000 | TestLoss: 1.77462 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 321 |  TrainLoss: 0.00000 | TestLoss: 1.77592 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 322 |  TrainLoss: 0.00000 | TestLoss: 1.77753 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 323 |  TrainLoss: 0.00001 | TestLoss: 1.77912 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 324 |  TrainLoss: 0.00000 | TestLoss: 1.78107 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 325 |  TrainLoss: 0.00001 | TestLoss: 1.78396 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 326 |  TrainLoss: 0.00000 | TestLoss: 1.78639 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 327 |  TrainLoss: 0.00001 | TestLoss: 1.78796 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 328 |  TrainLoss: 0.00000 | TestLoss: 1.78909 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 329 |  TrainLoss: 0.00002 | TestLoss: 1.79208 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 330 |  TrainLoss: 0.00000 | TestLoss: 1.78953 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 331 |  TrainLoss: 0.00001 | TestLoss: 1.78684 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 332 |  TrainLoss: 0.00000 | TestLoss: 1.78487 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 333 |  TrainLoss: 0.00000 | TestLoss: 1.78356 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 334 |  TrainLoss: 0.00000 | TestLoss: 1.78574 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 335 |  TrainLoss: 0.00000 | TestLoss: 1.78559 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 336 |  TrainLoss: 0.00000 | TestLoss: 1.78516 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 337 |  TrainLoss: 0.00000 | TestLoss: 1.78468 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 338 |  TrainLoss: 0.00000 | TestLoss: 1.78345 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 339 |  TrainLoss: 0.00000 | TestLoss: 1.78274 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 340 |  TrainLoss: 0.00002 | TestLoss: 1.77964 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 341 |  TrainLoss: 0.00000 | TestLoss: 1.77673 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 342 |  TrainLoss: 0.00001 | TestLoss: 1.77409 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 343 |  TrainLoss: 0.00000 | TestLoss: 1.77258 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 344 |  TrainLoss: 0.00001 | TestLoss: 1.76935 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 345 |  TrainLoss: 0.00000 | TestLoss: 1.76701 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 346 |  TrainLoss: 0.00000 | TestLoss: 1.76518 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 347 |  TrainLoss: 0.00002 | TestLoss: 1.76542 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 348 |  TrainLoss: 0.00000 | TestLoss: 1.76549 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 349 |  TrainLoss: 0.00000 | TestLoss: 1.76573 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 350 |  TrainLoss: 0.00000 | TestLoss: 1.76610 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 351 |  TrainLoss: 0.00000 | TestLoss: 1.76677 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 352 |  TrainLoss: 0.00000 | TestLoss: 1.76799 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 353 |  TrainLoss: 0.00000 | TestLoss: 1.76947 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 354 |  TrainLoss: 0.00001 | TestLoss: 1.77140 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 355 |  TrainLoss: 0.00001 | TestLoss: 1.77530 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 356 |  TrainLoss: 0.00000 | TestLoss: 1.77926 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 357 |  TrainLoss: 0.00000 | TestLoss: 1.78285 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 358 |  TrainLoss: 0.00001 | TestLoss: 1.78431 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 359 |  TrainLoss: 0.00000 | TestLoss: 1.78637 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 360 |  TrainLoss: 0.00000 | TestLoss: 1.78839 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 361 |  TrainLoss: 0.00000 | TestLoss: 1.78984 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 362 |  TrainLoss: 0.00001 | TestLoss: 1.79000 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 363 |  TrainLoss: 0.00000 | TestLoss: 1.79023 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 364 |  TrainLoss: 0.00001 | TestLoss: 1.78860 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 365 |  TrainLoss: 0.00000 | TestLoss: 1.78801 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 366 |  TrainLoss: 0.00000 | TestLoss: 1.78720 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 367 |  TrainLoss: 0.00000 | TestLoss: 1.78709 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 368 |  TrainLoss: 0.00000 | TestLoss: 1.78771 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 369 |  TrainLoss: 0.00000 | TestLoss: 1.78821 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 370 |  TrainLoss: 0.00000 | TestLoss: 1.78880 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 371 |  TrainLoss: 0.00000 | TestLoss: 1.78959 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 372 |  TrainLoss: 0.00000 | TestLoss: 1.79047 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 373 |  TrainLoss: 0.00000 | TestLoss: 1.79080 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 374 |  TrainLoss: 0.00000 | TestLoss: 1.79156 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 375 |  TrainLoss: 0.00000 | TestLoss: 1.79237 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 376 |  TrainLoss: 0.00000 | TestLoss: 1.79358 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 377 |  TrainLoss: 0.00000 | TestLoss: 1.79470 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 378 |  TrainLoss: 0.00000 | TestLoss: 1.79620 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 379 |  TrainLoss: 0.00000 | TestLoss: 1.79735 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 380 |  TrainLoss: 0.00000 | TestLoss: 1.79879 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 381 |  TrainLoss: 0.00000 | TestLoss: 1.80016 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 382 |  TrainLoss: 0.00001 | TestLoss: 1.79868 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 383 |  TrainLoss: 0.00000 | TestLoss: 1.79777 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 384 |  TrainLoss: 0.00000 | TestLoss: 1.79647 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 385 |  TrainLoss: 0.00000 | TestLoss: 1.79518 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 386 |  TrainLoss: 0.00000 | TestLoss: 1.79420 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 387 |  TrainLoss: 0.00000 | TestLoss: 1.79361 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 388 |  TrainLoss: 0.00000 | TestLoss: 1.79358 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 389 |  TrainLoss: 0.00002 | TestLoss: 1.79054 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 390 |  TrainLoss: 0.00001 | TestLoss: 1.78546 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 391 |  TrainLoss: 0.00000 | TestLoss: 1.78134 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 392 |  TrainLoss: 0.00001 | TestLoss: 1.77949 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 393 |  TrainLoss: 0.00001 | TestLoss: 1.77912 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 394 |  TrainLoss: 0.00000 | TestLoss: 1.77872 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 395 |  TrainLoss: 0.00000 | TestLoss: 1.77928 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 396 |  TrainLoss: 0.00000 | TestLoss: 1.77997 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 397 |  TrainLoss: 0.00000 | TestLoss: 1.78118 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 398 |  TrainLoss: 0.00001 | TestLoss: 1.78332 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 399 |  TrainLoss: 0.00000 | TestLoss: 1.78456 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 400 |  TrainLoss: 0.00001 | TestLoss: 1.78605 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 401 |  TrainLoss: 0.00000 | TestLoss: 1.78768 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 402 |  TrainLoss: 0.00001 | TestLoss: 1.79076 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 403 |  TrainLoss: 0.00000 | TestLoss: 1.79380 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 404 |  TrainLoss: 0.00000 | TestLoss: 1.79677 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 405 |  TrainLoss: 0.00000 | TestLoss: 1.79948 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 406 |  TrainLoss: 0.00001 | TestLoss: 1.80076 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 407 |  TrainLoss: 0.00000 | TestLoss: 1.80309 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 408 |  TrainLoss: 0.00000 | TestLoss: 1.80523 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 409 |  TrainLoss: 0.00000 | TestLoss: 1.80767 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 410 |  TrainLoss: 0.00000 | TestLoss: 1.80973 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 411 |  TrainLoss: 0.00000 | TestLoss: 1.81095 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 412 |  TrainLoss: 0.00000 | TestLoss: 1.81241 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 413 |  TrainLoss: 0.00000 | TestLoss: 1.81418 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 414 |  TrainLoss: 0.00000 | TestLoss: 1.81607 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 415 |  TrainLoss: 0.00000 | TestLoss: 1.81783 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 416 |  TrainLoss: 0.00000 | TestLoss: 1.82025 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 417 |  TrainLoss: 0.00000 | TestLoss: 1.82242 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 418 |  TrainLoss: 0.00000 | TestLoss: 1.82440 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 419 |  TrainLoss: 0.00000 | TestLoss: 1.82638 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 420 |  TrainLoss: 0.00000 | TestLoss: 1.82832 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 421 |  TrainLoss: 0.00000 | TestLoss: 1.82975 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 422 |  TrainLoss: 0.00001 | TestLoss: 1.83013 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 423 |  TrainLoss: 0.00000 | TestLoss: 1.83066 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 424 |  TrainLoss: 0.00000 | TestLoss: 1.83071 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 425 |  TrainLoss: 0.00000 | TestLoss: 1.83058 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 426 |  TrainLoss: 0.00000 | TestLoss: 1.83033 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 427 |  TrainLoss: 0.00000 | TestLoss: 1.82917 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 428 |  TrainLoss: 0.00000 | TestLoss: 1.82748 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 429 |  TrainLoss: 0.00000 | TestLoss: 1.82631 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 430 |  TrainLoss: 0.00001 | TestLoss: 1.82395 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 431 |  TrainLoss: 0.00001 | TestLoss: 1.82156 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 432 |  TrainLoss: 0.00000 | TestLoss: 1.81845 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 433 |  TrainLoss: 0.00000 | TestLoss: 1.81570 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 434 |  TrainLoss: 0.00001 | TestLoss: 1.81098 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 435 |  TrainLoss: 0.00000 | TestLoss: 1.80758 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 436 |  TrainLoss: 0.00001 | TestLoss: 1.80424 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 437 |  TrainLoss: 0.00000 | TestLoss: 1.80072 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 438 |  TrainLoss: 0.00000 | TestLoss: 1.79788 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 439 |  TrainLoss: 0.00001 | TestLoss: 1.79729 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 440 |  TrainLoss: 0.00000 | TestLoss: 1.79632 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 441 |  TrainLoss: 0.00000 | TestLoss: 1.79711 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 442 |  TrainLoss: 0.00000 | TestLoss: 1.79750 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 443 |  TrainLoss: 0.00000 | TestLoss: 1.79861 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 444 |  TrainLoss: 0.00000 | TestLoss: 1.79935 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 445 |  TrainLoss: 0.00000 | TestLoss: 1.80083 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 446 |  TrainLoss: 0.00000 | TestLoss: 1.80192 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 447 |  TrainLoss: 0.00001 | TestLoss: 1.80627 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 448 |  TrainLoss: 0.00000 | TestLoss: 1.81156 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 449 |  TrainLoss: 0.00000 | TestLoss: 1.81646 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 450 |  TrainLoss: 0.00001 | TestLoss: 1.81883 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 451 |  TrainLoss: 0.00000 | TestLoss: 1.82188 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 452 |  TrainLoss: 0.00000 | TestLoss: 1.82482 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 453 |  TrainLoss: 0.00000 | TestLoss: 1.82749 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 454 |  TrainLoss: 0.00000 | TestLoss: 1.83026 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 455 |  TrainLoss: 0.00001 | TestLoss: 1.82752 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 456 |  TrainLoss: 0.00000 | TestLoss: 1.82512 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 457 |  TrainLoss: 0.00000 | TestLoss: 1.82297 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 458 |  TrainLoss: 0.00000 | TestLoss: 1.82104 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 459 |  TrainLoss: 0.00001 | TestLoss: 1.81883 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 460 |  TrainLoss: 0.00000 | TestLoss: 1.81624 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 461 |  TrainLoss: 0.00000 | TestLoss: 1.81395 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 462 |  TrainLoss: 0.00000 | TestLoss: 1.81272 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 463 |  TrainLoss: 0.00000 | TestLoss: 1.81185 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 464 |  TrainLoss: 0.00000 | TestLoss: 1.81108 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 465 |  TrainLoss: 0.00000 | TestLoss: 2.19071 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 466 |  TrainLoss: 0.00000 | TestLoss: 2.19047 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 467 |  TrainLoss: 0.00000 | TestLoss: 2.19013 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 468 |  TrainLoss: 0.00000 | TestLoss: 2.18995 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 469 |  TrainLoss: 0.00000 | TestLoss: 2.18968 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 470 |  TrainLoss: 0.00000 | TestLoss: 2.19022 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 471 |  TrainLoss: 0.00000 | TestLoss: 2.19179 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 472 |  TrainLoss: 0.00000 | TestLoss: 2.19344 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 473 |  TrainLoss: 0.00000 | TestLoss: 2.19454 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 474 |  TrainLoss: 0.00000 | TestLoss: 2.19466 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 475 |  TrainLoss: 0.00000 | TestLoss: 2.19575 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 476 |  TrainLoss: 0.00000 | TestLoss: 2.19887 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 477 |  TrainLoss: 0.00000 | TestLoss: 1.82093 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 478 |  TrainLoss: 0.00000 | TestLoss: 1.82355 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 479 |  TrainLoss: 0.00000 | TestLoss: 1.82647 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 480 |  TrainLoss: 0.00000 | TestLoss: 1.82914 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 481 |  TrainLoss: 0.00000 | TestLoss: 1.83150 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 482 |  TrainLoss: 0.00000 | TestLoss: 1.83275 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 483 |  TrainLoss: 0.00000 | TestLoss: 1.83362 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 484 |  TrainLoss: 0.00000 | TestLoss: 1.83428 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 485 |  TrainLoss: 0.00000 | TestLoss: 1.83544 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 486 |  TrainLoss: 0.00000 | TestLoss: 1.83648 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 487 |  TrainLoss: 0.00000 | TestLoss: 1.83764 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 488 |  TrainLoss: 0.00000 | TestLoss: 1.83889 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 489 |  TrainLoss: 0.00000 | TestLoss: 1.84018 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 490 |  TrainLoss: 0.00000 | TestLoss: 1.84137 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 491 |  TrainLoss: 0.00000 | TestLoss: 1.84249 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 492 |  TrainLoss: 0.00000 | TestLoss: 1.84331 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 493 |  TrainLoss: 0.00001 | TestLoss: 1.84086 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 494 |  TrainLoss: 0.00001 | TestLoss: 1.83868 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 495 |  TrainLoss: 0.00000 | TestLoss: 1.83607 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 496 |  TrainLoss: 0.00001 | TestLoss: 2.21290 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 497 |  TrainLoss: 0.00000 | TestLoss: 2.21011 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 498 |  TrainLoss: 0.00000 | TestLoss: 2.20837 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 499 |  TrainLoss: 0.00000 | TestLoss: 2.20642 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Best WLoss: 0.00570 | Best Epoch: 19\n"
     ]
    }
   ],
   "source": [
    "wloss = []\n",
    "weighted_loss = 0\n",
    "exp_param = 0.8\n",
    "best_test_loss = float('inf')  # Initialize with a large value\n",
    "\n",
    "# Without dropout training results\n",
    "for epoch in range(500):\n",
    "  train_loss = train(epoch)\n",
    "  test_loss, test_acc, test_f1 = test(epoch)\n",
    "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
    "  \n",
    "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
    "  \n",
    "  if test_loss < best_test_loss:\n",
    "    best_test_loss = test_loss  # Update the best test loss\n",
    "\n",
    "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
    "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
    "\n",
    "# Print the best values\n",
    "best_wloss = min(wloss)\n",
    "best_epoch = wloss.index(best_wloss)\n",
    "print(f'Best WLoss: {best_wloss:.5f} | Best Epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c3ac07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
