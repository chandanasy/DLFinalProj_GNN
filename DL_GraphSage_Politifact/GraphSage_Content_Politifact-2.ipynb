{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d4eac49",
   "metadata": {},
   "source": [
    "# Model-1 \n",
    "GraphSage + content embeddings using default parameters 3 Layer MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b6db0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
      "Requirement already satisfied: torch-scatter in ./opt/anaconda3/lib/python3.9/site-packages (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
      "Requirement already satisfied: torch-sparse in ./opt/anaconda3/lib/python3.9/site-packages (0.6.17)\n",
      "Requirement already satisfied: scipy in ./opt/anaconda3/lib/python3.9/site-packages (from torch-sparse) (1.9.1)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.18.5 in ./opt/anaconda3/lib/python3.9/site-packages (from scipy->torch-sparse) (1.21.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch-geometric in ./opt/anaconda3/lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in ./opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.0.2)\n",
      "Requirement already satisfied: pyparsing in ./opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in ./opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: scipy in ./opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.9.1)\n",
      "Requirement already satisfied: tqdm in ./opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (4.64.1)\n",
      "Requirement already satisfied: numpy in ./opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.21.5)\n",
      "Requirement already satisfied: psutil>=5.8.0 in ./opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (5.9.0)\n",
      "Requirement already satisfied: requests in ./opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (2.28.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch-geometric) (2.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (1.26.11)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in ./opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf306af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import UPFD #importing the UPFD Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71e673f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the train and test split by defining the feature as content and setting the name as politifact\n",
    "test_data_pol = UPFD(root=\".\", name=\"politifact\", feature= \"content\",split=\"test\")\n",
    "train_data_pol = UPFD(root=\".\", name=\"politifact\", feature= \"content\", split=\"train\")\n",
    "val_data_pol = UPFD(root=\".\", name=\"politifact\", feature= \"content\", split=\"val\")\n",
    "train_data_pol = train_data_pol + val_data_pol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ede535b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politifact Dataset\n",
      "Train Samples:  93\n",
      "Test Samples:  221\n"
     ]
    }
   ],
   "source": [
    "print(\"Politifact Dataset\")\n",
    "print(\"Train Samples: \", len(train_data_pol))\n",
    "print(\"Test Samples: \", len(test_data_pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71aa48cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  8,  8,  8, 16, 16, 16, 16, 16, 16,\n",
       "         24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
       "         24, 24, 24, 24, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 60],\n",
       "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "         55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pol[0].edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c8621e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "train_loader = DataLoader(train_data_pol, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_data_pol, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1137282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LeakyReLU, Softmax, Linear, SELU,Dropout\n",
    "from torch_geometric.nn import SAGEConv, global_max_pool, GATv2Conv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "754d149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the GraphSage Model with 3 SageConv layers and 3 unit MLP\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
    "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
    "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
    "        \n",
    "        self.full1 = Linear(hidden_channels[2],hidden_channels[3])\n",
    "        self.full2 = Linear(hidden_channels[3],hidden_channels[4])\n",
    "        self.full3 = Linear(hidden_channels[4],hidden_channels[5])\n",
    "\n",
    "        self.softmax = Linear(hidden_channels[5],out_channels)\n",
    "\n",
    "        #droupouts\n",
    "        self.dp1 = Dropout(0.2)\n",
    "        self.dp2 = Dropout(0.2)\n",
    "        self.dp3 = Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = self.conv1(x, edge_index).relu()\n",
    "        h = self.conv2(h, edge_index).relu()\n",
    "        h = self.conv3(h, edge_index).relu()\n",
    "\n",
    "        h = global_max_pool(h,batch)\n",
    "\n",
    "        h = self.full1(h).relu()\n",
    "        h = self.dp1(h)\n",
    "        h = self.full2(h).relu()\n",
    "        h = self.dp2(h)\n",
    "        h = self.full3(h).relu()\n",
    "        h = self.dp3(h)\n",
    "        \n",
    "        h = self.softmax(h)\n",
    "\n",
    "        return torch.sigmoid(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63105653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score, f1_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df18b7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#specifying number of input features, hidden layer sizes, and number of output channels\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(test_data_pol.num_features,[512,512,512,256,256,256],1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "lossff = torch.nn.BCELoss()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f6fe76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the train and test function for the model\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        all_preds.append(torch.reshape(out, (-1,)))\n",
    "        all_labels.append(data.y.float())\n",
    "    # print(all_preds)\n",
    "    accuracy, f1 = metrics(all_preds, all_labels)\n",
    "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
    "\n",
    "\n",
    "def metrics(preds, gts):\n",
    "    preds = torch.round(torch.cat(preds))\n",
    "    gts = torch.cat(gts)\n",
    "    # print(preds.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcca2509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 |  TrainLoss: 0.69312 | TestLoss: 0.69312 | TestAcc: 0.49774 | TestF1: 0.43\n",
      "Epoch: 01 |  TrainLoss: 0.69356 | TestLoss: 0.69319 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 02 |  TrainLoss: 0.69373 | TestLoss: 0.69328 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 03 |  TrainLoss: 0.69266 | TestLoss: 0.69334 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 04 |  TrainLoss: 0.69371 | TestLoss: 0.69336 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 05 |  TrainLoss: 0.69290 | TestLoss: 0.69340 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 06 |  TrainLoss: 0.69266 | TestLoss: 0.69344 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 07 |  TrainLoss: 0.69281 | TestLoss: 0.69348 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 08 |  TrainLoss: 0.69147 | TestLoss: 0.69350 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 09 |  TrainLoss: 0.69199 | TestLoss: 0.69352 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 10 |  TrainLoss: 0.69187 | TestLoss: 0.69356 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 11 |  TrainLoss: 0.69214 | TestLoss: 0.69359 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 12 |  TrainLoss: 0.69180 | TestLoss: 0.69364 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 13 |  TrainLoss: 0.69123 | TestLoss: 0.69372 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 14 |  TrainLoss: 0.69251 | TestLoss: 0.69379 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 15 |  TrainLoss: 0.69191 | TestLoss: 0.69382 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 16 |  TrainLoss: 0.69264 | TestLoss: 0.69386 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 17 |  TrainLoss: 0.69113 | TestLoss: 0.69385 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 18 |  TrainLoss: 0.69171 | TestLoss: 0.69385 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 19 |  TrainLoss: 0.69144 | TestLoss: 0.69388 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 20 |  TrainLoss: 0.69160 | TestLoss: 0.69391 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 21 |  TrainLoss: 0.69159 | TestLoss: 0.69393 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 22 |  TrainLoss: 0.69062 | TestLoss: 0.69394 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 23 |  TrainLoss: 0.69168 | TestLoss: 0.69390 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 24 |  TrainLoss: 0.69161 | TestLoss: 0.69386 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 25 |  TrainLoss: 0.69019 | TestLoss: 0.69388 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 26 |  TrainLoss: 0.68964 | TestLoss: 0.69387 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 27 |  TrainLoss: 0.69079 | TestLoss: 0.69379 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 28 |  TrainLoss: 0.69083 | TestLoss: 0.69364 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 29 |  TrainLoss: 0.69096 | TestLoss: 0.69346 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 30 |  TrainLoss: 0.69104 | TestLoss: 0.69319 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 31 |  TrainLoss: 0.69045 | TestLoss: 0.69296 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 32 |  TrainLoss: 0.68917 | TestLoss: 0.69279 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 33 |  TrainLoss: 0.68922 | TestLoss: 0.69263 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 34 |  TrainLoss: 0.68908 | TestLoss: 0.69243 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 35 |  TrainLoss: 0.68970 | TestLoss: 0.69216 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 36 |  TrainLoss: 0.68744 | TestLoss: 0.69171 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 37 |  TrainLoss: 0.68898 | TestLoss: 0.69125 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 38 |  TrainLoss: 0.68767 | TestLoss: 0.69086 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 39 |  TrainLoss: 0.68852 | TestLoss: 0.69074 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 40 |  TrainLoss: 0.68728 | TestLoss: 0.69046 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 41 |  TrainLoss: 0.68445 | TestLoss: 0.69007 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 42 |  TrainLoss: 0.68608 | TestLoss: 0.68930 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 43 |  TrainLoss: 0.68569 | TestLoss: 0.68828 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 44 |  TrainLoss: 0.68386 | TestLoss: 0.68743 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 45 |  TrainLoss: 0.68438 | TestLoss: 0.68676 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 46 |  TrainLoss: 0.68135 | TestLoss: 0.68613 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 47 |  TrainLoss: 0.67933 | TestLoss: 0.68575 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 48 |  TrainLoss: 0.67984 | TestLoss: 0.68401 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 49 |  TrainLoss: 0.67880 | TestLoss: 0.68136 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 50 |  TrainLoss: 0.67942 | TestLoss: 0.67990 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 51 |  TrainLoss: 0.67429 | TestLoss: 0.67917 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 52 |  TrainLoss: 0.67418 | TestLoss: 0.67611 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 53 |  TrainLoss: 0.66927 | TestLoss: 0.67390 | TestAcc: 0.49321 | TestF1: 0.02\n",
      "Epoch: 54 |  TrainLoss: 0.66458 | TestLoss: 0.67502 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 55 |  TrainLoss: 0.66377 | TestLoss: 0.66966 | TestAcc: 0.49321 | TestF1: 0.02\n",
      "Epoch: 56 |  TrainLoss: 0.66170 | TestLoss: 0.66577 | TestAcc: 0.77828 | TestF1: 0.74\n",
      "Epoch: 57 |  TrainLoss: 0.65972 | TestLoss: 0.66356 | TestAcc: 0.51131 | TestF1: 0.08\n",
      "Epoch: 58 |  TrainLoss: 0.65664 | TestLoss: 0.66696 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 59 |  TrainLoss: 0.65489 | TestLoss: 0.65625 | TestAcc: 0.59729 | TestF1: 0.35\n",
      "Epoch: 60 |  TrainLoss: 0.64891 | TestLoss: 0.65084 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 61 |  TrainLoss: 0.64475 | TestLoss: 0.64732 | TestAcc: 0.68778 | TestF1: 0.57\n",
      "Epoch: 62 |  TrainLoss: 0.64054 | TestLoss: 0.64630 | TestAcc: 0.59276 | TestF1: 0.34\n",
      "Epoch: 63 |  TrainLoss: 0.63103 | TestLoss: 0.63688 | TestAcc: 0.74661 | TestF1: 0.69\n",
      "Epoch: 64 |  TrainLoss: 0.62879 | TestLoss: 0.62973 | TestAcc: 0.80090 | TestF1: 0.79\n",
      "Epoch: 65 |  TrainLoss: 0.62378 | TestLoss: 0.63170 | TestAcc: 0.65158 | TestF1: 0.48\n",
      "Epoch: 66 |  TrainLoss: 0.61526 | TestLoss: 0.62123 | TestAcc: 0.71041 | TestF1: 0.63\n",
      "Epoch: 67 |  TrainLoss: 0.60572 | TestLoss: 0.60980 | TestAcc: 0.83258 | TestF1: 0.83\n",
      "Epoch: 68 |  TrainLoss: 0.60679 | TestLoss: 0.61032 | TestAcc: 0.71041 | TestF1: 0.62\n",
      "Epoch: 69 |  TrainLoss: 0.58838 | TestLoss: 0.59736 | TestAcc: 0.76923 | TestF1: 0.72\n",
      "Epoch: 70 |  TrainLoss: 0.58001 | TestLoss: 0.58597 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 71 |  TrainLoss: 0.57173 | TestLoss: 0.59473 | TestAcc: 0.70588 | TestF1: 0.61\n",
      "Epoch: 72 |  TrainLoss: 0.57243 | TestLoss: 0.56843 | TestAcc: 0.80090 | TestF1: 0.78\n",
      "Epoch: 73 |  TrainLoss: 0.54864 | TestLoss: 0.55911 | TestAcc: 0.82805 | TestF1: 0.84\n",
      "Epoch: 74 |  TrainLoss: 0.54756 | TestLoss: 0.57730 | TestAcc: 0.70588 | TestF1: 0.61\n",
      "Epoch: 75 |  TrainLoss: 0.53248 | TestLoss: 0.53964 | TestAcc: 0.81900 | TestF1: 0.81\n",
      "Epoch: 76 |  TrainLoss: 0.50719 | TestLoss: 0.53369 | TestAcc: 0.83710 | TestF1: 0.85\n",
      "Epoch: 77 |  TrainLoss: 0.51228 | TestLoss: 0.54650 | TestAcc: 0.73756 | TestF1: 0.67\n",
      "Epoch: 78 |  TrainLoss: 0.49834 | TestLoss: 0.52278 | TestAcc: 0.77376 | TestF1: 0.74\n",
      "Epoch: 79 |  TrainLoss: 0.47336 | TestLoss: 0.50377 | TestAcc: 0.82805 | TestF1: 0.84\n",
      "Epoch: 80 |  TrainLoss: 0.47872 | TestLoss: 0.49527 | TestAcc: 0.80090 | TestF1: 0.78\n",
      "Epoch: 81 |  TrainLoss: 0.45389 | TestLoss: 0.49819 | TestAcc: 0.77828 | TestF1: 0.75\n",
      "Epoch: 82 |  TrainLoss: 0.43179 | TestLoss: 0.46691 | TestAcc: 0.83710 | TestF1: 0.84\n",
      "Epoch: 83 |  TrainLoss: 0.41602 | TestLoss: 0.45711 | TestAcc: 0.83710 | TestF1: 0.84\n",
      "Epoch: 84 |  TrainLoss: 0.40265 | TestLoss: 0.48083 | TestAcc: 0.77828 | TestF1: 0.75\n",
      "Epoch: 85 |  TrainLoss: 0.41241 | TestLoss: 0.43937 | TestAcc: 0.85068 | TestF1: 0.86\n",
      "Epoch: 86 |  TrainLoss: 0.38856 | TestLoss: 0.44756 | TestAcc: 0.80995 | TestF1: 0.80\n",
      "Epoch: 87 |  TrainLoss: 0.37001 | TestLoss: 0.42980 | TestAcc: 0.83258 | TestF1: 0.83\n",
      "Epoch: 88 |  TrainLoss: 0.35752 | TestLoss: 0.41579 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 89 |  TrainLoss: 0.34727 | TestLoss: 0.45295 | TestAcc: 0.78733 | TestF1: 0.76\n",
      "Epoch: 90 |  TrainLoss: 0.35618 | TestLoss: 0.41837 | TestAcc: 0.81900 | TestF1: 0.84\n",
      "Epoch: 91 |  TrainLoss: 0.34460 | TestLoss: 0.43560 | TestAcc: 0.80995 | TestF1: 0.79\n",
      "Epoch: 92 |  TrainLoss: 0.31110 | TestLoss: 0.41761 | TestAcc: 0.81448 | TestF1: 0.81\n",
      "Epoch: 93 |  TrainLoss: 0.30816 | TestLoss: 0.42651 | TestAcc: 0.81900 | TestF1: 0.84\n",
      "Epoch: 94 |  TrainLoss: 0.35793 | TestLoss: 0.48261 | TestAcc: 0.76471 | TestF1: 0.73\n",
      "Epoch: 95 |  TrainLoss: 0.29483 | TestLoss: 0.40965 | TestAcc: 0.83258 | TestF1: 0.83\n",
      "Epoch: 96 |  TrainLoss: 0.28477 | TestLoss: 0.43544 | TestAcc: 0.81448 | TestF1: 0.84\n",
      "Epoch: 97 |  TrainLoss: 0.31903 | TestLoss: 0.40016 | TestAcc: 0.83258 | TestF1: 0.83\n",
      "Epoch: 98 |  TrainLoss: 0.27372 | TestLoss: 0.50258 | TestAcc: 0.76471 | TestF1: 0.72\n",
      "Epoch: 99 |  TrainLoss: 0.31226 | TestLoss: 0.39202 | TestAcc: 0.83710 | TestF1: 0.85\n",
      "Epoch: 100 |  TrainLoss: 0.29504 | TestLoss: 0.38771 | TestAcc: 0.85068 | TestF1: 0.86\n",
      "Epoch: 101 |  TrainLoss: 0.27915 | TestLoss: 0.43579 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 102 |  TrainLoss: 0.26804 | TestLoss: 0.40696 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 103 |  TrainLoss: 0.25683 | TestLoss: 0.38110 | TestAcc: 0.85068 | TestF1: 0.86\n",
      "Epoch: 104 |  TrainLoss: 0.26951 | TestLoss: 0.39742 | TestAcc: 0.83258 | TestF1: 0.83\n",
      "Epoch: 105 |  TrainLoss: 0.24264 | TestLoss: 0.41982 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 106 |  TrainLoss: 0.24311 | TestLoss: 0.38608 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 107 |  TrainLoss: 0.22222 | TestLoss: 0.38054 | TestAcc: 0.85068 | TestF1: 0.86\n",
      "Epoch: 108 |  TrainLoss: 0.23089 | TestLoss: 0.39321 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 109 |  TrainLoss: 0.23305 | TestLoss: 0.39552 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 110 |  TrainLoss: 0.21965 | TestLoss: 0.38339 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 111 |  TrainLoss: 0.21035 | TestLoss: 0.39919 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 112 |  TrainLoss: 0.20835 | TestLoss: 0.40506 | TestAcc: 0.83258 | TestF1: 0.83\n",
      "Epoch: 113 |  TrainLoss: 0.21279 | TestLoss: 0.37812 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 114 |  TrainLoss: 0.21088 | TestLoss: 0.40408 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 115 |  TrainLoss: 0.21188 | TestLoss: 0.38218 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 116 |  TrainLoss: 0.21706 | TestLoss: 0.39964 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 117 |  TrainLoss: 0.17810 | TestLoss: 0.39967 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 118 |  TrainLoss: 0.19822 | TestLoss: 0.37750 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 119 |  TrainLoss: 0.18369 | TestLoss: 0.39423 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 120 |  TrainLoss: 0.21061 | TestLoss: 0.39311 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 121 |  TrainLoss: 0.18665 | TestLoss: 0.38437 | TestAcc: 0.85520 | TestF1: 0.87\n",
      "Epoch: 122 |  TrainLoss: 0.17155 | TestLoss: 0.45644 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 123 |  TrainLoss: 0.19113 | TestLoss: 0.37256 | TestAcc: 0.85068 | TestF1: 0.86\n",
      "Epoch: 124 |  TrainLoss: 0.17712 | TestLoss: 0.37919 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 125 |  TrainLoss: 0.16019 | TestLoss: 0.42415 | TestAcc: 0.83258 | TestF1: 0.82\n",
      "Epoch: 126 |  TrainLoss: 0.17652 | TestLoss: 0.38032 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 127 |  TrainLoss: 0.18873 | TestLoss: 0.40579 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 128 |  TrainLoss: 0.14833 | TestLoss: 0.42728 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 129 |  TrainLoss: 0.13564 | TestLoss: 0.38873 | TestAcc: 0.85068 | TestF1: 0.86\n",
      "Epoch: 130 |  TrainLoss: 0.17876 | TestLoss: 0.39920 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 131 |  TrainLoss: 0.15648 | TestLoss: 0.42457 | TestAcc: 0.84163 | TestF1: 0.83\n",
      "Epoch: 132 |  TrainLoss: 0.14119 | TestLoss: 0.39131 | TestAcc: 0.84615 | TestF1: 0.86\n",
      "Epoch: 133 |  TrainLoss: 0.16967 | TestLoss: 0.37571 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 134 |  TrainLoss: 0.17498 | TestLoss: 0.45571 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 135 |  TrainLoss: 0.16087 | TestLoss: 0.36248 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 136 |  TrainLoss: 0.11857 | TestLoss: 0.36683 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 137 |  TrainLoss: 0.11388 | TestLoss: 0.48419 | TestAcc: 0.80543 | TestF1: 0.78\n",
      "Epoch: 138 |  TrainLoss: 0.15442 | TestLoss: 0.36592 | TestAcc: 0.85973 | TestF1: 0.87\n",
      "Epoch: 139 |  TrainLoss: 0.15592 | TestLoss: 0.36864 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 140 |  TrainLoss: 0.11864 | TestLoss: 0.44332 | TestAcc: 0.84163 | TestF1: 0.83\n",
      "Epoch: 141 |  TrainLoss: 0.12063 | TestLoss: 0.36337 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 142 |  TrainLoss: 0.11763 | TestLoss: 0.36457 | TestAcc: 0.85973 | TestF1: 0.87\n",
      "Epoch: 143 |  TrainLoss: 0.13657 | TestLoss: 0.42877 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 144 |  TrainLoss: 0.12930 | TestLoss: 0.38420 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 145 |  TrainLoss: 0.09465 | TestLoss: 0.36704 | TestAcc: 0.85973 | TestF1: 0.87\n",
      "Epoch: 146 |  TrainLoss: 0.14258 | TestLoss: 0.43698 | TestAcc: 0.85068 | TestF1: 0.84\n",
      "Epoch: 147 |  TrainLoss: 0.10955 | TestLoss: 0.39140 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 148 |  TrainLoss: 0.11247 | TestLoss: 0.37785 | TestAcc: 0.86878 | TestF1: 0.88\n",
      "Epoch: 149 |  TrainLoss: 0.15217 | TestLoss: 0.40929 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 150 |  TrainLoss: 0.10417 | TestLoss: 0.40362 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 151 |  TrainLoss: 0.11975 | TestLoss: 0.38159 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 152 |  TrainLoss: 0.14856 | TestLoss: 0.42783 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 153 |  TrainLoss: 0.10888 | TestLoss: 0.47578 | TestAcc: 0.83258 | TestF1: 0.82\n",
      "Epoch: 154 |  TrainLoss: 0.14256 | TestLoss: 0.52445 | TestAcc: 0.80090 | TestF1: 0.83\n",
      "Epoch: 155 |  TrainLoss: 0.27241 | TestLoss: 0.37064 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 156 |  TrainLoss: 0.10156 | TestLoss: 0.69177 | TestAcc: 0.75113 | TestF1: 0.69\n",
      "Epoch: 157 |  TrainLoss: 0.23014 | TestLoss: 0.43164 | TestAcc: 0.83258 | TestF1: 0.85\n",
      "Epoch: 158 |  TrainLoss: 0.20092 | TestLoss: 0.44297 | TestAcc: 0.83710 | TestF1: 0.86\n",
      "Epoch: 159 |  TrainLoss: 0.19830 | TestLoss: 0.58327 | TestAcc: 0.79186 | TestF1: 0.76\n",
      "Epoch: 160 |  TrainLoss: 0.17416 | TestLoss: 0.43490 | TestAcc: 0.84615 | TestF1: 0.83\n",
      "Epoch: 161 |  TrainLoss: 0.08944 | TestLoss: 0.38888 | TestAcc: 0.85973 | TestF1: 0.87\n",
      "Epoch: 162 |  TrainLoss: 0.12901 | TestLoss: 0.37147 | TestAcc: 0.85973 | TestF1: 0.87\n",
      "Epoch: 163 |  TrainLoss: 0.12998 | TestLoss: 0.48478 | TestAcc: 0.82805 | TestF1: 0.81\n",
      "Epoch: 164 |  TrainLoss: 0.10638 | TestLoss: 0.48309 | TestAcc: 0.83258 | TestF1: 0.82\n",
      "Epoch: 165 |  TrainLoss: 0.11894 | TestLoss: 0.38017 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 166 |  TrainLoss: 0.12773 | TestLoss: 0.37908 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 167 |  TrainLoss: 0.10419 | TestLoss: 0.41168 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 168 |  TrainLoss: 0.07311 | TestLoss: 0.47875 | TestAcc: 0.83258 | TestF1: 0.82\n",
      "Epoch: 169 |  TrainLoss: 0.10853 | TestLoss: 0.36316 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 170 |  TrainLoss: 0.07956 | TestLoss: 0.37885 | TestAcc: 0.85068 | TestF1: 0.86\n",
      "Epoch: 171 |  TrainLoss: 0.11020 | TestLoss: 0.40034 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 172 |  TrainLoss: 0.06300 | TestLoss: 0.52153 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 173 |  TrainLoss: 0.11978 | TestLoss: 0.36576 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 174 |  TrainLoss: 0.08103 | TestLoss: 0.37635 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 175 |  TrainLoss: 0.08160 | TestLoss: 0.38422 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 176 |  TrainLoss: 0.06614 | TestLoss: 0.48194 | TestAcc: 0.83710 | TestF1: 0.82\n",
      "Epoch: 177 |  TrainLoss: 0.09271 | TestLoss: 0.37457 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 178 |  TrainLoss: 0.06936 | TestLoss: 0.37049 | TestAcc: 0.85973 | TestF1: 0.87\n",
      "Epoch: 179 |  TrainLoss: 0.07799 | TestLoss: 0.38041 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 180 |  TrainLoss: 0.08091 | TestLoss: 0.47266 | TestAcc: 0.83710 | TestF1: 0.82\n",
      "Epoch: 181 |  TrainLoss: 0.08725 | TestLoss: 0.38462 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 182 |  TrainLoss: 0.06129 | TestLoss: 0.37202 | TestAcc: 0.85973 | TestF1: 0.87\n",
      "Epoch: 183 |  TrainLoss: 0.06637 | TestLoss: 0.37153 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 184 |  TrainLoss: 0.05445 | TestLoss: 0.45324 | TestAcc: 0.84615 | TestF1: 0.83\n",
      "Epoch: 185 |  TrainLoss: 0.06556 | TestLoss: 0.40908 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 186 |  TrainLoss: 0.05667 | TestLoss: 0.37062 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 187 |  TrainLoss: 0.06694 | TestLoss: 0.38399 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 188 |  TrainLoss: 0.04973 | TestLoss: 0.40248 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 189 |  TrainLoss: 0.05211 | TestLoss: 0.39915 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 190 |  TrainLoss: 0.05133 | TestLoss: 0.37450 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 191 |  TrainLoss: 0.04672 | TestLoss: 0.38039 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 192 |  TrainLoss: 0.05205 | TestLoss: 0.42218 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 193 |  TrainLoss: 0.05186 | TestLoss: 0.41528 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 194 |  TrainLoss: 0.04916 | TestLoss: 0.38168 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 195 |  TrainLoss: 0.04570 | TestLoss: 0.38180 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 196 |  TrainLoss: 0.04639 | TestLoss: 0.42332 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 197 |  TrainLoss: 0.05352 | TestLoss: 0.42041 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 198 |  TrainLoss: 0.04989 | TestLoss: 0.38660 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 199 |  TrainLoss: 0.02937 | TestLoss: 0.38674 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 200 |  TrainLoss: 0.05034 | TestLoss: 0.42099 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 201 |  TrainLoss: 0.03490 | TestLoss: 0.46316 | TestAcc: 0.85520 | TestF1: 0.84\n",
      "Epoch: 202 |  TrainLoss: 0.05482 | TestLoss: 0.39643 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 203 |  TrainLoss: 0.03524 | TestLoss: 0.38989 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 204 |  TrainLoss: 0.05773 | TestLoss: 0.40372 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 205 |  TrainLoss: 0.04040 | TestLoss: 0.48921 | TestAcc: 0.85068 | TestF1: 0.84\n",
      "Epoch: 206 |  TrainLoss: 0.05443 | TestLoss: 0.40014 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 207 |  TrainLoss: 0.04453 | TestLoss: 0.39252 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 208 |  TrainLoss: 0.03028 | TestLoss: 0.39993 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 209 |  TrainLoss: 0.02771 | TestLoss: 0.44494 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 210 |  TrainLoss: 0.03875 | TestLoss: 0.42355 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 211 |  TrainLoss: 0.03389 | TestLoss: 0.40026 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 212 |  TrainLoss: 0.02972 | TestLoss: 0.40191 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 213 |  TrainLoss: 0.03194 | TestLoss: 0.41941 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 214 |  TrainLoss: 0.02103 | TestLoss: 0.44216 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 215 |  TrainLoss: 0.02982 | TestLoss: 0.41268 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 216 |  TrainLoss: 0.03020 | TestLoss: 0.40513 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 217 |  TrainLoss: 0.04781 | TestLoss: 0.47408 | TestAcc: 0.85520 | TestF1: 0.84\n",
      "Epoch: 218 |  TrainLoss: 0.03408 | TestLoss: 0.45147 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 219 |  TrainLoss: 0.02212 | TestLoss: 0.40975 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 220 |  TrainLoss: 0.03548 | TestLoss: 0.41188 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 221 |  TrainLoss: 0.03461 | TestLoss: 0.44984 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 222 |  TrainLoss: 0.02331 | TestLoss: 0.47319 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 223 |  TrainLoss: 0.03542 | TestLoss: 0.42608 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 224 |  TrainLoss: 0.02234 | TestLoss: 0.41717 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 225 |  TrainLoss: 0.02953 | TestLoss: 0.45382 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 226 |  TrainLoss: 0.02581 | TestLoss: 0.48531 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 227 |  TrainLoss: 0.03348 | TestLoss: 0.42036 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 228 |  TrainLoss: 0.02620 | TestLoss: 0.42554 | TestAcc: 0.85973 | TestF1: 0.87\n",
      "Epoch: 229 |  TrainLoss: 0.03258 | TestLoss: 0.44592 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 230 |  TrainLoss: 0.02251 | TestLoss: 0.49029 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 231 |  TrainLoss: 0.02548 | TestLoss: 0.47852 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 232 |  TrainLoss: 0.01877 | TestLoss: 0.42996 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 233 |  TrainLoss: 0.02157 | TestLoss: 0.43473 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 234 |  TrainLoss: 0.01649 | TestLoss: 0.46325 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 235 |  TrainLoss: 0.01839 | TestLoss: 0.44841 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 236 |  TrainLoss: 0.02194 | TestLoss: 0.47359 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 237 |  TrainLoss: 0.01764 | TestLoss: 0.45189 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 238 |  TrainLoss: 0.01090 | TestLoss: 0.43876 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 239 |  TrainLoss: 0.01660 | TestLoss: 0.44320 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 240 |  TrainLoss: 0.02153 | TestLoss: 0.47548 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 241 |  TrainLoss: 0.01468 | TestLoss: 0.47945 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 242 |  TrainLoss: 0.01473 | TestLoss: 0.44536 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 243 |  TrainLoss: 0.01428 | TestLoss: 0.44060 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 244 |  TrainLoss: 0.02130 | TestLoss: 0.48002 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 245 |  TrainLoss: 0.01803 | TestLoss: 0.48595 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 246 |  TrainLoss: 0.01814 | TestLoss: 0.44659 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 247 |  TrainLoss: 0.01802 | TestLoss: 0.45180 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 248 |  TrainLoss: 0.01059 | TestLoss: 0.45407 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 249 |  TrainLoss: 0.01075 | TestLoss: 0.46796 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 250 |  TrainLoss: 0.01168 | TestLoss: 0.46503 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 251 |  TrainLoss: 0.01654 | TestLoss: 0.47891 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 252 |  TrainLoss: 0.01054 | TestLoss: 0.47784 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 253 |  TrainLoss: 0.01820 | TestLoss: 0.49833 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 254 |  TrainLoss: 0.01138 | TestLoss: 0.48566 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 255 |  TrainLoss: 0.01374 | TestLoss: 0.47190 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 256 |  TrainLoss: 0.01273 | TestLoss: 0.47071 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 257 |  TrainLoss: 0.01281 | TestLoss: 0.48656 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 258 |  TrainLoss: 0.00780 | TestLoss: 0.48870 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 259 |  TrainLoss: 0.01618 | TestLoss: 0.47612 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 260 |  TrainLoss: 0.00832 | TestLoss: 0.47056 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 261 |  TrainLoss: 0.01430 | TestLoss: 0.48602 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 262 |  TrainLoss: 0.00794 | TestLoss: 0.50690 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 263 |  TrainLoss: 0.01078 | TestLoss: 0.49268 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 264 |  TrainLoss: 0.00969 | TestLoss: 0.47665 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 265 |  TrainLoss: 0.00748 | TestLoss: 0.47452 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 266 |  TrainLoss: 0.01149 | TestLoss: 0.50491 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 267 |  TrainLoss: 0.00956 | TestLoss: 0.51095 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 268 |  TrainLoss: 0.01425 | TestLoss: 0.47537 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 269 |  TrainLoss: 0.00859 | TestLoss: 0.47346 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 270 |  TrainLoss: 0.02010 | TestLoss: 0.54637 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 271 |  TrainLoss: 0.01191 | TestLoss: 0.56213 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 272 |  TrainLoss: 0.01151 | TestLoss: 0.49327 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 273 |  TrainLoss: 0.00640 | TestLoss: 0.47889 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 274 |  TrainLoss: 0.00869 | TestLoss: 0.48064 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 275 |  TrainLoss: 0.01173 | TestLoss: 0.53780 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 276 |  TrainLoss: 0.00785 | TestLoss: 0.56040 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 277 |  TrainLoss: 0.00791 | TestLoss: 0.53404 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 278 |  TrainLoss: 0.00686 | TestLoss: 0.50581 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 279 |  TrainLoss: 0.00592 | TestLoss: 0.48869 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 280 |  TrainLoss: 0.00957 | TestLoss: 0.49636 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 281 |  TrainLoss: 0.00596 | TestLoss: 0.52123 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 282 |  TrainLoss: 0.00906 | TestLoss: 0.51898 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 283 |  TrainLoss: 0.00573 | TestLoss: 0.51212 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 284 |  TrainLoss: 0.01205 | TestLoss: 0.49842 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 285 |  TrainLoss: 0.00961 | TestLoss: 0.50420 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 286 |  TrainLoss: 0.00742 | TestLoss: 0.53708 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 287 |  TrainLoss: 0.00708 | TestLoss: 0.54325 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 288 |  TrainLoss: 0.00679 | TestLoss: 0.51116 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 289 |  TrainLoss: 0.00421 | TestLoss: 0.50050 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 290 |  TrainLoss: 0.00568 | TestLoss: 0.50351 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 291 |  TrainLoss: 0.01082 | TestLoss: 0.56441 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 292 |  TrainLoss: 0.00643 | TestLoss: 0.62872 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 293 |  TrainLoss: 0.02132 | TestLoss: 0.49933 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 294 |  TrainLoss: 0.01155 | TestLoss: 0.51432 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 295 |  TrainLoss: 0.01734 | TestLoss: 0.54440 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 296 |  TrainLoss: 0.00490 | TestLoss: 0.70400 | TestAcc: 0.82805 | TestF1: 0.81\n",
      "Epoch: 297 |  TrainLoss: 0.02447 | TestLoss: 0.50870 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 298 |  TrainLoss: 0.00696 | TestLoss: 0.55054 | TestAcc: 0.83258 | TestF1: 0.85\n",
      "Epoch: 299 |  TrainLoss: 0.04843 | TestLoss: 0.69373 | TestAcc: 0.83710 | TestF1: 0.82\n",
      "Epoch: 300 |  TrainLoss: 0.01954 | TestLoss: 0.78110 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 301 |  TrainLoss: 0.03003 | TestLoss: 0.52502 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 302 |  TrainLoss: 0.01769 | TestLoss: 0.62383 | TestAcc: 0.81900 | TestF1: 0.84\n",
      "Epoch: 303 |  TrainLoss: 0.08467 | TestLoss: 1.16209 | TestAcc: 0.75113 | TestF1: 0.69\n",
      "Epoch: 304 |  TrainLoss: 0.20187 | TestLoss: 1.47546 | TestAcc: 0.71041 | TestF1: 0.78\n",
      "Epoch: 305 |  TrainLoss: 0.91824 | TestLoss: 1.49956 | TestAcc: 0.69683 | TestF1: 0.58\n",
      "Epoch: 306 |  TrainLoss: 0.51210 | TestLoss: 0.60530 | TestAcc: 0.83710 | TestF1: 0.85\n",
      "Epoch: 307 |  TrainLoss: 0.06069 | TestLoss: 1.21518 | TestAcc: 0.75113 | TestF1: 0.80\n",
      "Epoch: 308 |  TrainLoss: 0.68983 | TestLoss: 0.74822 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 309 |  TrainLoss: 0.05102 | TestLoss: 2.40154 | TestAcc: 0.58371 | TestF1: 0.31\n",
      "Epoch: 310 |  TrainLoss: 1.33910 | TestLoss: 0.70938 | TestAcc: 0.83258 | TestF1: 0.86\n",
      "Epoch: 311 |  TrainLoss: 0.18772 | TestLoss: 1.86299 | TestAcc: 0.63348 | TestF1: 0.73\n",
      "Epoch: 312 |  TrainLoss: 1.65912 | TestLoss: 0.62929 | TestAcc: 0.83258 | TestF1: 0.86\n",
      "Epoch: 313 |  TrainLoss: 0.14330 | TestLoss: 1.41359 | TestAcc: 0.68326 | TestF1: 0.55\n",
      "Epoch: 314 |  TrainLoss: 0.57893 | TestLoss: 0.93513 | TestAcc: 0.76471 | TestF1: 0.71\n",
      "Epoch: 315 |  TrainLoss: 0.19331 | TestLoss: 0.46009 | TestAcc: 0.84615 | TestF1: 0.86\n",
      "Epoch: 316 |  TrainLoss: 0.08752 | TestLoss: 0.83503 | TestAcc: 0.78281 | TestF1: 0.82\n",
      "Epoch: 317 |  TrainLoss: 0.40961 | TestLoss: 0.64462 | TestAcc: 0.81900 | TestF1: 0.85\n",
      "Epoch: 318 |  TrainLoss: 0.26059 | TestLoss: 0.45919 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 319 |  TrainLoss: 0.01887 | TestLoss: 1.07187 | TestAcc: 0.72398 | TestF1: 0.63\n",
      "Epoch: 320 |  TrainLoss: 0.32802 | TestLoss: 0.61255 | TestAcc: 0.82805 | TestF1: 0.81\n",
      "Epoch: 321 |  TrainLoss: 0.05440 | TestLoss: 0.39617 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 322 |  TrainLoss: 0.03411 | TestLoss: 0.52101 | TestAcc: 0.82353 | TestF1: 0.85\n",
      "Epoch: 323 |  TrainLoss: 0.17603 | TestLoss: 0.49019 | TestAcc: 0.83258 | TestF1: 0.85\n",
      "Epoch: 324 |  TrainLoss: 0.15008 | TestLoss: 0.39281 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 325 |  TrainLoss: 0.03027 | TestLoss: 0.54743 | TestAcc: 0.84615 | TestF1: 0.83\n",
      "Epoch: 326 |  TrainLoss: 0.04762 | TestLoss: 0.73586 | TestAcc: 0.78281 | TestF1: 0.74\n",
      "Epoch: 327 |  TrainLoss: 0.12748 | TestLoss: 0.55606 | TestAcc: 0.84163 | TestF1: 0.83\n",
      "Epoch: 328 |  TrainLoss: 0.05970 | TestLoss: 0.39947 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 329 |  TrainLoss: 0.02603 | TestLoss: 0.41425 | TestAcc: 0.83710 | TestF1: 0.85\n",
      "Epoch: 330 |  TrainLoss: 0.06862 | TestLoss: 0.43937 | TestAcc: 0.84615 | TestF1: 0.86\n",
      "Epoch: 331 |  TrainLoss: 0.07917 | TestLoss: 0.40132 | TestAcc: 0.85068 | TestF1: 0.86\n",
      "Epoch: 332 |  TrainLoss: 0.06113 | TestLoss: 0.41197 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 333 |  TrainLoss: 0.02484 | TestLoss: 0.52819 | TestAcc: 0.85068 | TestF1: 0.84\n",
      "Epoch: 334 |  TrainLoss: 0.03680 | TestLoss: 0.63659 | TestAcc: 0.79186 | TestF1: 0.76\n",
      "Epoch: 335 |  TrainLoss: 0.07143 | TestLoss: 0.57128 | TestAcc: 0.82805 | TestF1: 0.81\n",
      "Epoch: 336 |  TrainLoss: 0.04832 | TestLoss: 0.45082 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 337 |  TrainLoss: 0.03026 | TestLoss: 0.39569 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 338 |  TrainLoss: 0.02733 | TestLoss: 0.40638 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 339 |  TrainLoss: 0.05666 | TestLoss: 0.40251 | TestAcc: 0.85973 | TestF1: 0.87\n",
      "Epoch: 340 |  TrainLoss: 0.05196 | TestLoss: 0.40418 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 341 |  TrainLoss: 0.02462 | TestLoss: 0.46007 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 342 |  TrainLoss: 0.02590 | TestLoss: 0.52454 | TestAcc: 0.85520 | TestF1: 0.84\n",
      "Epoch: 343 |  TrainLoss: 0.03318 | TestLoss: 0.54545 | TestAcc: 0.84615 | TestF1: 0.83\n",
      "Epoch: 344 |  TrainLoss: 0.03074 | TestLoss: 0.51162 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 345 |  TrainLoss: 0.03213 | TestLoss: 0.44833 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 346 |  TrainLoss: 0.01775 | TestLoss: 0.41205 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 347 |  TrainLoss: 0.02366 | TestLoss: 0.40564 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 348 |  TrainLoss: 0.02457 | TestLoss: 0.40713 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 349 |  TrainLoss: 0.02870 | TestLoss: 0.40993 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 350 |  TrainLoss: 0.02482 | TestLoss: 0.42953 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 351 |  TrainLoss: 0.01779 | TestLoss: 0.46796 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 352 |  TrainLoss: 0.01735 | TestLoss: 0.50707 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 353 |  TrainLoss: 0.02421 | TestLoss: 0.51334 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 354 |  TrainLoss: 0.02357 | TestLoss: 0.48940 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 355 |  TrainLoss: 0.01818 | TestLoss: 0.45633 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 356 |  TrainLoss: 0.01590 | TestLoss: 0.43010 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 357 |  TrainLoss: 0.01927 | TestLoss: 0.42187 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 358 |  TrainLoss: 0.01592 | TestLoss: 0.42128 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 359 |  TrainLoss: 0.01770 | TestLoss: 0.42480 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 360 |  TrainLoss: 0.01430 | TestLoss: 0.43323 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 361 |  TrainLoss: 0.01590 | TestLoss: 0.45044 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 362 |  TrainLoss: 0.01308 | TestLoss: 0.47445 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 363 |  TrainLoss: 0.01242 | TestLoss: 0.49576 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 364 |  TrainLoss: 0.01314 | TestLoss: 0.50713 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 365 |  TrainLoss: 0.01353 | TestLoss: 0.50356 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 366 |  TrainLoss: 0.01484 | TestLoss: 0.48736 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 367 |  TrainLoss: 0.01180 | TestLoss: 0.47305 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 368 |  TrainLoss: 0.01392 | TestLoss: 0.46178 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 369 |  TrainLoss: 0.01008 | TestLoss: 0.45375 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 370 |  TrainLoss: 0.01279 | TestLoss: 0.45147 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 371 |  TrainLoss: 0.01400 | TestLoss: 0.45677 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 372 |  TrainLoss: 0.01096 | TestLoss: 0.46482 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 373 |  TrainLoss: 0.01008 | TestLoss: 0.47331 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 374 |  TrainLoss: 0.01013 | TestLoss: 0.48529 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 375 |  TrainLoss: 0.01197 | TestLoss: 0.49349 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 376 |  TrainLoss: 0.00966 | TestLoss: 0.49524 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 377 |  TrainLoss: 0.01071 | TestLoss: 0.49229 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 378 |  TrainLoss: 0.00809 | TestLoss: 0.48711 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 379 |  TrainLoss: 0.01057 | TestLoss: 0.48367 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 380 |  TrainLoss: 0.01071 | TestLoss: 0.47949 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 381 |  TrainLoss: 0.00769 | TestLoss: 0.47617 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 382 |  TrainLoss: 0.00811 | TestLoss: 0.47335 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 383 |  TrainLoss: 0.01087 | TestLoss: 0.47756 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 384 |  TrainLoss: 0.00793 | TestLoss: 0.48218 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 385 |  TrainLoss: 0.00768 | TestLoss: 0.48738 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 386 |  TrainLoss: 0.00829 | TestLoss: 0.49333 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 387 |  TrainLoss: 0.00735 | TestLoss: 0.49777 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 388 |  TrainLoss: 0.00753 | TestLoss: 0.49764 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 389 |  TrainLoss: 0.00868 | TestLoss: 0.49607 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 390 |  TrainLoss: 0.00818 | TestLoss: 0.49736 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 391 |  TrainLoss: 0.00736 | TestLoss: 0.50177 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 392 |  TrainLoss: 0.01062 | TestLoss: 0.50567 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 393 |  TrainLoss: 0.00691 | TestLoss: 0.50674 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 394 |  TrainLoss: 0.00863 | TestLoss: 0.51251 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 395 |  TrainLoss: 0.00784 | TestLoss: 0.51108 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 396 |  TrainLoss: 0.00893 | TestLoss: 0.50933 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 397 |  TrainLoss: 0.00856 | TestLoss: 0.50430 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 398 |  TrainLoss: 0.00648 | TestLoss: 0.50291 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 399 |  TrainLoss: 0.00845 | TestLoss: 0.50408 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 400 |  TrainLoss: 0.00549 | TestLoss: 0.50668 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 401 |  TrainLoss: 0.00704 | TestLoss: 0.50564 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 402 |  TrainLoss: 0.00660 | TestLoss: 0.50742 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 403 |  TrainLoss: 0.00561 | TestLoss: 0.51330 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 404 |  TrainLoss: 0.00551 | TestLoss: 0.51900 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 405 |  TrainLoss: 0.00828 | TestLoss: 0.52169 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 406 |  TrainLoss: 0.00628 | TestLoss: 0.52495 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 407 |  TrainLoss: 0.00651 | TestLoss: 0.52509 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 408 |  TrainLoss: 0.00809 | TestLoss: 0.53544 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 409 |  TrainLoss: 0.00654 | TestLoss: 0.54033 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 410 |  TrainLoss: 0.00696 | TestLoss: 0.53478 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 411 |  TrainLoss: 0.00525 | TestLoss: 0.52873 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 412 |  TrainLoss: 0.00655 | TestLoss: 0.52552 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 413 |  TrainLoss: 0.00396 | TestLoss: 0.52379 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 414 |  TrainLoss: 0.00627 | TestLoss: 0.52292 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 415 |  TrainLoss: 0.00598 | TestLoss: 0.52404 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 416 |  TrainLoss: 0.00501 | TestLoss: 0.52960 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 417 |  TrainLoss: 0.00473 | TestLoss: 0.53449 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 418 |  TrainLoss: 0.00717 | TestLoss: 0.54027 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 419 |  TrainLoss: 0.00619 | TestLoss: 0.54074 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 420 |  TrainLoss: 0.00352 | TestLoss: 0.53926 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 421 |  TrainLoss: 0.00430 | TestLoss: 0.53610 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 422 |  TrainLoss: 0.00570 | TestLoss: 0.53310 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 423 |  TrainLoss: 0.00518 | TestLoss: 0.52870 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 424 |  TrainLoss: 0.00402 | TestLoss: 0.52394 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 425 |  TrainLoss: 0.00332 | TestLoss: 0.52175 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 426 |  TrainLoss: 0.00441 | TestLoss: 0.52236 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 427 |  TrainLoss: 0.00573 | TestLoss: 0.52604 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 428 |  TrainLoss: 0.00470 | TestLoss: 0.53172 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 429 |  TrainLoss: 0.00471 | TestLoss: 0.53988 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 430 |  TrainLoss: 0.00549 | TestLoss: 0.54696 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 431 |  TrainLoss: 0.00394 | TestLoss: 0.55317 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 432 |  TrainLoss: 0.00586 | TestLoss: 0.55651 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 433 |  TrainLoss: 0.00390 | TestLoss: 0.55608 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 434 |  TrainLoss: 0.00426 | TestLoss: 0.55386 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 435 |  TrainLoss: 0.00527 | TestLoss: 0.54606 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 436 |  TrainLoss: 0.00444 | TestLoss: 0.53647 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 437 |  TrainLoss: 0.00508 | TestLoss: 0.52771 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 438 |  TrainLoss: 0.00605 | TestLoss: 0.53301 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 439 |  TrainLoss: 0.00400 | TestLoss: 0.54151 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 440 |  TrainLoss: 0.00717 | TestLoss: 0.55430 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 441 |  TrainLoss: 0.00469 | TestLoss: 0.56202 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 442 |  TrainLoss: 0.00319 | TestLoss: 0.56732 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 443 |  TrainLoss: 0.00537 | TestLoss: 0.56530 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 444 |  TrainLoss: 0.00366 | TestLoss: 0.55939 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 445 |  TrainLoss: 0.00322 | TestLoss: 0.55390 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 446 |  TrainLoss: 0.00426 | TestLoss: 0.54841 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 447 |  TrainLoss: 0.00424 | TestLoss: 0.54435 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 448 |  TrainLoss: 0.00343 | TestLoss: 0.54152 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 449 |  TrainLoss: 0.00558 | TestLoss: 0.54802 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 450 |  TrainLoss: 0.00508 | TestLoss: 0.55645 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 451 |  TrainLoss: 0.00338 | TestLoss: 0.56861 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 452 |  TrainLoss: 0.00318 | TestLoss: 0.57900 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 453 |  TrainLoss: 0.00327 | TestLoss: 0.58398 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 454 |  TrainLoss: 0.00339 | TestLoss: 0.58270 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 455 |  TrainLoss: 0.00362 | TestLoss: 0.57531 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 456 |  TrainLoss: 0.00354 | TestLoss: 0.56299 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 457 |  TrainLoss: 0.00440 | TestLoss: 0.55697 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 458 |  TrainLoss: 0.00349 | TestLoss: 0.55213 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 459 |  TrainLoss: 0.00347 | TestLoss: 0.55098 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 460 |  TrainLoss: 0.00393 | TestLoss: 0.55585 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 461 |  TrainLoss: 0.00360 | TestLoss: 0.56284 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 462 |  TrainLoss: 0.00378 | TestLoss: 0.56803 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 463 |  TrainLoss: 0.00307 | TestLoss: 0.57272 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 464 |  TrainLoss: 0.00350 | TestLoss: 0.57449 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 465 |  TrainLoss: 0.00322 | TestLoss: 0.57359 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 466 |  TrainLoss: 0.00414 | TestLoss: 0.57563 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 467 |  TrainLoss: 0.00449 | TestLoss: 0.57070 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 468 |  TrainLoss: 0.00277 | TestLoss: 0.56696 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 469 |  TrainLoss: 0.00360 | TestLoss: 0.56789 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 470 |  TrainLoss: 0.00304 | TestLoss: 0.56972 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 471 |  TrainLoss: 0.00398 | TestLoss: 0.56860 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 472 |  TrainLoss: 0.00318 | TestLoss: 0.57044 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 473 |  TrainLoss: 0.00360 | TestLoss: 0.57395 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 474 |  TrainLoss: 0.00588 | TestLoss: 0.57975 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 475 |  TrainLoss: 0.00743 | TestLoss: 0.59138 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 476 |  TrainLoss: 0.00254 | TestLoss: 0.59829 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 477 |  TrainLoss: 0.00248 | TestLoss: 0.60107 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 478 |  TrainLoss: 0.00286 | TestLoss: 0.59868 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 479 |  TrainLoss: 0.00333 | TestLoss: 0.60318 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 480 |  TrainLoss: 0.00330 | TestLoss: 0.59947 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 481 |  TrainLoss: 0.00282 | TestLoss: 0.59099 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 482 |  TrainLoss: 0.00275 | TestLoss: 0.57967 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 483 |  TrainLoss: 0.00209 | TestLoss: 0.57125 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 484 |  TrainLoss: 0.00285 | TestLoss: 0.56463 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 485 |  TrainLoss: 0.00261 | TestLoss: 0.56221 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 486 |  TrainLoss: 0.00394 | TestLoss: 0.56341 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 487 |  TrainLoss: 0.00238 | TestLoss: 0.56369 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 488 |  TrainLoss: 0.00213 | TestLoss: 0.56550 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 489 |  TrainLoss: 0.00396 | TestLoss: 0.57693 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 490 |  TrainLoss: 0.00328 | TestLoss: 0.59471 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 491 |  TrainLoss: 0.00202 | TestLoss: 0.61247 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 492 |  TrainLoss: 0.00215 | TestLoss: 0.62515 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 493 |  TrainLoss: 0.00251 | TestLoss: 0.62926 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 494 |  TrainLoss: 0.00295 | TestLoss: 0.62281 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 495 |  TrainLoss: 0.00392 | TestLoss: 0.60774 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 496 |  TrainLoss: 0.00216 | TestLoss: 0.59425 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 497 |  TrainLoss: 0.00247 | TestLoss: 0.58529 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 498 |  TrainLoss: 0.00254 | TestLoss: 0.58147 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 499 |  TrainLoss: 0.00328 | TestLoss: 0.57969 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Best WLoss: 0.00176 | Best Epoch: 145\n"
     ]
    }
   ],
   "source": [
    "#printing out the epoch at lowest wloss. \n",
    "wloss = []\n",
    "weighted_loss = 0\n",
    "exp_param = 0.8\n",
    "best_test_loss = float('inf')  # initialize with a large value\n",
    "\n",
    "#without dropout training results\n",
    "for epoch in range(500):\n",
    "  train_loss = train(epoch)\n",
    "  test_loss, test_acc, test_f1 = test(epoch)\n",
    "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
    "  \n",
    "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
    "  \n",
    "  if test_loss < best_test_loss:\n",
    "    best_test_loss = test_loss  # Update the best test loss\n",
    "\n",
    "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
    "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
    "\n",
    "#print the best values\n",
    "best_wloss = min(wloss)\n",
    "best_epoch = wloss.index(best_wloss)\n",
    "print(f'Best WLoss: {best_wloss:.5f} | Best Epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260ae15",
   "metadata": {},
   "source": [
    "# Model 2: \n",
    "\n",
    "GraphSage + Content with hyperparameters as defined by the paper and 3 Layer MLP\n",
    "\n",
    "(Embedding size = 128, batch size= 128, l2 Regularization = 0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29e128aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, embedding_size, batch_size, l2_reg_weight):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.batch_size = batch_size\n",
    "        self.l2_reg_weight = l2_reg_weight\n",
    "        \n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
    "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
    "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
    "        \n",
    "        self.full1 = Linear(hidden_channels[2], hidden_channels[3])\n",
    "        self.full2 = Linear(hidden_channels[3], hidden_channels[4])\n",
    "        self.full3 = Linear(hidden_channels[4], hidden_channels[5])\n",
    "\n",
    "        self.softmax = Linear(hidden_channels[5], out_channels)\n",
    "\n",
    "        # Dropouts\n",
    "        self.dp1 = Dropout(0.2)\n",
    "        self.dp2 = Dropout(0.2)\n",
    "        self.dp3 = Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = self.conv1(x, edge_index).relu()\n",
    "        h = self.conv2(h, edge_index).relu()\n",
    "        h = self.conv3(h, edge_index).relu()\n",
    "\n",
    "        h = global_max_pool(h, batch)\n",
    "\n",
    "        h = self.full1(h).relu()\n",
    "        h = self.dp1(h)\n",
    "        h = self.full2(h).relu()\n",
    "        h = self.dp2(h)\n",
    "        h = self.full3(h).relu()\n",
    "        h = self.dp3(h)\n",
    "        \n",
    "        h = self.softmax(h)\n",
    "\n",
    "        return torch.sigmoid(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec6961ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(test_data_pol.num_features,[512,512,512,256,256,256],1, 128, 128, 0.001).to(device) #setting embedding size=128, batch size=128, l2 regularization weight=0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "lossff = torch.nn.BCELoss()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2db1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        all_preds.append(torch.reshape(out, (-1,)))\n",
    "        all_labels.append(data.y.float())\n",
    "    # print(all_preds)\n",
    "    accuracy, f1 = metrics(all_preds, all_labels)\n",
    "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
    "\n",
    "\n",
    "def metrics(preds, gts):\n",
    "    preds = torch.round(torch.cat(preds))\n",
    "    gts = torch.cat(gts)\n",
    "    # print(preds.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b37f7cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 |  TrainLoss: 0.69222 | TestLoss: 0.69389 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 01 |  TrainLoss: 0.69244 | TestLoss: 0.69387 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 02 |  TrainLoss: 0.69245 | TestLoss: 0.69385 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 03 |  TrainLoss: 0.69243 | TestLoss: 0.69387 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 04 |  TrainLoss: 0.69078 | TestLoss: 0.69394 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 05 |  TrainLoss: 0.69128 | TestLoss: 0.69401 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 06 |  TrainLoss: 0.69146 | TestLoss: 0.69407 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 07 |  TrainLoss: 0.69201 | TestLoss: 0.69410 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 08 |  TrainLoss: 0.69262 | TestLoss: 0.69417 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 09 |  TrainLoss: 0.69138 | TestLoss: 0.69419 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 10 |  TrainLoss: 0.69220 | TestLoss: 0.69419 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 11 |  TrainLoss: 0.69057 | TestLoss: 0.69416 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 12 |  TrainLoss: 0.69191 | TestLoss: 0.69418 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 13 |  TrainLoss: 0.69067 | TestLoss: 0.69416 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 14 |  TrainLoss: 0.69129 | TestLoss: 0.69414 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 15 |  TrainLoss: 0.69051 | TestLoss: 0.69413 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 16 |  TrainLoss: 0.69141 | TestLoss: 0.69410 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 17 |  TrainLoss: 0.69169 | TestLoss: 0.69404 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 18 |  TrainLoss: 0.69112 | TestLoss: 0.69392 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 19 |  TrainLoss: 0.69029 | TestLoss: 0.69381 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 20 |  TrainLoss: 0.69024 | TestLoss: 0.69365 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 21 |  TrainLoss: 0.69121 | TestLoss: 0.69343 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 22 |  TrainLoss: 0.68872 | TestLoss: 0.69319 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 23 |  TrainLoss: 0.68963 | TestLoss: 0.69293 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 24 |  TrainLoss: 0.68887 | TestLoss: 0.69286 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 25 |  TrainLoss: 0.69007 | TestLoss: 0.69283 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 26 |  TrainLoss: 0.69018 | TestLoss: 0.69278 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 27 |  TrainLoss: 0.68952 | TestLoss: 0.69269 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 28 |  TrainLoss: 0.68851 | TestLoss: 0.69235 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 29 |  TrainLoss: 0.68666 | TestLoss: 0.69194 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 30 |  TrainLoss: 0.68698 | TestLoss: 0.69140 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 31 |  TrainLoss: 0.68873 | TestLoss: 0.69075 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 32 |  TrainLoss: 0.68747 | TestLoss: 0.69016 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 33 |  TrainLoss: 0.68601 | TestLoss: 0.68970 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 34 |  TrainLoss: 0.68397 | TestLoss: 0.68927 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 35 |  TrainLoss: 0.68653 | TestLoss: 0.68892 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 36 |  TrainLoss: 0.68379 | TestLoss: 0.68814 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 37 |  TrainLoss: 0.68349 | TestLoss: 0.68703 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 38 |  TrainLoss: 0.68326 | TestLoss: 0.68593 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 39 |  TrainLoss: 0.68358 | TestLoss: 0.68491 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 40 |  TrainLoss: 0.68089 | TestLoss: 0.68372 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 41 |  TrainLoss: 0.68126 | TestLoss: 0.68235 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 42 |  TrainLoss: 0.67972 | TestLoss: 0.68063 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 43 |  TrainLoss: 0.67501 | TestLoss: 0.67917 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 44 |  TrainLoss: 0.67497 | TestLoss: 0.67783 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 45 |  TrainLoss: 0.67339 | TestLoss: 0.67470 | TestAcc: 0.55204 | TestF1: 0.22\n",
      "Epoch: 46 |  TrainLoss: 0.67099 | TestLoss: 0.67240 | TestAcc: 0.56561 | TestF1: 0.26\n",
      "Epoch: 47 |  TrainLoss: 0.66774 | TestLoss: 0.67096 | TestAcc: 0.51584 | TestF1: 0.10\n",
      "Epoch: 48 |  TrainLoss: 0.66501 | TestLoss: 0.66698 | TestAcc: 0.60181 | TestF1: 0.37\n",
      "Epoch: 49 |  TrainLoss: 0.66214 | TestLoss: 0.66234 | TestAcc: 0.80090 | TestF1: 0.78\n",
      "Epoch: 50 |  TrainLoss: 0.66161 | TestLoss: 0.66320 | TestAcc: 0.53846 | TestF1: 0.18\n",
      "Epoch: 51 |  TrainLoss: 0.65557 | TestLoss: 0.65664 | TestAcc: 0.70136 | TestF1: 0.59\n",
      "Epoch: 52 |  TrainLoss: 0.65421 | TestLoss: 0.65153 | TestAcc: 0.80995 | TestF1: 0.81\n",
      "Epoch: 53 |  TrainLoss: 0.64489 | TestLoss: 0.65051 | TestAcc: 0.66063 | TestF1: 0.51\n",
      "Epoch: 54 |  TrainLoss: 0.64186 | TestLoss: 0.64560 | TestAcc: 0.69683 | TestF1: 0.58\n",
      "Epoch: 55 |  TrainLoss: 0.63335 | TestLoss: 0.63627 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 56 |  TrainLoss: 0.63207 | TestLoss: 0.63147 | TestAcc: 0.80543 | TestF1: 0.78\n",
      "Epoch: 57 |  TrainLoss: 0.62507 | TestLoss: 0.63029 | TestAcc: 0.73303 | TestF1: 0.65\n",
      "Epoch: 58 |  TrainLoss: 0.62139 | TestLoss: 0.61767 | TestAcc: 0.81900 | TestF1: 0.82\n",
      "Epoch: 59 |  TrainLoss: 0.60787 | TestLoss: 0.61388 | TestAcc: 0.77828 | TestF1: 0.74\n",
      "Epoch: 60 |  TrainLoss: 0.59803 | TestLoss: 0.60943 | TestAcc: 0.77828 | TestF1: 0.74\n",
      "Epoch: 61 |  TrainLoss: 0.59722 | TestLoss: 0.59429 | TestAcc: 0.81448 | TestF1: 0.83\n",
      "Epoch: 62 |  TrainLoss: 0.58613 | TestLoss: 0.59332 | TestAcc: 0.77828 | TestF1: 0.74\n",
      "Epoch: 63 |  TrainLoss: 0.56633 | TestLoss: 0.57791 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 64 |  TrainLoss: 0.55748 | TestLoss: 0.56521 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 65 |  TrainLoss: 0.56106 | TestLoss: 0.56832 | TestAcc: 0.77376 | TestF1: 0.73\n",
      "Epoch: 66 |  TrainLoss: 0.54720 | TestLoss: 0.54572 | TestAcc: 0.82805 | TestF1: 0.85\n",
      "Epoch: 67 |  TrainLoss: 0.52923 | TestLoss: 0.54280 | TestAcc: 0.79638 | TestF1: 0.77\n",
      "Epoch: 68 |  TrainLoss: 0.50597 | TestLoss: 0.52324 | TestAcc: 0.84163 | TestF1: 0.83\n",
      "Epoch: 69 |  TrainLoss: 0.50777 | TestLoss: 0.51232 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 70 |  TrainLoss: 0.48021 | TestLoss: 0.50006 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 71 |  TrainLoss: 0.46237 | TestLoss: 0.48454 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 72 |  TrainLoss: 0.45038 | TestLoss: 0.48416 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 73 |  TrainLoss: 0.42562 | TestLoss: 0.47327 | TestAcc: 0.81900 | TestF1: 0.84\n",
      "Epoch: 74 |  TrainLoss: 0.44476 | TestLoss: 0.55485 | TestAcc: 0.68778 | TestF1: 0.56\n",
      "Epoch: 75 |  TrainLoss: 0.47921 | TestLoss: 0.44265 | TestAcc: 0.82353 | TestF1: 0.84\n",
      "Epoch: 76 |  TrainLoss: 0.41076 | TestLoss: 0.44053 | TestAcc: 0.81900 | TestF1: 0.84\n",
      "Epoch: 77 |  TrainLoss: 0.41844 | TestLoss: 0.48751 | TestAcc: 0.76471 | TestF1: 0.72\n",
      "Epoch: 78 |  TrainLoss: 0.40128 | TestLoss: 0.42696 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 79 |  TrainLoss: 0.36804 | TestLoss: 0.44657 | TestAcc: 0.78733 | TestF1: 0.82\n",
      "Epoch: 80 |  TrainLoss: 0.40438 | TestLoss: 0.40508 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 81 |  TrainLoss: 0.33189 | TestLoss: 0.48212 | TestAcc: 0.76018 | TestF1: 0.71\n",
      "Epoch: 82 |  TrainLoss: 0.37922 | TestLoss: 0.39260 | TestAcc: 0.85973 | TestF1: 0.87\n",
      "Epoch: 83 |  TrainLoss: 0.32778 | TestLoss: 0.41835 | TestAcc: 0.81900 | TestF1: 0.84\n",
      "Epoch: 84 |  TrainLoss: 0.37114 | TestLoss: 0.40038 | TestAcc: 0.81900 | TestF1: 0.81\n",
      "Epoch: 85 |  TrainLoss: 0.30611 | TestLoss: 0.45533 | TestAcc: 0.77376 | TestF1: 0.73\n",
      "Epoch: 86 |  TrainLoss: 0.35021 | TestLoss: 0.37883 | TestAcc: 0.85520 | TestF1: 0.87\n",
      "Epoch: 87 |  TrainLoss: 0.31452 | TestLoss: 0.38464 | TestAcc: 0.82805 | TestF1: 0.85\n",
      "Epoch: 88 |  TrainLoss: 0.28599 | TestLoss: 0.40291 | TestAcc: 0.80995 | TestF1: 0.79\n",
      "Epoch: 89 |  TrainLoss: 0.30383 | TestLoss: 0.40719 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 90 |  TrainLoss: 0.29027 | TestLoss: 0.37080 | TestAcc: 0.85068 | TestF1: 0.86\n",
      "Epoch: 91 |  TrainLoss: 0.28123 | TestLoss: 0.36959 | TestAcc: 0.85068 | TestF1: 0.86\n",
      "Epoch: 92 |  TrainLoss: 0.27033 | TestLoss: 0.40746 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 93 |  TrainLoss: 0.27738 | TestLoss: 0.37624 | TestAcc: 0.84163 | TestF1: 0.83\n",
      "Epoch: 94 |  TrainLoss: 0.27337 | TestLoss: 0.37061 | TestAcc: 0.83258 | TestF1: 0.85\n",
      "Epoch: 95 |  TrainLoss: 0.28711 | TestLoss: 0.35469 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 96 |  TrainLoss: 0.24075 | TestLoss: 0.40309 | TestAcc: 0.80995 | TestF1: 0.79\n",
      "Epoch: 97 |  TrainLoss: 0.25291 | TestLoss: 0.35266 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 98 |  TrainLoss: 0.21978 | TestLoss: 0.35232 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 99 |  TrainLoss: 0.24562 | TestLoss: 0.36146 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 100 |  TrainLoss: 0.22257 | TestLoss: 0.36028 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 101 |  TrainLoss: 0.22602 | TestLoss: 0.35023 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 102 |  TrainLoss: 0.21352 | TestLoss: 0.35298 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 103 |  TrainLoss: 0.21634 | TestLoss: 0.36731 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 104 |  TrainLoss: 0.20930 | TestLoss: 0.34881 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 105 |  TrainLoss: 0.24216 | TestLoss: 0.34986 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 106 |  TrainLoss: 0.19336 | TestLoss: 0.37794 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 107 |  TrainLoss: 0.19642 | TestLoss: 0.34660 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 108 |  TrainLoss: 0.17010 | TestLoss: 0.34741 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 109 |  TrainLoss: 0.19368 | TestLoss: 0.42987 | TestAcc: 0.80995 | TestF1: 0.79\n",
      "Epoch: 110 |  TrainLoss: 0.22469 | TestLoss: 0.36796 | TestAcc: 0.84615 | TestF1: 0.86\n",
      "Epoch: 111 |  TrainLoss: 0.22036 | TestLoss: 0.34817 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 112 |  TrainLoss: 0.22029 | TestLoss: 0.52175 | TestAcc: 0.77828 | TestF1: 0.74\n",
      "Epoch: 113 |  TrainLoss: 0.27761 | TestLoss: 0.35982 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 114 |  TrainLoss: 0.21075 | TestLoss: 0.37296 | TestAcc: 0.83710 | TestF1: 0.85\n",
      "Epoch: 115 |  TrainLoss: 0.21271 | TestLoss: 0.46108 | TestAcc: 0.79638 | TestF1: 0.77\n",
      "Epoch: 116 |  TrainLoss: 0.22702 | TestLoss: 0.35103 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 117 |  TrainLoss: 0.17140 | TestLoss: 0.37657 | TestAcc: 0.84163 | TestF1: 0.86\n",
      "Epoch: 118 |  TrainLoss: 0.21300 | TestLoss: 0.34531 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 119 |  TrainLoss: 0.13050 | TestLoss: 0.46883 | TestAcc: 0.80090 | TestF1: 0.77\n",
      "Epoch: 120 |  TrainLoss: 0.22535 | TestLoss: 0.34173 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 121 |  TrainLoss: 0.16967 | TestLoss: 0.34956 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 122 |  TrainLoss: 0.19192 | TestLoss: 0.42127 | TestAcc: 0.82353 | TestF1: 0.80\n",
      "Epoch: 123 |  TrainLoss: 0.19227 | TestLoss: 0.36020 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 124 |  TrainLoss: 0.16694 | TestLoss: 0.37639 | TestAcc: 0.84163 | TestF1: 0.86\n",
      "Epoch: 125 |  TrainLoss: 0.20371 | TestLoss: 0.33888 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 126 |  TrainLoss: 0.15418 | TestLoss: 0.51076 | TestAcc: 0.78733 | TestF1: 0.75\n",
      "Epoch: 127 |  TrainLoss: 0.22272 | TestLoss: 0.34823 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 128 |  TrainLoss: 0.12979 | TestLoss: 0.39804 | TestAcc: 0.83710 | TestF1: 0.86\n",
      "Epoch: 129 |  TrainLoss: 0.22089 | TestLoss: 0.33869 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 130 |  TrainLoss: 0.13475 | TestLoss: 0.49244 | TestAcc: 0.78733 | TestF1: 0.75\n",
      "Epoch: 131 |  TrainLoss: 0.22761 | TestLoss: 0.33561 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 132 |  TrainLoss: 0.14214 | TestLoss: 0.39761 | TestAcc: 0.83710 | TestF1: 0.86\n",
      "Epoch: 133 |  TrainLoss: 0.23763 | TestLoss: 0.33769 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 134 |  TrainLoss: 0.13419 | TestLoss: 0.51111 | TestAcc: 0.78733 | TestF1: 0.75\n",
      "Epoch: 135 |  TrainLoss: 0.23781 | TestLoss: 0.33437 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 136 |  TrainLoss: 0.13495 | TestLoss: 0.37452 | TestAcc: 0.85068 | TestF1: 0.87\n",
      "Epoch: 137 |  TrainLoss: 0.17480 | TestLoss: 0.34072 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 138 |  TrainLoss: 0.11001 | TestLoss: 0.47766 | TestAcc: 0.81448 | TestF1: 0.79\n",
      "Epoch: 139 |  TrainLoss: 0.19505 | TestLoss: 0.33465 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 140 |  TrainLoss: 0.10970 | TestLoss: 0.37960 | TestAcc: 0.84163 | TestF1: 0.86\n",
      "Epoch: 141 |  TrainLoss: 0.16801 | TestLoss: 0.33291 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 142 |  TrainLoss: 0.11670 | TestLoss: 0.41861 | TestAcc: 0.83710 | TestF1: 0.82\n",
      "Epoch: 143 |  TrainLoss: 0.18287 | TestLoss: 0.33336 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 144 |  TrainLoss: 0.11864 | TestLoss: 0.33537 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 145 |  TrainLoss: 0.14492 | TestLoss: 0.33984 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 146 |  TrainLoss: 0.13211 | TestLoss: 0.38162 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 147 |  TrainLoss: 0.12116 | TestLoss: 0.34109 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 148 |  TrainLoss: 0.09837 | TestLoss: 0.34028 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 149 |  TrainLoss: 0.13125 | TestLoss: 0.33326 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 150 |  TrainLoss: 0.09845 | TestLoss: 0.37714 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 151 |  TrainLoss: 0.10487 | TestLoss: 0.35514 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 152 |  TrainLoss: 0.09202 | TestLoss: 0.33310 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 153 |  TrainLoss: 0.11170 | TestLoss: 0.33322 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 154 |  TrainLoss: 0.10916 | TestLoss: 0.35047 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 155 |  TrainLoss: 0.09182 | TestLoss: 0.36355 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 156 |  TrainLoss: 0.09531 | TestLoss: 0.33443 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 157 |  TrainLoss: 0.08420 | TestLoss: 0.33757 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 158 |  TrainLoss: 0.10039 | TestLoss: 0.35040 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 159 |  TrainLoss: 0.11145 | TestLoss: 0.36008 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 160 |  TrainLoss: 0.07963 | TestLoss: 0.33629 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 161 |  TrainLoss: 0.07664 | TestLoss: 0.33596 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 162 |  TrainLoss: 0.07559 | TestLoss: 0.34636 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 163 |  TrainLoss: 0.07412 | TestLoss: 0.35875 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 164 |  TrainLoss: 0.06776 | TestLoss: 0.34159 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 165 |  TrainLoss: 0.07906 | TestLoss: 0.34007 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 166 |  TrainLoss: 0.08100 | TestLoss: 0.35850 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 167 |  TrainLoss: 0.09057 | TestLoss: 0.34827 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 168 |  TrainLoss: 0.05738 | TestLoss: 0.34076 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 169 |  TrainLoss: 0.08189 | TestLoss: 0.35246 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 170 |  TrainLoss: 0.07938 | TestLoss: 0.38121 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 171 |  TrainLoss: 0.07649 | TestLoss: 0.34237 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 172 |  TrainLoss: 0.05783 | TestLoss: 0.34806 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 173 |  TrainLoss: 0.08895 | TestLoss: 0.37561 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 174 |  TrainLoss: 0.08269 | TestLoss: 0.38519 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 175 |  TrainLoss: 0.09043 | TestLoss: 0.35478 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 176 |  TrainLoss: 0.09155 | TestLoss: 0.34499 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 177 |  TrainLoss: 0.08002 | TestLoss: 0.38536 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 178 |  TrainLoss: 0.05195 | TestLoss: 0.36107 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 179 |  TrainLoss: 0.05416 | TestLoss: 0.34546 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 180 |  TrainLoss: 0.06258 | TestLoss: 0.34793 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 181 |  TrainLoss: 0.05998 | TestLoss: 0.39920 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 182 |  TrainLoss: 0.07476 | TestLoss: 0.34799 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 183 |  TrainLoss: 0.05383 | TestLoss: 0.36099 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 184 |  TrainLoss: 0.09702 | TestLoss: 0.40095 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 185 |  TrainLoss: 0.05865 | TestLoss: 0.39492 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 186 |  TrainLoss: 0.06755 | TestLoss: 0.35135 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 187 |  TrainLoss: 0.06052 | TestLoss: 0.35137 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 188 |  TrainLoss: 0.05999 | TestLoss: 0.42452 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 189 |  TrainLoss: 0.08735 | TestLoss: 0.35362 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 190 |  TrainLoss: 0.06661 | TestLoss: 0.35443 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 191 |  TrainLoss: 0.06131 | TestLoss: 0.39502 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 192 |  TrainLoss: 0.05391 | TestLoss: 0.36092 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 193 |  TrainLoss: 0.04493 | TestLoss: 0.35669 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 194 |  TrainLoss: 0.03777 | TestLoss: 0.35819 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 195 |  TrainLoss: 0.04274 | TestLoss: 0.39337 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 196 |  TrainLoss: 0.05502 | TestLoss: 0.37741 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 197 |  TrainLoss: 0.04364 | TestLoss: 0.36233 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 198 |  TrainLoss: 0.05643 | TestLoss: 0.37663 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 199 |  TrainLoss: 0.03484 | TestLoss: 0.42256 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 200 |  TrainLoss: 0.05476 | TestLoss: 0.36175 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 201 |  TrainLoss: 0.04384 | TestLoss: 0.36351 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 202 |  TrainLoss: 0.04198 | TestLoss: 0.40774 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 203 |  TrainLoss: 0.03995 | TestLoss: 0.37897 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 204 |  TrainLoss: 0.04265 | TestLoss: 0.36890 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 205 |  TrainLoss: 0.03638 | TestLoss: 0.36798 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 206 |  TrainLoss: 0.04389 | TestLoss: 0.38394 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 207 |  TrainLoss: 0.03014 | TestLoss: 0.38921 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 208 |  TrainLoss: 0.03372 | TestLoss: 0.36840 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 209 |  TrainLoss: 0.03520 | TestLoss: 0.37752 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 210 |  TrainLoss: 0.02391 | TestLoss: 0.40383 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 211 |  TrainLoss: 0.02923 | TestLoss: 0.37697 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 212 |  TrainLoss: 0.02779 | TestLoss: 0.37621 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 213 |  TrainLoss: 0.04036 | TestLoss: 0.43523 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 214 |  TrainLoss: 0.03529 | TestLoss: 0.42795 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 215 |  TrainLoss: 0.03304 | TestLoss: 0.38027 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 216 |  TrainLoss: 0.03312 | TestLoss: 0.37896 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 217 |  TrainLoss: 0.02871 | TestLoss: 0.44665 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 218 |  TrainLoss: 0.04087 | TestLoss: 0.38699 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 219 |  TrainLoss: 0.03120 | TestLoss: 0.39774 | TestAcc: 0.86878 | TestF1: 0.88\n",
      "Epoch: 220 |  TrainLoss: 0.05280 | TestLoss: 0.42589 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 221 |  TrainLoss: 0.02411 | TestLoss: 0.46805 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 222 |  TrainLoss: 0.04367 | TestLoss: 0.38893 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 223 |  TrainLoss: 0.03963 | TestLoss: 0.38467 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 224 |  TrainLoss: 0.03014 | TestLoss: 0.51847 | TestAcc: 0.84163 | TestF1: 0.83\n",
      "Epoch: 225 |  TrainLoss: 0.06845 | TestLoss: 0.39672 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 226 |  TrainLoss: 0.03958 | TestLoss: 0.40183 | TestAcc: 0.86878 | TestF1: 0.88\n",
      "Epoch: 227 |  TrainLoss: 0.04158 | TestLoss: 0.47858 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 228 |  TrainLoss: 0.03526 | TestLoss: 0.45682 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 229 |  TrainLoss: 0.03161 | TestLoss: 0.39826 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 230 |  TrainLoss: 0.04159 | TestLoss: 0.38933 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 231 |  TrainLoss: 0.02124 | TestLoss: 0.43904 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 232 |  TrainLoss: 0.02500 | TestLoss: 0.42368 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 233 |  TrainLoss: 0.02635 | TestLoss: 0.39967 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 234 |  TrainLoss: 0.02407 | TestLoss: 0.40844 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 235 |  TrainLoss: 0.03809 | TestLoss: 0.47965 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 236 |  TrainLoss: 0.03737 | TestLoss: 0.43772 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 237 |  TrainLoss: 0.02059 | TestLoss: 0.40460 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 238 |  TrainLoss: 0.03404 | TestLoss: 0.40374 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 239 |  TrainLoss: 0.01583 | TestLoss: 0.45345 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 240 |  TrainLoss: 0.01587 | TestLoss: 0.43544 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 241 |  TrainLoss: 0.01647 | TestLoss: 0.40077 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 242 |  TrainLoss: 0.01736 | TestLoss: 0.40282 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 243 |  TrainLoss: 0.01768 | TestLoss: 0.40769 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 244 |  TrainLoss: 0.00975 | TestLoss: 0.42438 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 245 |  TrainLoss: 0.01085 | TestLoss: 0.43002 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 246 |  TrainLoss: 0.02335 | TestLoss: 0.41006 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 247 |  TrainLoss: 0.01864 | TestLoss: 0.41301 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 248 |  TrainLoss: 0.01155 | TestLoss: 0.41370 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 249 |  TrainLoss: 0.01149 | TestLoss: 0.41425 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 250 |  TrainLoss: 0.01469 | TestLoss: 0.41778 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 251 |  TrainLoss: 0.01448 | TestLoss: 0.41549 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 252 |  TrainLoss: 0.01581 | TestLoss: 0.42438 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 253 |  TrainLoss: 0.01179 | TestLoss: 0.42504 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 254 |  TrainLoss: 0.00885 | TestLoss: 0.41994 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 255 |  TrainLoss: 0.01064 | TestLoss: 0.42692 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 256 |  TrainLoss: 0.00969 | TestLoss: 0.43647 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 257 |  TrainLoss: 0.01339 | TestLoss: 0.44890 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 258 |  TrainLoss: 0.01181 | TestLoss: 0.42799 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 259 |  TrainLoss: 0.01233 | TestLoss: 0.42281 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 260 |  TrainLoss: 0.01426 | TestLoss: 0.43845 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 261 |  TrainLoss: 0.01337 | TestLoss: 0.44791 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 262 |  TrainLoss: 0.01327 | TestLoss: 0.43179 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 263 |  TrainLoss: 0.00810 | TestLoss: 0.42616 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 264 |  TrainLoss: 0.01325 | TestLoss: 0.43390 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 265 |  TrainLoss: 0.01307 | TestLoss: 0.45901 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 266 |  TrainLoss: 0.01121 | TestLoss: 0.44672 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 267 |  TrainLoss: 0.01442 | TestLoss: 0.43102 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 268 |  TrainLoss: 0.00805 | TestLoss: 0.43098 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 269 |  TrainLoss: 0.01208 | TestLoss: 0.44946 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 270 |  TrainLoss: 0.00548 | TestLoss: 0.48043 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 271 |  TrainLoss: 0.01178 | TestLoss: 0.44698 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 272 |  TrainLoss: 0.00741 | TestLoss: 0.43664 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 273 |  TrainLoss: 0.00538 | TestLoss: 0.43622 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 274 |  TrainLoss: 0.00796 | TestLoss: 0.44172 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 275 |  TrainLoss: 0.00646 | TestLoss: 0.45930 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 276 |  TrainLoss: 0.00715 | TestLoss: 0.46642 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 277 |  TrainLoss: 0.00619 | TestLoss: 0.45747 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 278 |  TrainLoss: 0.00695 | TestLoss: 0.44418 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 279 |  TrainLoss: 0.00870 | TestLoss: 0.45128 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 280 |  TrainLoss: 0.00843 | TestLoss: 0.46498 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 281 |  TrainLoss: 0.00713 | TestLoss: 0.47133 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 282 |  TrainLoss: 0.00880 | TestLoss: 0.45213 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 283 |  TrainLoss: 0.00728 | TestLoss: 0.44697 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 284 |  TrainLoss: 0.00712 | TestLoss: 0.45125 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 285 |  TrainLoss: 0.00851 | TestLoss: 0.47733 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 286 |  TrainLoss: 0.00761 | TestLoss: 0.47704 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 287 |  TrainLoss: 0.00963 | TestLoss: 0.45665 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 288 |  TrainLoss: 0.00616 | TestLoss: 0.45131 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 289 |  TrainLoss: 0.00731 | TestLoss: 0.45266 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 290 |  TrainLoss: 0.01166 | TestLoss: 0.46418 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 291 |  TrainLoss: 0.00556 | TestLoss: 0.49696 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 292 |  TrainLoss: 0.00819 | TestLoss: 0.48533 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 293 |  TrainLoss: 0.00639 | TestLoss: 0.45940 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 294 |  TrainLoss: 0.00626 | TestLoss: 0.45755 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 295 |  TrainLoss: 0.01664 | TestLoss: 0.50151 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 296 |  TrainLoss: 0.01120 | TestLoss: 0.52129 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 297 |  TrainLoss: 0.01367 | TestLoss: 0.45985 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 298 |  TrainLoss: 0.00714 | TestLoss: 0.47984 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 299 |  TrainLoss: 0.01376 | TestLoss: 0.46832 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 300 |  TrainLoss: 0.00453 | TestLoss: 0.55030 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 301 |  TrainLoss: 0.01534 | TestLoss: 0.48053 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 302 |  TrainLoss: 0.00665 | TestLoss: 0.46970 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 303 |  TrainLoss: 0.01208 | TestLoss: 0.46385 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 304 |  TrainLoss: 0.00637 | TestLoss: 0.49119 | TestAcc: 0.87783 | TestF1: 0.87\n",
      "Epoch: 305 |  TrainLoss: 0.00655 | TestLoss: 0.52032 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 306 |  TrainLoss: 0.01446 | TestLoss: 0.46605 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 307 |  TrainLoss: 0.00543 | TestLoss: 0.47440 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 308 |  TrainLoss: 0.01565 | TestLoss: 0.50036 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 309 |  TrainLoss: 0.00833 | TestLoss: 0.54305 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 310 |  TrainLoss: 0.01493 | TestLoss: 0.47015 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 311 |  TrainLoss: 0.00500 | TestLoss: 0.48262 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 312 |  TrainLoss: 0.01159 | TestLoss: 0.47180 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 313 |  TrainLoss: 0.00531 | TestLoss: 0.53194 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 314 |  TrainLoss: 0.00841 | TestLoss: 0.54917 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 315 |  TrainLoss: 0.00653 | TestLoss: 0.49868 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 316 |  TrainLoss: 0.00520 | TestLoss: 0.47617 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 317 |  TrainLoss: 0.00762 | TestLoss: 0.47852 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 318 |  TrainLoss: 0.00645 | TestLoss: 0.48774 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 319 |  TrainLoss: 0.00715 | TestLoss: 0.55146 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 320 |  TrainLoss: 0.01133 | TestLoss: 0.51054 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 321 |  TrainLoss: 0.00461 | TestLoss: 0.48130 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 322 |  TrainLoss: 0.00374 | TestLoss: 0.48280 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 323 |  TrainLoss: 0.00665 | TestLoss: 0.48392 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 324 |  TrainLoss: 0.00400 | TestLoss: 0.51057 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 325 |  TrainLoss: 0.00303 | TestLoss: 0.54770 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 326 |  TrainLoss: 0.00451 | TestLoss: 0.54753 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 327 |  TrainLoss: 0.00611 | TestLoss: 0.49494 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 328 |  TrainLoss: 0.00539 | TestLoss: 0.48711 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 329 |  TrainLoss: 0.00454 | TestLoss: 0.48741 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 330 |  TrainLoss: 0.00745 | TestLoss: 0.51547 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 331 |  TrainLoss: 0.00611 | TestLoss: 0.58452 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 332 |  TrainLoss: 0.00860 | TestLoss: 0.53491 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 333 |  TrainLoss: 0.00443 | TestLoss: 0.49363 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 334 |  TrainLoss: 0.00565 | TestLoss: 0.49201 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 335 |  TrainLoss: 0.00335 | TestLoss: 0.49510 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 336 |  TrainLoss: 0.00421 | TestLoss: 0.51382 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 337 |  TrainLoss: 0.00291 | TestLoss: 0.54352 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 338 |  TrainLoss: 0.00335 | TestLoss: 0.55144 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 339 |  TrainLoss: 0.00417 | TestLoss: 0.52233 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 340 |  TrainLoss: 0.00415 | TestLoss: 0.49789 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 341 |  TrainLoss: 0.00249 | TestLoss: 0.49995 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 342 |  TrainLoss: 0.00505 | TestLoss: 0.49852 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 343 |  TrainLoss: 0.00371 | TestLoss: 0.51907 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 344 |  TrainLoss: 0.00252 | TestLoss: 0.56368 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 345 |  TrainLoss: 0.00377 | TestLoss: 0.57230 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 346 |  TrainLoss: 0.00699 | TestLoss: 0.51113 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 347 |  TrainLoss: 0.00265 | TestLoss: 0.50268 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 348 |  TrainLoss: 0.00525 | TestLoss: 0.50309 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 349 |  TrainLoss: 0.00539 | TestLoss: 0.53223 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 350 |  TrainLoss: 0.00232 | TestLoss: 0.58432 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 351 |  TrainLoss: 0.00512 | TestLoss: 0.57026 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 352 |  TrainLoss: 0.00580 | TestLoss: 0.51208 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 353 |  TrainLoss: 0.00246 | TestLoss: 0.50804 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 354 |  TrainLoss: 0.00325 | TestLoss: 0.51330 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 355 |  TrainLoss: 0.00913 | TestLoss: 0.53051 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 356 |  TrainLoss: 0.00418 | TestLoss: 0.60542 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 357 |  TrainLoss: 0.00922 | TestLoss: 0.54132 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 358 |  TrainLoss: 0.00357 | TestLoss: 0.50857 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 359 |  TrainLoss: 0.00289 | TestLoss: 0.51432 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 360 |  TrainLoss: 0.00456 | TestLoss: 0.50928 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 361 |  TrainLoss: 0.00412 | TestLoss: 0.52686 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 362 |  TrainLoss: 0.00206 | TestLoss: 0.57414 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 363 |  TrainLoss: 0.00284 | TestLoss: 0.59249 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 364 |  TrainLoss: 0.00430 | TestLoss: 0.54612 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 365 |  TrainLoss: 0.00233 | TestLoss: 0.51873 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 366 |  TrainLoss: 0.00317 | TestLoss: 0.51327 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 367 |  TrainLoss: 0.00312 | TestLoss: 0.51493 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 368 |  TrainLoss: 0.00307 | TestLoss: 0.52870 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 369 |  TrainLoss: 0.00251 | TestLoss: 0.56678 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 370 |  TrainLoss: 0.00416 | TestLoss: 0.55874 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 371 |  TrainLoss: 0.00236 | TestLoss: 0.53871 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 372 |  TrainLoss: 0.00179 | TestLoss: 0.52608 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 373 |  TrainLoss: 0.00167 | TestLoss: 0.52128 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 374 |  TrainLoss: 0.00207 | TestLoss: 0.52048 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 375 |  TrainLoss: 0.00220 | TestLoss: 0.52303 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 376 |  TrainLoss: 0.00234 | TestLoss: 0.53540 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 377 |  TrainLoss: 0.00340 | TestLoss: 0.58738 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 378 |  TrainLoss: 0.00229 | TestLoss: 0.62335 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 379 |  TrainLoss: 0.00556 | TestLoss: 0.56298 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 380 |  TrainLoss: 0.00157 | TestLoss: 0.53284 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 381 |  TrainLoss: 0.00225 | TestLoss: 0.52609 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 382 |  TrainLoss: 0.00318 | TestLoss: 0.52684 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 383 |  TrainLoss: 0.00239 | TestLoss: 0.53050 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 384 |  TrainLoss: 0.00201 | TestLoss: 0.55255 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 385 |  TrainLoss: 0.00265 | TestLoss: 0.58030 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 386 |  TrainLoss: 0.00235 | TestLoss: 0.59210 | TestAcc: 0.86878 | TestF1: 0.86\n",
      "Epoch: 387 |  TrainLoss: 0.00424 | TestLoss: 0.54678 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 388 |  TrainLoss: 0.00216 | TestLoss: 0.53336 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 389 |  TrainLoss: 0.00167 | TestLoss: 0.53234 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 390 |  TrainLoss: 0.00317 | TestLoss: 0.53660 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 391 |  TrainLoss: 0.00154 | TestLoss: 0.55369 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 392 |  TrainLoss: 0.00155 | TestLoss: 0.58154 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 393 |  TrainLoss: 0.00294 | TestLoss: 0.59649 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 394 |  TrainLoss: 0.00215 | TestLoss: 0.59356 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 395 |  TrainLoss: 0.00251 | TestLoss: 0.56272 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 396 |  TrainLoss: 0.00254 | TestLoss: 0.53990 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 397 |  TrainLoss: 0.00330 | TestLoss: 0.53958 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 398 |  TrainLoss: 0.00333 | TestLoss: 0.54890 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 399 |  TrainLoss: 0.00131 | TestLoss: 0.57117 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 400 |  TrainLoss: 0.00129 | TestLoss: 0.59324 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 401 |  TrainLoss: 0.00160 | TestLoss: 0.60004 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 402 |  TrainLoss: 0.00103 | TestLoss: 0.59824 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 403 |  TrainLoss: 0.00179 | TestLoss: 0.57640 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 404 |  TrainLoss: 0.00115 | TestLoss: 0.55759 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 405 |  TrainLoss: 0.00115 | TestLoss: 0.54906 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 406 |  TrainLoss: 0.00166 | TestLoss: 0.54922 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 407 |  TrainLoss: 0.00151 | TestLoss: 0.55820 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 408 |  TrainLoss: 0.00182 | TestLoss: 0.57657 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 409 |  TrainLoss: 0.00152 | TestLoss: 0.60468 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 410 |  TrainLoss: 0.00209 | TestLoss: 0.60708 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 411 |  TrainLoss: 0.00175 | TestLoss: 0.58852 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 412 |  TrainLoss: 0.00197 | TestLoss: 0.56539 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 413 |  TrainLoss: 0.00198 | TestLoss: 0.55720 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 414 |  TrainLoss: 0.00161 | TestLoss: 0.55540 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 415 |  TrainLoss: 0.00131 | TestLoss: 0.55862 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 416 |  TrainLoss: 0.00107 | TestLoss: 0.56375 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 417 |  TrainLoss: 0.00115 | TestLoss: 0.57533 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 418 |  TrainLoss: 0.00222 | TestLoss: 0.57794 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 419 |  TrainLoss: 0.00131 | TestLoss: 0.57914 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 420 |  TrainLoss: 0.00111 | TestLoss: 0.57567 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 421 |  TrainLoss: 0.00202 | TestLoss: 0.56466 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 422 |  TrainLoss: 0.00134 | TestLoss: 0.56056 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 423 |  TrainLoss: 0.00108 | TestLoss: 0.56010 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 424 |  TrainLoss: 0.00134 | TestLoss: 0.56513 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 425 |  TrainLoss: 0.00125 | TestLoss: 0.57512 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 426 |  TrainLoss: 0.00129 | TestLoss: 0.58293 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 427 |  TrainLoss: 0.00083 | TestLoss: 0.58820 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 428 |  TrainLoss: 0.00200 | TestLoss: 0.57501 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 429 |  TrainLoss: 0.00185 | TestLoss: 0.56407 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 430 |  TrainLoss: 0.00224 | TestLoss: 0.56795 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 431 |  TrainLoss: 0.00124 | TestLoss: 0.57795 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 432 |  TrainLoss: 0.00246 | TestLoss: 0.58800 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 433 |  TrainLoss: 0.00156 | TestLoss: 0.60513 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 434 |  TrainLoss: 0.00129 | TestLoss: 0.60804 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 435 |  TrainLoss: 0.00472 | TestLoss: 0.56335 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 436 |  TrainLoss: 0.00108 | TestLoss: 0.57668 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 437 |  TrainLoss: 0.00821 | TestLoss: 0.57395 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 438 |  TrainLoss: 0.00078 | TestLoss: 0.65189 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 439 |  TrainLoss: 0.00277 | TestLoss: 0.69468 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 440 |  TrainLoss: 0.00706 | TestLoss: 0.58484 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 441 |  TrainLoss: 0.00156 | TestLoss: 0.56728 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 442 |  TrainLoss: 0.00811 | TestLoss: 0.57438 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 443 |  TrainLoss: 0.00108 | TestLoss: 0.62569 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 444 |  TrainLoss: 0.00189 | TestLoss: 0.66222 | TestAcc: 0.85973 | TestF1: 0.85\n",
      "Epoch: 445 |  TrainLoss: 0.00324 | TestLoss: 0.61917 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 446 |  TrainLoss: 0.00137 | TestLoss: 0.58107 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 447 |  TrainLoss: 0.00108 | TestLoss: 0.56458 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 448 |  TrainLoss: 0.00130 | TestLoss: 0.56478 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 449 |  TrainLoss: 0.00259 | TestLoss: 0.56520 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 450 |  TrainLoss: 0.00124 | TestLoss: 0.57903 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 451 |  TrainLoss: 0.00120 | TestLoss: 0.61176 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 452 |  TrainLoss: 0.00181 | TestLoss: 0.62585 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 453 |  TrainLoss: 0.00129 | TestLoss: 0.62247 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 454 |  TrainLoss: 0.00119 | TestLoss: 0.60196 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 455 |  TrainLoss: 0.00075 | TestLoss: 0.58554 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 456 |  TrainLoss: 0.00106 | TestLoss: 0.57799 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 457 |  TrainLoss: 0.00150 | TestLoss: 0.58012 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 458 |  TrainLoss: 0.00106 | TestLoss: 0.58513 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 459 |  TrainLoss: 0.00070 | TestLoss: 0.58937 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 460 |  TrainLoss: 0.00072 | TestLoss: 0.59009 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 461 |  TrainLoss: 0.00119 | TestLoss: 0.58801 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 462 |  TrainLoss: 0.00072 | TestLoss: 0.58565 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 463 |  TrainLoss: 0.00111 | TestLoss: 0.58932 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 464 |  TrainLoss: 0.00048 | TestLoss: 0.59252 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 465 |  TrainLoss: 0.00091 | TestLoss: 0.59477 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 466 |  TrainLoss: 0.00173 | TestLoss: 0.58253 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 467 |  TrainLoss: 0.00081 | TestLoss: 0.57762 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 468 |  TrainLoss: 0.00130 | TestLoss: 0.58061 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 469 |  TrainLoss: 0.00060 | TestLoss: 0.58611 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 470 |  TrainLoss: 0.00101 | TestLoss: 0.59475 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 471 |  TrainLoss: 0.00097 | TestLoss: 0.60689 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 472 |  TrainLoss: 0.00107 | TestLoss: 0.61758 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 473 |  TrainLoss: 0.00130 | TestLoss: 0.61323 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 474 |  TrainLoss: 0.00092 | TestLoss: 0.59996 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 475 |  TrainLoss: 0.00077 | TestLoss: 0.58951 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 476 |  TrainLoss: 0.00061 | TestLoss: 0.58330 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 477 |  TrainLoss: 0.00151 | TestLoss: 0.58589 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 478 |  TrainLoss: 0.00060 | TestLoss: 0.58925 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 479 |  TrainLoss: 0.00084 | TestLoss: 0.59950 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 480 |  TrainLoss: 0.00073 | TestLoss: 0.61084 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 481 |  TrainLoss: 0.00131 | TestLoss: 0.60892 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 482 |  TrainLoss: 0.00057 | TestLoss: 0.60688 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 483 |  TrainLoss: 0.00101 | TestLoss: 0.59974 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 484 |  TrainLoss: 0.00090 | TestLoss: 0.59369 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 485 |  TrainLoss: 0.00067 | TestLoss: 0.59289 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 486 |  TrainLoss: 0.00085 | TestLoss: 0.59460 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 487 |  TrainLoss: 0.00077 | TestLoss: 0.59919 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 488 |  TrainLoss: 0.00068 | TestLoss: 0.60248 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 489 |  TrainLoss: 0.00071 | TestLoss: 0.60621 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 490 |  TrainLoss: 0.00064 | TestLoss: 0.61010 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 491 |  TrainLoss: 0.00089 | TestLoss: 0.61210 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 492 |  TrainLoss: 0.00061 | TestLoss: 0.61502 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 493 |  TrainLoss: 0.00052 | TestLoss: 0.61614 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 494 |  TrainLoss: 0.00063 | TestLoss: 0.61580 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 495 |  TrainLoss: 0.00071 | TestLoss: 0.61474 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 496 |  TrainLoss: 0.00070 | TestLoss: 0.61261 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 497 |  TrainLoss: 0.00081 | TestLoss: 0.61128 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 498 |  TrainLoss: 0.00087 | TestLoss: 0.61117 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 499 |  TrainLoss: 0.00053 | TestLoss: 0.61059 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Best WLoss: 0.00156 | Best Epoch: 161\n"
     ]
    }
   ],
   "source": [
    "wloss = []\n",
    "weighted_loss = 0\n",
    "exp_param = 0.8\n",
    "best_test_loss = float('inf')  # Initialize with a large value\n",
    "\n",
    "# Without dropout training results\n",
    "for epoch in range(500):\n",
    "  train_loss = train(epoch)\n",
    "  test_loss, test_acc, test_f1 = test(epoch)\n",
    "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
    "  \n",
    "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
    "  \n",
    "  if test_loss < best_test_loss:\n",
    "    best_test_loss = test_loss  # Update the best test loss\n",
    "\n",
    "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
    "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
    "\n",
    "# Print the best values\n",
    "best_wloss = min(wloss)\n",
    "best_epoch = wloss.index(best_wloss)\n",
    "print(f'Best WLoss: {best_wloss:.5f} | Best Epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f3d394",
   "metadata": {},
   "source": [
    "# Model 3: \n",
    "GraphSage + Content with 2 Layer MLP as defined by the paper. (This is the model implemented in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d7e4b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, embedding_size, batch_size, l2_reg_weight):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.batch_size = batch_size\n",
    "        self.l2_reg_weight = l2_reg_weight\n",
    "        \n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
    "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
    "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
    "        \n",
    "        self.full1 = Linear(hidden_channels[2], hidden_channels[3])\n",
    "        self.full2 = Linear(hidden_channels[3], hidden_channels[4])\n",
    "        self.softmax = Linear(hidden_channels[4], out_channels)\n",
    "\n",
    "        # Dropouts\n",
    "        self.dp1 = Dropout(0.2)\n",
    "        self.dp2 = Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = self.conv1(x, edge_index).relu()\n",
    "        h = self.conv2(h, edge_index).relu()\n",
    "        h = self.conv3(h, edge_index).relu()\n",
    "\n",
    "        h = global_max_pool(h, batch)\n",
    "\n",
    "        h = self.full1(h).relu()\n",
    "        h = self.dp1(h)\n",
    "        h = self.full2(h).relu()\n",
    "        h = self.dp2(h)\n",
    "        \n",
    "        h = self.softmax(h)\n",
    "\n",
    "        return torch.sigmoid(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ecea1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(test_data_pol.num_features,[512,512,512,256,256,256],1, 128, 128, 0.001).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "lossff = torch.nn.BCELoss()\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f345ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        all_preds.append(torch.reshape(out, (-1,)))\n",
    "        all_labels.append(data.y.float())\n",
    "    # print(all_preds)\n",
    "    accuracy, f1 = metrics(all_preds, all_labels)\n",
    "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
    "\n",
    "\n",
    "def metrics(preds, gts):\n",
    "    preds = torch.round(torch.cat(preds))\n",
    "    gts = torch.cat(gts)\n",
    "    # print(preds.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f355e974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 |  TrainLoss: 0.69671 | TestLoss: 0.69282 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 01 |  TrainLoss: 0.69413 | TestLoss: 0.69280 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 02 |  TrainLoss: 0.69507 | TestLoss: 0.69285 | TestAcc: 0.57014 | TestF1: 0.69\n",
      "Epoch: 03 |  TrainLoss: 0.69253 | TestLoss: 0.69307 | TestAcc: 0.49774 | TestF1: 0.03\n",
      "Epoch: 04 |  TrainLoss: 0.69297 | TestLoss: 0.69333 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 05 |  TrainLoss: 0.68861 | TestLoss: 0.69350 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 06 |  TrainLoss: 0.68912 | TestLoss: 0.69365 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 07 |  TrainLoss: 0.68609 | TestLoss: 0.69376 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 08 |  TrainLoss: 0.68814 | TestLoss: 0.69375 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 09 |  TrainLoss: 0.69032 | TestLoss: 0.69369 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 10 |  TrainLoss: 0.69106 | TestLoss: 0.69328 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 11 |  TrainLoss: 0.68663 | TestLoss: 0.69253 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 12 |  TrainLoss: 0.68456 | TestLoss: 0.69158 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 13 |  TrainLoss: 0.68396 | TestLoss: 0.69030 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 14 |  TrainLoss: 0.67797 | TestLoss: 0.68900 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 15 |  TrainLoss: 0.68028 | TestLoss: 0.68746 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 16 |  TrainLoss: 0.67849 | TestLoss: 0.68578 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 17 |  TrainLoss: 0.67797 | TestLoss: 0.68406 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 18 |  TrainLoss: 0.67057 | TestLoss: 0.68237 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 19 |  TrainLoss: 0.67342 | TestLoss: 0.68046 | TestAcc: 0.49321 | TestF1: 0.02\n",
      "Epoch: 20 |  TrainLoss: 0.66777 | TestLoss: 0.67817 | TestAcc: 0.50226 | TestF1: 0.05\n",
      "Epoch: 21 |  TrainLoss: 0.66631 | TestLoss: 0.67522 | TestAcc: 0.52489 | TestF1: 0.13\n",
      "Epoch: 22 |  TrainLoss: 0.66184 | TestLoss: 0.67184 | TestAcc: 0.56561 | TestF1: 0.26\n",
      "Epoch: 23 |  TrainLoss: 0.65483 | TestLoss: 0.66789 | TestAcc: 0.59276 | TestF1: 0.35\n",
      "Epoch: 24 |  TrainLoss: 0.64850 | TestLoss: 0.66344 | TestAcc: 0.64253 | TestF1: 0.48\n",
      "Epoch: 25 |  TrainLoss: 0.64415 | TestLoss: 0.65832 | TestAcc: 0.66516 | TestF1: 0.53\n",
      "Epoch: 26 |  TrainLoss: 0.63985 | TestLoss: 0.65247 | TestAcc: 0.69683 | TestF1: 0.60\n",
      "Epoch: 27 |  TrainLoss: 0.62846 | TestLoss: 0.64631 | TestAcc: 0.70588 | TestF1: 0.63\n",
      "Epoch: 28 |  TrainLoss: 0.62406 | TestLoss: 0.63959 | TestAcc: 0.70588 | TestF1: 0.63\n",
      "Epoch: 29 |  TrainLoss: 0.61153 | TestLoss: 0.63233 | TestAcc: 0.68326 | TestF1: 0.59\n",
      "Epoch: 30 |  TrainLoss: 0.60725 | TestLoss: 0.62335 | TestAcc: 0.72398 | TestF1: 0.66\n",
      "Epoch: 31 |  TrainLoss: 0.58644 | TestLoss: 0.61411 | TestAcc: 0.75113 | TestF1: 0.71\n",
      "Epoch: 32 |  TrainLoss: 0.57927 | TestLoss: 0.60384 | TestAcc: 0.77376 | TestF1: 0.74\n",
      "Epoch: 33 |  TrainLoss: 0.56446 | TestLoss: 0.59254 | TestAcc: 0.78733 | TestF1: 0.76\n",
      "Epoch: 34 |  TrainLoss: 0.54316 | TestLoss: 0.58154 | TestAcc: 0.78281 | TestF1: 0.76\n",
      "Epoch: 35 |  TrainLoss: 0.52177 | TestLoss: 0.57014 | TestAcc: 0.78281 | TestF1: 0.76\n",
      "Epoch: 36 |  TrainLoss: 0.50841 | TestLoss: 0.55763 | TestAcc: 0.78281 | TestF1: 0.76\n",
      "Epoch: 37 |  TrainLoss: 0.48587 | TestLoss: 0.54138 | TestAcc: 0.80995 | TestF1: 0.79\n",
      "Epoch: 38 |  TrainLoss: 0.46575 | TestLoss: 0.52808 | TestAcc: 0.80543 | TestF1: 0.79\n",
      "Epoch: 39 |  TrainLoss: 0.43821 | TestLoss: 0.51409 | TestAcc: 0.80543 | TestF1: 0.79\n",
      "Epoch: 40 |  TrainLoss: 0.42535 | TestLoss: 0.49734 | TestAcc: 0.80995 | TestF1: 0.80\n",
      "Epoch: 41 |  TrainLoss: 0.39122 | TestLoss: 0.48107 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 42 |  TrainLoss: 0.36029 | TestLoss: 0.46933 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 43 |  TrainLoss: 0.33403 | TestLoss: 0.45745 | TestAcc: 0.80995 | TestF1: 0.79\n",
      "Epoch: 44 |  TrainLoss: 0.30294 | TestLoss: 0.43803 | TestAcc: 0.81900 | TestF1: 0.81\n",
      "Epoch: 45 |  TrainLoss: 0.27660 | TestLoss: 0.42565 | TestAcc: 0.83258 | TestF1: 0.83\n",
      "Epoch: 46 |  TrainLoss: 0.25897 | TestLoss: 0.42407 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 47 |  TrainLoss: 0.22448 | TestLoss: 0.41869 | TestAcc: 0.81900 | TestF1: 0.81\n",
      "Epoch: 48 |  TrainLoss: 0.20469 | TestLoss: 0.40625 | TestAcc: 0.83258 | TestF1: 0.83\n",
      "Epoch: 49 |  TrainLoss: 0.17191 | TestLoss: 0.40592 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 50 |  TrainLoss: 0.16198 | TestLoss: 0.40994 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 51 |  TrainLoss: 0.13308 | TestLoss: 0.42576 | TestAcc: 0.84163 | TestF1: 0.83\n",
      "Epoch: 52 |  TrainLoss: 0.12190 | TestLoss: 0.41048 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 53 |  TrainLoss: 0.10395 | TestLoss: 0.42924 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 54 |  TrainLoss: 0.08369 | TestLoss: 0.47046 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 55 |  TrainLoss: 0.07792 | TestLoss: 0.45252 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 56 |  TrainLoss: 0.06143 | TestLoss: 0.44763 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 57 |  TrainLoss: 0.06043 | TestLoss: 0.48279 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 58 |  TrainLoss: 0.04427 | TestLoss: 0.55581 | TestAcc: 0.80995 | TestF1: 0.79\n",
      "Epoch: 59 |  TrainLoss: 0.03968 | TestLoss: 0.53422 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 60 |  TrainLoss: 0.04036 | TestLoss: 0.49966 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 61 |  TrainLoss: 0.02637 | TestLoss: 0.51495 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 62 |  TrainLoss: 0.04106 | TestLoss: 0.54255 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 63 |  TrainLoss: 0.01435 | TestLoss: 0.63614 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 64 |  TrainLoss: 0.02922 | TestLoss: 0.63783 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 65 |  TrainLoss: 0.02429 | TestLoss: 0.58229 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 66 |  TrainLoss: 0.01043 | TestLoss: 0.57214 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 67 |  TrainLoss: 0.01815 | TestLoss: 0.58474 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 68 |  TrainLoss: 0.01776 | TestLoss: 0.62548 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 69 |  TrainLoss: 0.00681 | TestLoss: 0.69830 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 70 |  TrainLoss: 0.00890 | TestLoss: 0.75174 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 71 |  TrainLoss: 0.01606 | TestLoss: 0.72164 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 72 |  TrainLoss: 0.00683 | TestLoss: 0.68018 | TestAcc: 0.84163 | TestF1: 0.83\n",
      "Epoch: 73 |  TrainLoss: 0.00351 | TestLoss: 0.65597 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 74 |  TrainLoss: 0.00385 | TestLoss: 0.65189 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 75 |  TrainLoss: 0.00713 | TestLoss: 0.65928 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 76 |  TrainLoss: 0.00507 | TestLoss: 0.67045 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 77 |  TrainLoss: 0.00412 | TestLoss: 0.69026 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 78 |  TrainLoss: 0.00273 | TestLoss: 0.72170 | TestAcc: 0.83258 | TestF1: 0.83\n",
      "Epoch: 79 |  TrainLoss: 0.00202 | TestLoss: 0.75742 | TestAcc: 0.84163 | TestF1: 0.83\n",
      "Epoch: 80 |  TrainLoss: 0.00177 | TestLoss: 0.79067 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 81 |  TrainLoss: 0.00240 | TestLoss: 0.81433 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 82 |  TrainLoss: 0.00261 | TestLoss: 0.82110 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 83 |  TrainLoss: 0.00268 | TestLoss: 0.81191 | TestAcc: 0.84163 | TestF1: 0.83\n",
      "Epoch: 84 |  TrainLoss: 0.00197 | TestLoss: 0.79766 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 85 |  TrainLoss: 0.00176 | TestLoss: 0.78153 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 86 |  TrainLoss: 0.00113 | TestLoss: 0.76822 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 87 |  TrainLoss: 0.00159 | TestLoss: 0.75931 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 88 |  TrainLoss: 0.00132 | TestLoss: 0.75518 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 89 |  TrainLoss: 0.00155 | TestLoss: 0.75519 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 90 |  TrainLoss: 0.00111 | TestLoss: 0.75758 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 91 |  TrainLoss: 0.00126 | TestLoss: 0.76228 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 92 |  TrainLoss: 0.00132 | TestLoss: 0.76942 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 93 |  TrainLoss: 0.00099 | TestLoss: 0.77754 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 94 |  TrainLoss: 0.00093 | TestLoss: 0.78741 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 95 |  TrainLoss: 0.00062 | TestLoss: 0.79802 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 96 |  TrainLoss: 0.00068 | TestLoss: 0.80930 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 97 |  TrainLoss: 0.00077 | TestLoss: 0.82028 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 98 |  TrainLoss: 0.00067 | TestLoss: 0.82985 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 99 |  TrainLoss: 0.00063 | TestLoss: 0.83785 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 100 |  TrainLoss: 0.00097 | TestLoss: 0.84488 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 101 |  TrainLoss: 0.00079 | TestLoss: 0.84943 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 102 |  TrainLoss: 0.00075 | TestLoss: 0.85062 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 103 |  TrainLoss: 0.00064 | TestLoss: 0.85162 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 104 |  TrainLoss: 0.00110 | TestLoss: 0.84924 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 105 |  TrainLoss: 0.00056 | TestLoss: 0.84733 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 106 |  TrainLoss: 0.00068 | TestLoss: 0.84420 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 107 |  TrainLoss: 0.00053 | TestLoss: 0.84103 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 108 |  TrainLoss: 0.00077 | TestLoss: 0.83743 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 109 |  TrainLoss: 0.00073 | TestLoss: 0.83409 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 110 |  TrainLoss: 0.00055 | TestLoss: 0.83186 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 111 |  TrainLoss: 0.00054 | TestLoss: 0.83011 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 112 |  TrainLoss: 0.00074 | TestLoss: 0.82967 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 113 |  TrainLoss: 0.00063 | TestLoss: 0.83016 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 114 |  TrainLoss: 0.00075 | TestLoss: 0.83354 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 115 |  TrainLoss: 0.00052 | TestLoss: 0.83680 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 116 |  TrainLoss: 0.00050 | TestLoss: 0.84087 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 117 |  TrainLoss: 0.00048 | TestLoss: 0.84532 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 118 |  TrainLoss: 0.00056 | TestLoss: 0.85003 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 119 |  TrainLoss: 0.00062 | TestLoss: 0.85285 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 120 |  TrainLoss: 0.00056 | TestLoss: 0.85505 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 121 |  TrainLoss: 0.00048 | TestLoss: 0.85769 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 122 |  TrainLoss: 0.00039 | TestLoss: 0.86056 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 123 |  TrainLoss: 0.00059 | TestLoss: 0.86290 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 124 |  TrainLoss: 0.00040 | TestLoss: 0.86527 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 125 |  TrainLoss: 0.00045 | TestLoss: 0.86715 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 126 |  TrainLoss: 0.00041 | TestLoss: 0.86890 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 127 |  TrainLoss: 0.00062 | TestLoss: 0.86921 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 128 |  TrainLoss: 0.00051 | TestLoss: 0.86955 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 129 |  TrainLoss: 0.00041 | TestLoss: 0.86948 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 130 |  TrainLoss: 0.00045 | TestLoss: 0.87025 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 131 |  TrainLoss: 0.00038 | TestLoss: 0.87109 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 132 |  TrainLoss: 0.00040 | TestLoss: 0.87282 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 133 |  TrainLoss: 0.00037 | TestLoss: 0.87450 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 134 |  TrainLoss: 0.00032 | TestLoss: 0.87569 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 135 |  TrainLoss: 0.00042 | TestLoss: 0.87718 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 136 |  TrainLoss: 0.00041 | TestLoss: 0.87949 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 137 |  TrainLoss: 0.00040 | TestLoss: 0.88294 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 138 |  TrainLoss: 0.00031 | TestLoss: 0.88634 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 139 |  TrainLoss: 0.00034 | TestLoss: 0.88896 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 140 |  TrainLoss: 0.00036 | TestLoss: 0.89103 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 141 |  TrainLoss: 0.00025 | TestLoss: 0.89282 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 142 |  TrainLoss: 0.00027 | TestLoss: 0.89471 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 143 |  TrainLoss: 0.00034 | TestLoss: 0.89583 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 144 |  TrainLoss: 0.00036 | TestLoss: 0.89708 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 145 |  TrainLoss: 0.00026 | TestLoss: 0.89851 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 146 |  TrainLoss: 0.00034 | TestLoss: 0.90026 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 147 |  TrainLoss: 0.00033 | TestLoss: 0.90140 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 148 |  TrainLoss: 0.00029 | TestLoss: 0.90289 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 149 |  TrainLoss: 0.00033 | TestLoss: 0.90405 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 150 |  TrainLoss: 0.00053 | TestLoss: 0.90429 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 151 |  TrainLoss: 0.00044 | TestLoss: 0.90586 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 152 |  TrainLoss: 0.00031 | TestLoss: 0.90696 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 153 |  TrainLoss: 0.00035 | TestLoss: 0.90907 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 154 |  TrainLoss: 0.00026 | TestLoss: 0.91106 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 155 |  TrainLoss: 0.00030 | TestLoss: 0.91292 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 156 |  TrainLoss: 0.00047 | TestLoss: 0.91475 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 157 |  TrainLoss: 0.00027 | TestLoss: 0.91677 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 158 |  TrainLoss: 0.00021 | TestLoss: 0.91866 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 159 |  TrainLoss: 0.00029 | TestLoss: 0.91966 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 160 |  TrainLoss: 0.00021 | TestLoss: 0.92043 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 161 |  TrainLoss: 0.00031 | TestLoss: 0.91992 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 162 |  TrainLoss: 0.00039 | TestLoss: 0.91960 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 163 |  TrainLoss: 0.00030 | TestLoss: 0.91967 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 164 |  TrainLoss: 0.00024 | TestLoss: 0.91996 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 165 |  TrainLoss: 0.00033 | TestLoss: 0.91932 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 166 |  TrainLoss: 0.00025 | TestLoss: 0.91882 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 167 |  TrainLoss: 0.00023 | TestLoss: 0.91773 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 168 |  TrainLoss: 0.00025 | TestLoss: 0.91758 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 169 |  TrainLoss: 0.00018 | TestLoss: 0.91779 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 170 |  TrainLoss: 0.00021 | TestLoss: 0.91775 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 171 |  TrainLoss: 0.00027 | TestLoss: 0.91879 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 172 |  TrainLoss: 0.00038 | TestLoss: 0.92126 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 173 |  TrainLoss: 0.00026 | TestLoss: 0.92401 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 174 |  TrainLoss: 0.00040 | TestLoss: 0.92900 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 175 |  TrainLoss: 0.00030 | TestLoss: 0.93497 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 176 |  TrainLoss: 0.00030 | TestLoss: 0.94006 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 177 |  TrainLoss: 0.00034 | TestLoss: 0.94516 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 178 |  TrainLoss: 0.00031 | TestLoss: 0.94947 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 179 |  TrainLoss: 0.00032 | TestLoss: 0.95356 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 180 |  TrainLoss: 0.00027 | TestLoss: 0.95551 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 181 |  TrainLoss: 0.00026 | TestLoss: 0.95672 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 182 |  TrainLoss: 0.00026 | TestLoss: 0.95684 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 183 |  TrainLoss: 0.00022 | TestLoss: 0.95687 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 184 |  TrainLoss: 0.00029 | TestLoss: 0.95637 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 185 |  TrainLoss: 0.00023 | TestLoss: 0.95551 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 186 |  TrainLoss: 0.00019 | TestLoss: 0.95389 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 187 |  TrainLoss: 0.00026 | TestLoss: 0.95242 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 188 |  TrainLoss: 0.00021 | TestLoss: 0.95102 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 189 |  TrainLoss: 0.00030 | TestLoss: 0.94914 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 190 |  TrainLoss: 0.00027 | TestLoss: 0.94954 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 191 |  TrainLoss: 0.00023 | TestLoss: 0.95046 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 192 |  TrainLoss: 0.00022 | TestLoss: 0.95185 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 193 |  TrainLoss: 0.00016 | TestLoss: 0.95314 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 194 |  TrainLoss: 0.00026 | TestLoss: 0.95546 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 195 |  TrainLoss: 0.00020 | TestLoss: 0.95785 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 196 |  TrainLoss: 0.00022 | TestLoss: 0.96006 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 197 |  TrainLoss: 0.00019 | TestLoss: 0.96189 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 198 |  TrainLoss: 0.00018 | TestLoss: 0.96327 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 199 |  TrainLoss: 0.00023 | TestLoss: 0.96508 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 200 |  TrainLoss: 0.00015 | TestLoss: 0.96701 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 201 |  TrainLoss: 0.00014 | TestLoss: 0.96859 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 202 |  TrainLoss: 0.00018 | TestLoss: 0.96963 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 203 |  TrainLoss: 0.00015 | TestLoss: 0.97104 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 204 |  TrainLoss: 0.00014 | TestLoss: 0.97223 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 205 |  TrainLoss: 0.00021 | TestLoss: 0.97377 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 206 |  TrainLoss: 0.00022 | TestLoss: 0.97669 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 207 |  TrainLoss: 0.00022 | TestLoss: 0.97780 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 208 |  TrainLoss: 0.00020 | TestLoss: 0.97764 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 209 |  TrainLoss: 0.00017 | TestLoss: 0.97681 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 210 |  TrainLoss: 0.00013 | TestLoss: 0.97535 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 211 |  TrainLoss: 0.00022 | TestLoss: 0.97490 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 212 |  TrainLoss: 0.00019 | TestLoss: 0.97484 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 213 |  TrainLoss: 0.00017 | TestLoss: 0.97454 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 214 |  TrainLoss: 0.00014 | TestLoss: 0.97431 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 215 |  TrainLoss: 0.00018 | TestLoss: 0.97535 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 216 |  TrainLoss: 0.00017 | TestLoss: 0.97683 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 217 |  TrainLoss: 0.00019 | TestLoss: 0.97839 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 218 |  TrainLoss: 0.00020 | TestLoss: 0.97985 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 219 |  TrainLoss: 0.00012 | TestLoss: 0.98115 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 220 |  TrainLoss: 0.00017 | TestLoss: 0.98241 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 221 |  TrainLoss: 0.00011 | TestLoss: 0.98384 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 222 |  TrainLoss: 0.00017 | TestLoss: 0.98417 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 223 |  TrainLoss: 0.00018 | TestLoss: 0.98557 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 224 |  TrainLoss: 0.00019 | TestLoss: 0.98752 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 225 |  TrainLoss: 0.00013 | TestLoss: 0.98887 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 226 |  TrainLoss: 0.00014 | TestLoss: 0.98983 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 227 |  TrainLoss: 0.00011 | TestLoss: 0.99119 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 228 |  TrainLoss: 0.00014 | TestLoss: 0.99324 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 229 |  TrainLoss: 0.00014 | TestLoss: 0.99448 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 230 |  TrainLoss: 0.00022 | TestLoss: 0.99633 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 231 |  TrainLoss: 0.00013 | TestLoss: 0.99733 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 232 |  TrainLoss: 0.00015 | TestLoss: 0.99950 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 233 |  TrainLoss: 0.00025 | TestLoss: 1.00373 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 234 |  TrainLoss: 0.00017 | TestLoss: 1.00791 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 235 |  TrainLoss: 0.00011 | TestLoss: 1.01115 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 236 |  TrainLoss: 0.00009 | TestLoss: 1.01469 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 237 |  TrainLoss: 0.00014 | TestLoss: 1.01762 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 238 |  TrainLoss: 0.00020 | TestLoss: 1.02016 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 239 |  TrainLoss: 0.00012 | TestLoss: 1.02236 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 240 |  TrainLoss: 0.00016 | TestLoss: 1.02384 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 241 |  TrainLoss: 0.00012 | TestLoss: 1.02496 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 242 |  TrainLoss: 0.00016 | TestLoss: 1.02540 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 243 |  TrainLoss: 0.00012 | TestLoss: 1.02524 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 244 |  TrainLoss: 0.00014 | TestLoss: 1.02477 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 245 |  TrainLoss: 0.00019 | TestLoss: 1.02365 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 246 |  TrainLoss: 0.00010 | TestLoss: 1.02282 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 247 |  TrainLoss: 0.00010 | TestLoss: 1.02199 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 248 |  TrainLoss: 0.00015 | TestLoss: 1.02160 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 249 |  TrainLoss: 0.00012 | TestLoss: 1.02132 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 250 |  TrainLoss: 0.00010 | TestLoss: 1.02121 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 251 |  TrainLoss: 0.00011 | TestLoss: 1.02164 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 252 |  TrainLoss: 0.00012 | TestLoss: 1.02115 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 253 |  TrainLoss: 0.00016 | TestLoss: 1.02145 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 254 |  TrainLoss: 0.00019 | TestLoss: 1.01997 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 255 |  TrainLoss: 0.00009 | TestLoss: 1.01837 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 256 |  TrainLoss: 0.00010 | TestLoss: 1.01675 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 257 |  TrainLoss: 0.00008 | TestLoss: 1.01493 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 258 |  TrainLoss: 0.00011 | TestLoss: 1.01310 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 259 |  TrainLoss: 0.00013 | TestLoss: 1.01399 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 260 |  TrainLoss: 0.00008 | TestLoss: 1.01511 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 261 |  TrainLoss: 0.00012 | TestLoss: 1.01718 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 262 |  TrainLoss: 0.00009 | TestLoss: 1.01916 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 263 |  TrainLoss: 0.00010 | TestLoss: 1.02245 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 264 |  TrainLoss: 0.00006 | TestLoss: 1.02560 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 265 |  TrainLoss: 0.00014 | TestLoss: 1.02966 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 266 |  TrainLoss: 0.00008 | TestLoss: 1.03331 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 267 |  TrainLoss: 0.00010 | TestLoss: 1.03747 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 268 |  TrainLoss: 0.00011 | TestLoss: 1.04196 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 269 |  TrainLoss: 0.00008 | TestLoss: 1.04582 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 270 |  TrainLoss: 0.00011 | TestLoss: 1.04806 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 271 |  TrainLoss: 0.00011 | TestLoss: 1.04918 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 272 |  TrainLoss: 0.00008 | TestLoss: 1.04988 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 273 |  TrainLoss: 0.00009 | TestLoss: 1.05054 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 274 |  TrainLoss: 0.00009 | TestLoss: 1.05096 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 275 |  TrainLoss: 0.00007 | TestLoss: 1.05123 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 276 |  TrainLoss: 0.00011 | TestLoss: 1.05212 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 277 |  TrainLoss: 0.00008 | TestLoss: 1.05286 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 278 |  TrainLoss: 0.00010 | TestLoss: 1.05308 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 279 |  TrainLoss: 0.00006 | TestLoss: 1.05354 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 280 |  TrainLoss: 0.00008 | TestLoss: 1.05470 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 281 |  TrainLoss: 0.00008 | TestLoss: 1.05650 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 282 |  TrainLoss: 0.00009 | TestLoss: 1.05793 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 283 |  TrainLoss: 0.00007 | TestLoss: 1.05954 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 284 |  TrainLoss: 0.00008 | TestLoss: 1.06027 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 285 |  TrainLoss: 0.00007 | TestLoss: 1.06033 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 286 |  TrainLoss: 0.00005 | TestLoss: 1.06012 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 287 |  TrainLoss: 0.00007 | TestLoss: 1.06071 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 288 |  TrainLoss: 0.00006 | TestLoss: 1.06115 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 289 |  TrainLoss: 0.00008 | TestLoss: 1.06290 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 290 |  TrainLoss: 0.00009 | TestLoss: 1.06458 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 291 |  TrainLoss: 0.00007 | TestLoss: 1.06636 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 292 |  TrainLoss: 0.00008 | TestLoss: 1.06871 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 293 |  TrainLoss: 0.00006 | TestLoss: 1.07050 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 294 |  TrainLoss: 0.00007 | TestLoss: 1.07193 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 295 |  TrainLoss: 0.00006 | TestLoss: 1.07383 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 296 |  TrainLoss: 0.00012 | TestLoss: 1.07605 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 297 |  TrainLoss: 0.00006 | TestLoss: 1.07789 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 298 |  TrainLoss: 0.00006 | TestLoss: 1.07931 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 299 |  TrainLoss: 0.00009 | TestLoss: 1.07925 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 300 |  TrainLoss: 0.00009 | TestLoss: 1.08037 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 301 |  TrainLoss: 0.00006 | TestLoss: 1.08128 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 302 |  TrainLoss: 0.00008 | TestLoss: 1.08157 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 303 |  TrainLoss: 0.00006 | TestLoss: 1.08219 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 304 |  TrainLoss: 0.00006 | TestLoss: 1.08211 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 305 |  TrainLoss: 0.00004 | TestLoss: 1.08277 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 306 |  TrainLoss: 0.00007 | TestLoss: 1.08307 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 307 |  TrainLoss: 0.00006 | TestLoss: 1.08326 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 308 |  TrainLoss: 0.00005 | TestLoss: 1.08452 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 309 |  TrainLoss: 0.00004 | TestLoss: 1.08569 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 310 |  TrainLoss: 0.00004 | TestLoss: 1.08712 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 311 |  TrainLoss: 0.00005 | TestLoss: 1.08895 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 312 |  TrainLoss: 0.00007 | TestLoss: 1.09080 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 313 |  TrainLoss: 0.00005 | TestLoss: 1.09159 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 314 |  TrainLoss: 0.00006 | TestLoss: 1.09189 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 315 |  TrainLoss: 0.00004 | TestLoss: 1.09231 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 316 |  TrainLoss: 0.00004 | TestLoss: 1.09377 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 317 |  TrainLoss: 0.00006 | TestLoss: 1.09504 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 318 |  TrainLoss: 0.00005 | TestLoss: 1.09578 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 319 |  TrainLoss: 0.00006 | TestLoss: 1.09700 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 320 |  TrainLoss: 0.00006 | TestLoss: 1.09950 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 321 |  TrainLoss: 0.00005 | TestLoss: 1.10221 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 322 |  TrainLoss: 0.00004 | TestLoss: 1.10498 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 323 |  TrainLoss: 0.00005 | TestLoss: 1.10727 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 324 |  TrainLoss: 0.00004 | TestLoss: 1.10875 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 325 |  TrainLoss: 0.00004 | TestLoss: 1.10995 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 326 |  TrainLoss: 0.00005 | TestLoss: 1.11191 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 327 |  TrainLoss: 0.00005 | TestLoss: 1.11519 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 328 |  TrainLoss: 0.00005 | TestLoss: 1.11649 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 329 |  TrainLoss: 0.00004 | TestLoss: 1.11745 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 330 |  TrainLoss: 0.00008 | TestLoss: 1.11876 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 331 |  TrainLoss: 0.00005 | TestLoss: 1.12120 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 332 |  TrainLoss: 0.00004 | TestLoss: 1.12265 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 333 |  TrainLoss: 0.00005 | TestLoss: 1.12489 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 334 |  TrainLoss: 0.00005 | TestLoss: 1.12661 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 335 |  TrainLoss: 0.00005 | TestLoss: 1.12931 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 336 |  TrainLoss: 0.00003 | TestLoss: 1.13104 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 337 |  TrainLoss: 0.00005 | TestLoss: 1.13289 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 338 |  TrainLoss: 0.00013 | TestLoss: 1.13035 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 339 |  TrainLoss: 0.00004 | TestLoss: 1.12821 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 340 |  TrainLoss: 0.00004 | TestLoss: 1.12685 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 341 |  TrainLoss: 0.00005 | TestLoss: 1.12472 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 342 |  TrainLoss: 0.00003 | TestLoss: 1.12311 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 343 |  TrainLoss: 0.00005 | TestLoss: 1.12129 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 344 |  TrainLoss: 0.00005 | TestLoss: 1.11942 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 345 |  TrainLoss: 0.00005 | TestLoss: 1.12019 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 346 |  TrainLoss: 0.00005 | TestLoss: 1.12129 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 347 |  TrainLoss: 0.00004 | TestLoss: 1.12328 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 348 |  TrainLoss: 0.00005 | TestLoss: 1.12633 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 349 |  TrainLoss: 0.00004 | TestLoss: 1.12893 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 350 |  TrainLoss: 0.00004 | TestLoss: 1.13126 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 351 |  TrainLoss: 0.00007 | TestLoss: 1.13543 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 352 |  TrainLoss: 0.00004 | TestLoss: 1.13968 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 353 |  TrainLoss: 0.00005 | TestLoss: 1.14431 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 354 |  TrainLoss: 0.00004 | TestLoss: 1.14877 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 355 |  TrainLoss: 0.00003 | TestLoss: 1.15274 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 356 |  TrainLoss: 0.00003 | TestLoss: 1.15591 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 357 |  TrainLoss: 0.00003 | TestLoss: 1.15867 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 358 |  TrainLoss: 0.00004 | TestLoss: 1.16079 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 359 |  TrainLoss: 0.00002 | TestLoss: 1.16294 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 360 |  TrainLoss: 0.00003 | TestLoss: 1.16461 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 361 |  TrainLoss: 0.00005 | TestLoss: 1.16426 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 362 |  TrainLoss: 0.00003 | TestLoss: 1.16360 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 363 |  TrainLoss: 0.00005 | TestLoss: 1.16357 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 364 |  TrainLoss: 0.00004 | TestLoss: 1.16254 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 365 |  TrainLoss: 0.00003 | TestLoss: 1.16152 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 366 |  TrainLoss: 0.00004 | TestLoss: 1.15927 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 367 |  TrainLoss: 0.00004 | TestLoss: 1.15777 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 368 |  TrainLoss: 0.00005 | TestLoss: 1.15586 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 369 |  TrainLoss: 0.00003 | TestLoss: 1.15570 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 370 |  TrainLoss: 0.00003 | TestLoss: 1.15505 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 371 |  TrainLoss: 0.00003 | TestLoss: 1.15447 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 372 |  TrainLoss: 0.00002 | TestLoss: 1.15381 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 373 |  TrainLoss: 0.00006 | TestLoss: 1.15520 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 374 |  TrainLoss: 0.00005 | TestLoss: 1.15574 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 375 |  TrainLoss: 0.00003 | TestLoss: 1.15659 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 376 |  TrainLoss: 0.00003 | TestLoss: 1.15795 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 377 |  TrainLoss: 0.00003 | TestLoss: 1.15957 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 378 |  TrainLoss: 0.00002 | TestLoss: 1.16149 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 379 |  TrainLoss: 0.00005 | TestLoss: 1.16207 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 380 |  TrainLoss: 0.00002 | TestLoss: 1.16225 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 381 |  TrainLoss: 0.00003 | TestLoss: 1.16366 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 382 |  TrainLoss: 0.00002 | TestLoss: 1.16449 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 383 |  TrainLoss: 0.00004 | TestLoss: 1.16739 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 384 |  TrainLoss: 0.00004 | TestLoss: 1.16993 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 385 |  TrainLoss: 0.00003 | TestLoss: 1.17283 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 386 |  TrainLoss: 0.00003 | TestLoss: 1.17498 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 387 |  TrainLoss: 0.00002 | TestLoss: 1.17698 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 388 |  TrainLoss: 0.00003 | TestLoss: 1.17854 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 389 |  TrainLoss: 0.00003 | TestLoss: 1.18000 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 390 |  TrainLoss: 0.00002 | TestLoss: 1.18093 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 391 |  TrainLoss: 0.00003 | TestLoss: 1.18134 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 392 |  TrainLoss: 0.00003 | TestLoss: 1.18212 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 393 |  TrainLoss: 0.00002 | TestLoss: 1.18241 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 394 |  TrainLoss: 0.00002 | TestLoss: 1.18233 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 395 |  TrainLoss: 0.00003 | TestLoss: 1.18222 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 396 |  TrainLoss: 0.00002 | TestLoss: 1.18158 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 397 |  TrainLoss: 0.00001 | TestLoss: 1.18142 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 398 |  TrainLoss: 0.00002 | TestLoss: 1.18103 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 399 |  TrainLoss: 0.00002 | TestLoss: 1.18075 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 400 |  TrainLoss: 0.00002 | TestLoss: 1.18071 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 401 |  TrainLoss: 0.00003 | TestLoss: 1.18205 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 402 |  TrainLoss: 0.00002 | TestLoss: 1.18355 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 403 |  TrainLoss: 0.00004 | TestLoss: 1.18578 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 404 |  TrainLoss: 0.00002 | TestLoss: 1.18769 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 405 |  TrainLoss: 0.00003 | TestLoss: 1.18949 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 406 |  TrainLoss: 0.00002 | TestLoss: 1.19077 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 407 |  TrainLoss: 0.00002 | TestLoss: 1.19183 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 408 |  TrainLoss: 0.00002 | TestLoss: 1.19435 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 409 |  TrainLoss: 0.00003 | TestLoss: 1.19609 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 410 |  TrainLoss: 0.00002 | TestLoss: 1.19794 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 411 |  TrainLoss: 0.00002 | TestLoss: 1.19940 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 412 |  TrainLoss: 0.00002 | TestLoss: 1.20044 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 413 |  TrainLoss: 0.00001 | TestLoss: 1.20127 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 414 |  TrainLoss: 0.00002 | TestLoss: 1.20204 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 415 |  TrainLoss: 0.00003 | TestLoss: 1.20356 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 416 |  TrainLoss: 0.00002 | TestLoss: 1.20475 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 417 |  TrainLoss: 0.00002 | TestLoss: 1.20584 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 418 |  TrainLoss: 0.00002 | TestLoss: 1.20705 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 419 |  TrainLoss: 0.00002 | TestLoss: 1.20808 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 420 |  TrainLoss: 0.00001 | TestLoss: 1.20904 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 421 |  TrainLoss: 0.00002 | TestLoss: 1.21004 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 422 |  TrainLoss: 0.00002 | TestLoss: 1.21204 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 423 |  TrainLoss: 0.00002 | TestLoss: 1.21266 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 424 |  TrainLoss: 0.00002 | TestLoss: 1.21249 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 425 |  TrainLoss: 0.00003 | TestLoss: 1.21213 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 426 |  TrainLoss: 0.00002 | TestLoss: 1.21226 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 427 |  TrainLoss: 0.00004 | TestLoss: 1.21093 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 428 |  TrainLoss: 0.00002 | TestLoss: 1.21009 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 429 |  TrainLoss: 0.00002 | TestLoss: 1.20938 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 430 |  TrainLoss: 0.00002 | TestLoss: 1.20773 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 431 |  TrainLoss: 0.00001 | TestLoss: 1.20735 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 432 |  TrainLoss: 0.00004 | TestLoss: 1.20524 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 433 |  TrainLoss: 0.00002 | TestLoss: 1.20458 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 434 |  TrainLoss: 0.00003 | TestLoss: 1.20290 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 435 |  TrainLoss: 0.00003 | TestLoss: 1.20248 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 436 |  TrainLoss: 0.00003 | TestLoss: 1.20329 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 437 |  TrainLoss: 0.00002 | TestLoss: 1.20454 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 438 |  TrainLoss: 0.00002 | TestLoss: 1.20561 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 439 |  TrainLoss: 0.00003 | TestLoss: 1.20709 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 440 |  TrainLoss: 0.00001 | TestLoss: 1.20844 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 441 |  TrainLoss: 0.00001 | TestLoss: 1.20985 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 442 |  TrainLoss: 0.00002 | TestLoss: 1.21217 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 443 |  TrainLoss: 0.00001 | TestLoss: 1.21446 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 444 |  TrainLoss: 0.00001 | TestLoss: 1.21681 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 445 |  TrainLoss: 0.00001 | TestLoss: 1.21857 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 446 |  TrainLoss: 0.00002 | TestLoss: 1.22092 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 447 |  TrainLoss: 0.00002 | TestLoss: 1.22326 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 448 |  TrainLoss: 0.00002 | TestLoss: 1.22534 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 449 |  TrainLoss: 0.00002 | TestLoss: 1.22788 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 450 |  TrainLoss: 0.00003 | TestLoss: 1.23218 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 451 |  TrainLoss: 0.00002 | TestLoss: 1.23481 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 452 |  TrainLoss: 0.00001 | TestLoss: 1.23679 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 453 |  TrainLoss: 0.00002 | TestLoss: 1.23914 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 454 |  TrainLoss: 0.00002 | TestLoss: 1.24157 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 455 |  TrainLoss: 0.00002 | TestLoss: 1.24283 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 456 |  TrainLoss: 0.00002 | TestLoss: 1.24278 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 457 |  TrainLoss: 0.00002 | TestLoss: 1.24615 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 458 |  TrainLoss: 0.00001 | TestLoss: 1.24617 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 459 |  TrainLoss: 0.00002 | TestLoss: 1.24631 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 460 |  TrainLoss: 0.00002 | TestLoss: 1.24594 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 461 |  TrainLoss: 0.00002 | TestLoss: 1.24535 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 462 |  TrainLoss: 0.00002 | TestLoss: 1.24368 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 463 |  TrainLoss: 0.00002 | TestLoss: 1.24233 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 464 |  TrainLoss: 0.00002 | TestLoss: 1.24034 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 465 |  TrainLoss: 0.00002 | TestLoss: 1.23791 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 466 |  TrainLoss: 0.00001 | TestLoss: 1.23635 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 467 |  TrainLoss: 0.00003 | TestLoss: 1.23465 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 468 |  TrainLoss: 0.00001 | TestLoss: 1.23270 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 469 |  TrainLoss: 0.00001 | TestLoss: 1.23205 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 470 |  TrainLoss: 0.00001 | TestLoss: 1.23198 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 471 |  TrainLoss: 0.00001 | TestLoss: 1.23173 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 472 |  TrainLoss: 0.00002 | TestLoss: 1.23143 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 473 |  TrainLoss: 0.00003 | TestLoss: 1.23045 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 474 |  TrainLoss: 0.00002 | TestLoss: 1.23056 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 475 |  TrainLoss: 0.00002 | TestLoss: 1.23103 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 476 |  TrainLoss: 0.00002 | TestLoss: 1.23111 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 477 |  TrainLoss: 0.00001 | TestLoss: 1.23145 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 478 |  TrainLoss: 0.00002 | TestLoss: 1.23248 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 479 |  TrainLoss: 0.00002 | TestLoss: 1.23399 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 480 |  TrainLoss: 0.00002 | TestLoss: 1.23535 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 481 |  TrainLoss: 0.00001 | TestLoss: 1.23718 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 482 |  TrainLoss: 0.00001 | TestLoss: 1.23892 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 483 |  TrainLoss: 0.00002 | TestLoss: 1.24138 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 484 |  TrainLoss: 0.00001 | TestLoss: 1.24346 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 485 |  TrainLoss: 0.00001 | TestLoss: 1.24582 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 486 |  TrainLoss: 0.00001 | TestLoss: 1.24776 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 487 |  TrainLoss: 0.00001 | TestLoss: 1.24958 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 488 |  TrainLoss: 0.00001 | TestLoss: 1.25176 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 489 |  TrainLoss: 0.00002 | TestLoss: 1.25418 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 490 |  TrainLoss: 0.00001 | TestLoss: 1.25639 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 491 |  TrainLoss: 0.00001 | TestLoss: 1.25880 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 492 |  TrainLoss: 0.00001 | TestLoss: 1.26057 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 493 |  TrainLoss: 0.00002 | TestLoss: 1.26143 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 494 |  TrainLoss: 0.00002 | TestLoss: 1.26181 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 495 |  TrainLoss: 0.00001 | TestLoss: 1.26192 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 496 |  TrainLoss: 0.00001 | TestLoss: 1.26174 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 497 |  TrainLoss: 0.00001 | TestLoss: 1.26170 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 498 |  TrainLoss: 0.00001 | TestLoss: 1.26174 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 499 |  TrainLoss: 0.00002 | TestLoss: 1.26061 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Best WLoss: 0.00194 | Best Epoch: 52\n"
     ]
    }
   ],
   "source": [
    "wloss = []\n",
    "weighted_loss = 0\n",
    "exp_param = 0.8\n",
    "best_test_loss = float('inf')  # Initialize with a large value\n",
    "\n",
    "# Without dropout training results\n",
    "for epoch in range(500):\n",
    "  train_loss = train(epoch)\n",
    "  test_loss, test_acc, test_f1 = test(epoch)\n",
    "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
    "  \n",
    "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
    "  \n",
    "  if test_loss < best_test_loss:\n",
    "    best_test_loss = test_loss  # Update the best test loss\n",
    "\n",
    "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
    "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
    "\n",
    "# Print the best values\n",
    "best_wloss = min(wloss)\n",
    "best_epoch = wloss.index(best_wloss)\n",
    "print(f'Best WLoss: {best_wloss:.5f} | Best Epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ecf28",
   "metadata": {},
   "source": [
    "# Code 4: \n",
    "GraphSage + Content with hyperparameters as defined in the paper and replacemenent of 1 MLP layer with 1 RNN Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c7b2ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, embedding_size, batch_size, l2_reg_weight):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.batch_size = batch_size\n",
    "        self.l2_reg_weight = l2_reg_weight\n",
    "        \n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
    "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
    "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_size, hidden_channels[3], batch_first=True)\n",
    "        self.full1 = nn.Linear(hidden_channels[3], hidden_channels[4])\n",
    "        self.softmax = nn.Linear(hidden_channels[4], out_channels)\n",
    "\n",
    "        # Dropouts\n",
    "        self.dp1 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = self.conv1(x, edge_index).relu()\n",
    "        h = self.conv2(h, edge_index).relu()\n",
    "        h = self.conv3(h, edge_index).relu()\n",
    "\n",
    "        h = global_max_pool(h, batch)\n",
    "        \n",
    "        # Reshape the input tensor for RNN\n",
    "        h = h.unsqueeze(0)  # Add a time dimension\n",
    "        h = self.dp1(h)\n",
    "        \n",
    "        # Apply RNN\n",
    "        h, _ = self.rnn(h)\n",
    "        h = h.squeeze(0)  # Remove the time dimension\n",
    "        \n",
    "        h = self.full1(h).relu()\n",
    "        h = self.softmax(h)\n",
    "\n",
    "        return torch.sigmoid(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad863038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch: 00 | TrainLoss: 0.69532 | TestLoss: 0.69257 | TestAcc: 0.51131 | TestF1: 0.21\n",
      "Epoch: 01 | TrainLoss: 0.69383 | TestLoss: 0.69354 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 02 | TrainLoss: 0.69133 | TestLoss: 0.69481 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 03 | TrainLoss: 0.69154 | TestLoss: 0.69585 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 04 | TrainLoss: 0.69093 | TestLoss: 0.69642 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 05 | TrainLoss: 0.69183 | TestLoss: 0.69668 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 06 | TrainLoss: 0.69188 | TestLoss: 0.69624 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 07 | TrainLoss: 0.69076 | TestLoss: 0.69543 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 08 | TrainLoss: 0.68994 | TestLoss: 0.69437 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 09 | TrainLoss: 0.69015 | TestLoss: 0.69334 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 10 | TrainLoss: 0.68911 | TestLoss: 0.69225 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 11 | TrainLoss: 0.68867 | TestLoss: 0.69133 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 12 | TrainLoss: 0.68736 | TestLoss: 0.69072 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 13 | TrainLoss: 0.68903 | TestLoss: 0.69020 | TestAcc: 0.49321 | TestF1: 0.02\n",
      "Epoch: 14 | TrainLoss: 0.68878 | TestLoss: 0.68998 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 15 | TrainLoss: 0.68826 | TestLoss: 0.68987 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 16 | TrainLoss: 0.68767 | TestLoss: 0.68981 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 17 | TrainLoss: 0.68605 | TestLoss: 0.68978 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 18 | TrainLoss: 0.68367 | TestLoss: 0.68961 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 19 | TrainLoss: 0.68294 | TestLoss: 0.68929 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 20 | TrainLoss: 0.68487 | TestLoss: 0.68860 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 21 | TrainLoss: 0.68264 | TestLoss: 0.68770 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 22 | TrainLoss: 0.68560 | TestLoss: 0.68652 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 23 | TrainLoss: 0.68498 | TestLoss: 0.68531 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 24 | TrainLoss: 0.68237 | TestLoss: 0.68410 | TestAcc: 0.49321 | TestF1: 0.02\n",
      "Epoch: 25 | TrainLoss: 0.68232 | TestLoss: 0.68307 | TestAcc: 0.49321 | TestF1: 0.02\n",
      "Epoch: 26 | TrainLoss: 0.67902 | TestLoss: 0.68234 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 27 | TrainLoss: 0.67664 | TestLoss: 0.68196 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 28 | TrainLoss: 0.67637 | TestLoss: 0.68158 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 29 | TrainLoss: 0.67304 | TestLoss: 0.68068 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 30 | TrainLoss: 0.67359 | TestLoss: 0.67891 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 31 | TrainLoss: 0.67324 | TestLoss: 0.67631 | TestAcc: 0.49321 | TestF1: 0.02\n",
      "Epoch: 32 | TrainLoss: 0.67060 | TestLoss: 0.67346 | TestAcc: 0.57919 | TestF1: 0.30\n",
      "Epoch: 33 | TrainLoss: 0.66750 | TestLoss: 0.67079 | TestAcc: 0.68326 | TestF1: 0.57\n",
      "Epoch: 34 | TrainLoss: 0.66526 | TestLoss: 0.66897 | TestAcc: 0.66516 | TestF1: 0.51\n",
      "Epoch: 35 | TrainLoss: 0.66418 | TestLoss: 0.66673 | TestAcc: 0.67421 | TestF1: 0.53\n",
      "Epoch: 36 | TrainLoss: 0.65816 | TestLoss: 0.66434 | TestAcc: 0.66063 | TestF1: 0.50\n",
      "Epoch: 37 | TrainLoss: 0.65846 | TestLoss: 0.66117 | TestAcc: 0.68778 | TestF1: 0.57\n",
      "Epoch: 38 | TrainLoss: 0.65039 | TestLoss: 0.65759 | TestAcc: 0.70588 | TestF1: 0.61\n",
      "Epoch: 39 | TrainLoss: 0.64721 | TestLoss: 0.65317 | TestAcc: 0.76018 | TestF1: 0.71\n",
      "Epoch: 40 | TrainLoss: 0.64606 | TestLoss: 0.64799 | TestAcc: 0.79186 | TestF1: 0.77\n",
      "Epoch: 41 | TrainLoss: 0.63771 | TestLoss: 0.64516 | TestAcc: 0.76018 | TestF1: 0.71\n",
      "Epoch: 42 | TrainLoss: 0.63090 | TestLoss: 0.64028 | TestAcc: 0.76471 | TestF1: 0.72\n",
      "Epoch: 43 | TrainLoss: 0.62381 | TestLoss: 0.63211 | TestAcc: 0.81900 | TestF1: 0.82\n",
      "Epoch: 44 | TrainLoss: 0.61567 | TestLoss: 0.62757 | TestAcc: 0.80090 | TestF1: 0.78\n",
      "Epoch: 45 | TrainLoss: 0.61156 | TestLoss: 0.62408 | TestAcc: 0.77376 | TestF1: 0.73\n",
      "Epoch: 46 | TrainLoss: 0.60354 | TestLoss: 0.61231 | TestAcc: 0.81448 | TestF1: 0.81\n",
      "Epoch: 47 | TrainLoss: 0.59705 | TestLoss: 0.60507 | TestAcc: 0.81900 | TestF1: 0.81\n",
      "Epoch: 48 | TrainLoss: 0.58900 | TestLoss: 0.60135 | TestAcc: 0.79638 | TestF1: 0.76\n",
      "Epoch: 49 | TrainLoss: 0.57650 | TestLoss: 0.58601 | TestAcc: 0.81900 | TestF1: 0.82\n",
      "Epoch: 50 | TrainLoss: 0.56671 | TestLoss: 0.57863 | TestAcc: 0.81900 | TestF1: 0.81\n",
      "Epoch: 51 | TrainLoss: 0.55329 | TestLoss: 0.57866 | TestAcc: 0.78281 | TestF1: 0.74\n",
      "Epoch: 52 | TrainLoss: 0.54613 | TestLoss: 0.55684 | TestAcc: 0.80995 | TestF1: 0.83\n",
      "Epoch: 53 | TrainLoss: 0.54136 | TestLoss: 0.55411 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 54 | TrainLoss: 0.52809 | TestLoss: 0.54581 | TestAcc: 0.82805 | TestF1: 0.81\n",
      "Epoch: 55 | TrainLoss: 0.51449 | TestLoss: 0.52567 | TestAcc: 0.83258 | TestF1: 0.84\n",
      "Epoch: 56 | TrainLoss: 0.50091 | TestLoss: 0.52871 | TestAcc: 0.82805 | TestF1: 0.81\n",
      "Epoch: 57 | TrainLoss: 0.48430 | TestLoss: 0.50664 | TestAcc: 0.84615 | TestF1: 0.85\n",
      "Epoch: 58 | TrainLoss: 0.47211 | TestLoss: 0.49463 | TestAcc: 0.84163 | TestF1: 0.85\n",
      "Epoch: 59 | TrainLoss: 0.46215 | TestLoss: 0.51576 | TestAcc: 0.78733 | TestF1: 0.75\n",
      "Epoch: 60 | TrainLoss: 0.45998 | TestLoss: 0.47456 | TestAcc: 0.85520 | TestF1: 0.86\n",
      "Epoch: 61 | TrainLoss: 0.42885 | TestLoss: 0.46485 | TestAcc: 0.84615 | TestF1: 0.86\n",
      "Epoch: 62 | TrainLoss: 0.42259 | TestLoss: 0.48061 | TestAcc: 0.80995 | TestF1: 0.79\n",
      "Epoch: 63 | TrainLoss: 0.41385 | TestLoss: 0.44605 | TestAcc: 0.86425 | TestF1: 0.87\n",
      "Epoch: 64 | TrainLoss: 0.39788 | TestLoss: 0.43980 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 65 | TrainLoss: 0.39012 | TestLoss: 0.44297 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 66 | TrainLoss: 0.36867 | TestLoss: 0.41896 | TestAcc: 0.86878 | TestF1: 0.88\n",
      "Epoch: 67 | TrainLoss: 0.36288 | TestLoss: 0.41592 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 68 | TrainLoss: 0.36238 | TestLoss: 0.40342 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 69 | TrainLoss: 0.34124 | TestLoss: 0.39688 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 70 | TrainLoss: 0.32920 | TestLoss: 0.38514 | TestAcc: 0.87783 | TestF1: 0.89\n",
      "Epoch: 71 | TrainLoss: 0.32249 | TestLoss: 0.41121 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 72 | TrainLoss: 0.30926 | TestLoss: 0.39787 | TestAcc: 0.80995 | TestF1: 0.83\n",
      "Epoch: 73 | TrainLoss: 0.33372 | TestLoss: 0.41323 | TestAcc: 0.82805 | TestF1: 0.81\n",
      "Epoch: 74 | TrainLoss: 0.32588 | TestLoss: 0.36522 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 75 | TrainLoss: 0.29414 | TestLoss: 0.38337 | TestAcc: 0.81900 | TestF1: 0.84\n",
      "Epoch: 76 | TrainLoss: 0.31740 | TestLoss: 0.39474 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 77 | TrainLoss: 0.29870 | TestLoss: 0.35882 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 78 | TrainLoss: 0.25648 | TestLoss: 0.36029 | TestAcc: 0.83710 | TestF1: 0.85\n",
      "Epoch: 79 | TrainLoss: 0.29014 | TestLoss: 0.34660 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 80 | TrainLoss: 0.23483 | TestLoss: 0.37489 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 81 | TrainLoss: 0.26376 | TestLoss: 0.33825 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 82 | TrainLoss: 0.24986 | TestLoss: 0.33711 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 83 | TrainLoss: 0.23799 | TestLoss: 0.35848 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 84 | TrainLoss: 0.25417 | TestLoss: 0.33199 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 85 | TrainLoss: 0.22514 | TestLoss: 0.33520 | TestAcc: 0.87783 | TestF1: 0.89\n",
      "Epoch: 86 | TrainLoss: 0.23810 | TestLoss: 0.33097 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 87 | TrainLoss: 0.21524 | TestLoss: 0.34260 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 88 | TrainLoss: 0.22985 | TestLoss: 0.32101 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 89 | TrainLoss: 0.21224 | TestLoss: 0.31978 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 90 | TrainLoss: 0.21437 | TestLoss: 0.34847 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 91 | TrainLoss: 0.21941 | TestLoss: 0.31579 | TestAcc: 0.90045 | TestF1: 0.91\n",
      "Epoch: 92 | TrainLoss: 0.20691 | TestLoss: 0.31299 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 93 | TrainLoss: 0.19070 | TestLoss: 0.32545 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 94 | TrainLoss: 0.19846 | TestLoss: 0.30958 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 95 | TrainLoss: 0.18334 | TestLoss: 0.31281 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 96 | TrainLoss: 0.20728 | TestLoss: 0.34005 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 97 | TrainLoss: 0.19711 | TestLoss: 0.30372 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 98 | TrainLoss: 0.17896 | TestLoss: 0.30482 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 99 | TrainLoss: 0.17508 | TestLoss: 0.32346 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 100 | TrainLoss: 0.16796 | TestLoss: 0.29987 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 101 | TrainLoss: 0.16420 | TestLoss: 0.29729 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 102 | TrainLoss: 0.15188 | TestLoss: 0.31615 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 103 | TrainLoss: 0.18872 | TestLoss: 0.30140 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 104 | TrainLoss: 0.17602 | TestLoss: 0.29303 | TestAcc: 0.90498 | TestF1: 0.91\n",
      "Epoch: 105 | TrainLoss: 0.15941 | TestLoss: 0.30777 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 106 | TrainLoss: 0.16056 | TestLoss: 0.29865 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 107 | TrainLoss: 0.16637 | TestLoss: 0.31074 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 108 | TrainLoss: 0.15062 | TestLoss: 0.28827 | TestAcc: 0.90950 | TestF1: 0.91\n",
      "Epoch: 109 | TrainLoss: 0.13013 | TestLoss: 0.29232 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 110 | TrainLoss: 0.14791 | TestLoss: 0.31066 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 111 | TrainLoss: 0.14053 | TestLoss: 0.29000 | TestAcc: 0.90045 | TestF1: 0.91\n",
      "Epoch: 112 | TrainLoss: 0.14375 | TestLoss: 0.28804 | TestAcc: 0.90950 | TestF1: 0.91\n",
      "Epoch: 113 | TrainLoss: 0.12934 | TestLoss: 0.28441 | TestAcc: 0.90498 | TestF1: 0.91\n",
      "Epoch: 114 | TrainLoss: 0.11812 | TestLoss: 0.28289 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 115 | TrainLoss: 0.12161 | TestLoss: 0.28223 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 116 | TrainLoss: 0.11713 | TestLoss: 0.28808 | TestAcc: 0.90498 | TestF1: 0.91\n",
      "Epoch: 117 | TrainLoss: 0.12013 | TestLoss: 0.28133 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 118 | TrainLoss: 0.11491 | TestLoss: 0.28135 | TestAcc: 0.90950 | TestF1: 0.91\n",
      "Epoch: 119 | TrainLoss: 0.10976 | TestLoss: 0.28255 | TestAcc: 0.90950 | TestF1: 0.91\n",
      "Epoch: 120 | TrainLoss: 0.11196 | TestLoss: 0.29266 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 121 | TrainLoss: 0.12743 | TestLoss: 0.30712 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 122 | TrainLoss: 0.10949 | TestLoss: 0.28052 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 123 | TrainLoss: 0.11191 | TestLoss: 0.28239 | TestAcc: 0.90498 | TestF1: 0.91\n",
      "Epoch: 124 | TrainLoss: 0.09230 | TestLoss: 0.27851 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 125 | TrainLoss: 0.10723 | TestLoss: 0.27812 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 126 | TrainLoss: 0.09616 | TestLoss: 0.27874 | TestAcc: 0.91403 | TestF1: 0.92\n",
      "Epoch: 127 | TrainLoss: 0.10248 | TestLoss: 0.27770 | TestAcc: 0.90498 | TestF1: 0.91\n",
      "Epoch: 128 | TrainLoss: 0.08797 | TestLoss: 0.28325 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 129 | TrainLoss: 0.10329 | TestLoss: 0.31465 | TestAcc: 0.88688 | TestF1: 0.88\n",
      "Epoch: 130 | TrainLoss: 0.09210 | TestLoss: 0.30430 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 131 | TrainLoss: 0.09919 | TestLoss: 0.29100 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 132 | TrainLoss: 0.08321 | TestLoss: 0.28848 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 133 | TrainLoss: 0.08361 | TestLoss: 0.31228 | TestAcc: 0.86878 | TestF1: 0.88\n",
      "Epoch: 134 | TrainLoss: 0.10509 | TestLoss: 0.33743 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 135 | TrainLoss: 0.11974 | TestLoss: 0.30273 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 136 | TrainLoss: 0.10013 | TestLoss: 0.28031 | TestAcc: 0.90950 | TestF1: 0.91\n",
      "Epoch: 137 | TrainLoss: 0.08280 | TestLoss: 0.29465 | TestAcc: 0.89593 | TestF1: 0.89\n",
      "Epoch: 138 | TrainLoss: 0.07587 | TestLoss: 0.28044 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 139 | TrainLoss: 0.05912 | TestLoss: 0.28139 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 140 | TrainLoss: 0.07266 | TestLoss: 0.28737 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 141 | TrainLoss: 0.06887 | TestLoss: 0.29329 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 142 | TrainLoss: 0.07064 | TestLoss: 0.29246 | TestAcc: 0.89593 | TestF1: 0.89\n",
      "Epoch: 143 | TrainLoss: 0.06917 | TestLoss: 0.28021 | TestAcc: 0.90498 | TestF1: 0.91\n",
      "Epoch: 144 | TrainLoss: 0.05856 | TestLoss: 0.28063 | TestAcc: 0.90498 | TestF1: 0.91\n",
      "Epoch: 145 | TrainLoss: 0.05223 | TestLoss: 0.28287 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 146 | TrainLoss: 0.05048 | TestLoss: 0.28101 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 147 | TrainLoss: 0.05793 | TestLoss: 0.28119 | TestAcc: 0.90498 | TestF1: 0.91\n",
      "Epoch: 148 | TrainLoss: 0.04704 | TestLoss: 0.28126 | TestAcc: 0.90498 | TestF1: 0.91\n",
      "Epoch: 149 | TrainLoss: 0.05156 | TestLoss: 0.28152 | TestAcc: 0.90498 | TestF1: 0.91\n",
      "Epoch: 150 | TrainLoss: 0.04866 | TestLoss: 0.28925 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 151 | TrainLoss: 0.05578 | TestLoss: 0.30377 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 152 | TrainLoss: 0.05345 | TestLoss: 0.29778 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 153 | TrainLoss: 0.05201 | TestLoss: 0.28502 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 154 | TrainLoss: 0.04314 | TestLoss: 0.28459 | TestAcc: 0.90498 | TestF1: 0.91\n",
      "Epoch: 155 | TrainLoss: 0.04827 | TestLoss: 0.28521 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 156 | TrainLoss: 0.03677 | TestLoss: 0.29638 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 157 | TrainLoss: 0.04535 | TestLoss: 0.31378 | TestAcc: 0.87783 | TestF1: 0.89\n",
      "Epoch: 158 | TrainLoss: 0.06013 | TestLoss: 0.32263 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 159 | TrainLoss: 0.06112 | TestLoss: 0.31523 | TestAcc: 0.87783 | TestF1: 0.89\n",
      "Epoch: 160 | TrainLoss: 0.05036 | TestLoss: 0.29230 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 161 | TrainLoss: 0.03930 | TestLoss: 0.32357 | TestAcc: 0.88688 | TestF1: 0.88\n",
      "Epoch: 162 | TrainLoss: 0.05228 | TestLoss: 0.33791 | TestAcc: 0.86878 | TestF1: 0.88\n",
      "Epoch: 163 | TrainLoss: 0.05632 | TestLoss: 0.29101 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 164 | TrainLoss: 0.03322 | TestLoss: 0.31232 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 165 | TrainLoss: 0.04798 | TestLoss: 0.31238 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 166 | TrainLoss: 0.04560 | TestLoss: 0.29807 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 167 | TrainLoss: 0.03607 | TestLoss: 0.30189 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 168 | TrainLoss: 0.03619 | TestLoss: 0.31963 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 169 | TrainLoss: 0.03560 | TestLoss: 0.29693 | TestAcc: 0.90498 | TestF1: 0.91\n",
      "Epoch: 170 | TrainLoss: 0.02474 | TestLoss: 0.30923 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 171 | TrainLoss: 0.03999 | TestLoss: 0.30201 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 172 | TrainLoss: 0.02969 | TestLoss: 0.30093 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 173 | TrainLoss: 0.02508 | TestLoss: 0.30667 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 174 | TrainLoss: 0.02368 | TestLoss: 0.30291 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 175 | TrainLoss: 0.02184 | TestLoss: 0.30501 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 176 | TrainLoss: 0.02750 | TestLoss: 0.30666 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 177 | TrainLoss: 0.02381 | TestLoss: 0.31112 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 178 | TrainLoss: 0.02273 | TestLoss: 0.30681 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 179 | TrainLoss: 0.02522 | TestLoss: 0.31421 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 180 | TrainLoss: 0.02601 | TestLoss: 0.30939 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 181 | TrainLoss: 0.02513 | TestLoss: 0.32326 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 182 | TrainLoss: 0.02850 | TestLoss: 0.32499 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 183 | TrainLoss: 0.02743 | TestLoss: 0.31402 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 184 | TrainLoss: 0.02065 | TestLoss: 0.31789 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 185 | TrainLoss: 0.02454 | TestLoss: 0.31436 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 186 | TrainLoss: 0.01694 | TestLoss: 0.31704 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 187 | TrainLoss: 0.01812 | TestLoss: 0.31977 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 188 | TrainLoss: 0.01800 | TestLoss: 0.32622 | TestAcc: 0.89593 | TestF1: 0.89\n",
      "Epoch: 189 | TrainLoss: 0.02031 | TestLoss: 0.33286 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 190 | TrainLoss: 0.02282 | TestLoss: 0.31951 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 191 | TrainLoss: 0.01724 | TestLoss: 0.32530 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 192 | TrainLoss: 0.01562 | TestLoss: 0.32111 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 193 | TrainLoss: 0.01682 | TestLoss: 0.33136 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 194 | TrainLoss: 0.01834 | TestLoss: 0.32132 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 195 | TrainLoss: 0.01335 | TestLoss: 0.32744 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 196 | TrainLoss: 0.01605 | TestLoss: 0.32374 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 197 | TrainLoss: 0.01463 | TestLoss: 0.32757 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 198 | TrainLoss: 0.02210 | TestLoss: 0.37689 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 199 | TrainLoss: 0.02778 | TestLoss: 0.33147 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 200 | TrainLoss: 0.01365 | TestLoss: 0.36867 | TestAcc: 0.87783 | TestF1: 0.89\n",
      "Epoch: 201 | TrainLoss: 0.02011 | TestLoss: 0.33554 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 202 | TrainLoss: 0.01533 | TestLoss: 0.35050 | TestAcc: 0.89593 | TestF1: 0.89\n",
      "Epoch: 203 | TrainLoss: 0.01558 | TestLoss: 0.33617 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 204 | TrainLoss: 0.01339 | TestLoss: 0.34483 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 205 | TrainLoss: 0.01305 | TestLoss: 0.33319 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 206 | TrainLoss: 0.01227 | TestLoss: 0.34212 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 207 | TrainLoss: 0.01427 | TestLoss: 0.33480 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 208 | TrainLoss: 0.01148 | TestLoss: 0.34942 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 209 | TrainLoss: 0.01667 | TestLoss: 0.34214 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 210 | TrainLoss: 0.00959 | TestLoss: 0.36210 | TestAcc: 0.89593 | TestF1: 0.89\n",
      "Epoch: 211 | TrainLoss: 0.01369 | TestLoss: 0.33744 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 212 | TrainLoss: 0.00897 | TestLoss: 0.35308 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 213 | TrainLoss: 0.01187 | TestLoss: 0.33950 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 214 | TrainLoss: 0.00950 | TestLoss: 0.35911 | TestAcc: 0.89593 | TestF1: 0.89\n",
      "Epoch: 215 | TrainLoss: 0.01013 | TestLoss: 0.34925 | TestAcc: 0.89593 | TestF1: 0.89\n",
      "Epoch: 216 | TrainLoss: 0.00979 | TestLoss: 0.35036 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 217 | TrainLoss: 0.00937 | TestLoss: 0.35690 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 218 | TrainLoss: 0.01245 | TestLoss: 0.35515 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 219 | TrainLoss: 0.00964 | TestLoss: 0.37170 | TestAcc: 0.89593 | TestF1: 0.89\n",
      "Epoch: 220 | TrainLoss: 0.01127 | TestLoss: 0.34697 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 221 | TrainLoss: 0.00714 | TestLoss: 0.36310 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 222 | TrainLoss: 0.01271 | TestLoss: 0.34992 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 223 | TrainLoss: 0.00882 | TestLoss: 0.36180 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 224 | TrainLoss: 0.01211 | TestLoss: 0.35377 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 225 | TrainLoss: 0.00737 | TestLoss: 0.36182 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 226 | TrainLoss: 0.01201 | TestLoss: 0.35590 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 227 | TrainLoss: 0.00785 | TestLoss: 0.38521 | TestAcc: 0.88688 | TestF1: 0.88\n",
      "Epoch: 228 | TrainLoss: 0.01252 | TestLoss: 0.35535 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 229 | TrainLoss: 0.00887 | TestLoss: 0.38257 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 230 | TrainLoss: 0.01230 | TestLoss: 0.35657 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 231 | TrainLoss: 0.00648 | TestLoss: 0.38009 | TestAcc: 0.89593 | TestF1: 0.89\n",
      "Epoch: 232 | TrainLoss: 0.01378 | TestLoss: 0.35855 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 233 | TrainLoss: 0.00670 | TestLoss: 0.37972 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 234 | TrainLoss: 0.01048 | TestLoss: 0.35986 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 235 | TrainLoss: 0.00712 | TestLoss: 0.39143 | TestAcc: 0.88688 | TestF1: 0.88\n",
      "Epoch: 236 | TrainLoss: 0.00916 | TestLoss: 0.37927 | TestAcc: 0.89593 | TestF1: 0.89\n",
      "Epoch: 237 | TrainLoss: 0.00842 | TestLoss: 0.36933 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 238 | TrainLoss: 0.00625 | TestLoss: 0.39621 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 239 | TrainLoss: 0.01123 | TestLoss: 0.36413 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 240 | TrainLoss: 0.00719 | TestLoss: 0.39762 | TestAcc: 0.88688 | TestF1: 0.88\n",
      "Epoch: 241 | TrainLoss: 0.01099 | TestLoss: 0.37412 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 242 | TrainLoss: 0.00581 | TestLoss: 0.36897 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 243 | TrainLoss: 0.00530 | TestLoss: 0.38258 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 244 | TrainLoss: 0.00962 | TestLoss: 0.37008 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 245 | TrainLoss: 0.00713 | TestLoss: 0.38170 | TestAcc: 0.89593 | TestF1: 0.89\n",
      "Epoch: 246 | TrainLoss: 0.01190 | TestLoss: 0.36897 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 247 | TrainLoss: 0.00444 | TestLoss: 0.38167 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 248 | TrainLoss: 0.00515 | TestLoss: 0.37769 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 249 | TrainLoss: 0.00735 | TestLoss: 0.37694 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 250 | TrainLoss: 0.00688 | TestLoss: 0.38128 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 251 | TrainLoss: 0.00672 | TestLoss: 0.37274 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 252 | TrainLoss: 0.00458 | TestLoss: 0.38326 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 253 | TrainLoss: 0.00575 | TestLoss: 0.37641 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 254 | TrainLoss: 0.00443 | TestLoss: 0.37713 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 255 | TrainLoss: 0.00569 | TestLoss: 0.37842 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 256 | TrainLoss: 0.00473 | TestLoss: 0.37728 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 257 | TrainLoss: 0.00438 | TestLoss: 0.37719 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 258 | TrainLoss: 0.00477 | TestLoss: 0.38075 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 259 | TrainLoss: 0.00437 | TestLoss: 0.38055 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 260 | TrainLoss: 0.00448 | TestLoss: 0.38015 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 261 | TrainLoss: 0.00572 | TestLoss: 0.38037 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 262 | TrainLoss: 0.00412 | TestLoss: 0.38011 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 263 | TrainLoss: 0.00384 | TestLoss: 0.38156 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 264 | TrainLoss: 0.00408 | TestLoss: 0.38134 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 265 | TrainLoss: 0.00459 | TestLoss: 0.38208 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 266 | TrainLoss: 0.00417 | TestLoss: 0.38276 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 267 | TrainLoss: 0.00360 | TestLoss: 0.38368 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 268 | TrainLoss: 0.00453 | TestLoss: 0.38422 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 269 | TrainLoss: 0.00299 | TestLoss: 0.38572 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 270 | TrainLoss: 0.00392 | TestLoss: 0.38524 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 271 | TrainLoss: 0.00335 | TestLoss: 0.38864 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 272 | TrainLoss: 0.00325 | TestLoss: 0.38941 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 273 | TrainLoss: 0.00419 | TestLoss: 0.38710 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 274 | TrainLoss: 0.00356 | TestLoss: 0.38875 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 275 | TrainLoss: 0.00322 | TestLoss: 0.38896 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 276 | TrainLoss: 0.00322 | TestLoss: 0.38893 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 277 | TrainLoss: 0.00321 | TestLoss: 0.39115 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 278 | TrainLoss: 0.00320 | TestLoss: 0.39097 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 279 | TrainLoss: 0.00295 | TestLoss: 0.39068 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 280 | TrainLoss: 0.00272 | TestLoss: 0.39144 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 281 | TrainLoss: 0.00382 | TestLoss: 0.39213 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 282 | TrainLoss: 0.00310 | TestLoss: 0.39351 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 283 | TrainLoss: 0.00223 | TestLoss: 0.39428 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 284 | TrainLoss: 0.00353 | TestLoss: 0.39325 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 285 | TrainLoss: 0.00243 | TestLoss: 0.39459 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 286 | TrainLoss: 0.00343 | TestLoss: 0.39425 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 287 | TrainLoss: 0.00325 | TestLoss: 0.39487 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 288 | TrainLoss: 0.00259 | TestLoss: 0.39622 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 289 | TrainLoss: 0.00276 | TestLoss: 0.39628 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 290 | TrainLoss: 0.00360 | TestLoss: 0.39723 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 291 | TrainLoss: 0.00350 | TestLoss: 0.40252 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 292 | TrainLoss: 0.00300 | TestLoss: 0.40043 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 293 | TrainLoss: 0.00284 | TestLoss: 0.39795 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 294 | TrainLoss: 0.00354 | TestLoss: 0.40405 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 295 | TrainLoss: 0.00268 | TestLoss: 0.40962 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 296 | TrainLoss: 0.00252 | TestLoss: 0.40395 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 297 | TrainLoss: 0.00247 | TestLoss: 0.39994 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 298 | TrainLoss: 0.00352 | TestLoss: 0.40190 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 299 | TrainLoss: 0.00281 | TestLoss: 0.40081 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 300 | TrainLoss: 0.00293 | TestLoss: 0.40310 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 301 | TrainLoss: 0.00274 | TestLoss: 0.40547 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 302 | TrainLoss: 0.00212 | TestLoss: 0.40380 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 303 | TrainLoss: 0.00254 | TestLoss: 0.40238 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 304 | TrainLoss: 0.00337 | TestLoss: 0.40279 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 305 | TrainLoss: 0.00221 | TestLoss: 0.40334 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 306 | TrainLoss: 0.00198 | TestLoss: 0.40371 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 307 | TrainLoss: 0.00243 | TestLoss: 0.40441 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 308 | TrainLoss: 0.00227 | TestLoss: 0.40483 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 309 | TrainLoss: 0.00196 | TestLoss: 0.40545 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 310 | TrainLoss: 0.00210 | TestLoss: 0.40574 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 311 | TrainLoss: 0.00217 | TestLoss: 0.40623 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 312 | TrainLoss: 0.00233 | TestLoss: 0.40668 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 313 | TrainLoss: 0.00209 | TestLoss: 0.40764 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 314 | TrainLoss: 0.00216 | TestLoss: 0.40862 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 315 | TrainLoss: 0.00231 | TestLoss: 0.40812 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 316 | TrainLoss: 0.00241 | TestLoss: 0.40898 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 317 | TrainLoss: 0.00247 | TestLoss: 0.41117 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 318 | TrainLoss: 0.00196 | TestLoss: 0.41034 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 319 | TrainLoss: 0.00247 | TestLoss: 0.40950 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 320 | TrainLoss: 0.00277 | TestLoss: 0.40965 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 321 | TrainLoss: 0.00180 | TestLoss: 0.41009 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 322 | TrainLoss: 0.00180 | TestLoss: 0.41123 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 323 | TrainLoss: 0.00182 | TestLoss: 0.41155 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 324 | TrainLoss: 0.00208 | TestLoss: 0.41119 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 325 | TrainLoss: 0.00261 | TestLoss: 0.41173 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 326 | TrainLoss: 0.00151 | TestLoss: 0.41258 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 327 | TrainLoss: 0.00195 | TestLoss: 0.41379 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 328 | TrainLoss: 0.00200 | TestLoss: 0.41344 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 329 | TrainLoss: 0.00189 | TestLoss: 0.41405 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 330 | TrainLoss: 0.00209 | TestLoss: 0.41566 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 331 | TrainLoss: 0.00250 | TestLoss: 0.41530 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 332 | TrainLoss: 0.00169 | TestLoss: 0.41605 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 333 | TrainLoss: 0.00213 | TestLoss: 0.41909 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 334 | TrainLoss: 0.00193 | TestLoss: 0.41846 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 335 | TrainLoss: 0.00206 | TestLoss: 0.41750 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 336 | TrainLoss: 0.00213 | TestLoss: 0.42077 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 337 | TrainLoss: 0.00250 | TestLoss: 0.41953 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 338 | TrainLoss: 0.00219 | TestLoss: 0.41887 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 339 | TrainLoss: 0.00193 | TestLoss: 0.42083 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 340 | TrainLoss: 0.00168 | TestLoss: 0.42153 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 341 | TrainLoss: 0.00169 | TestLoss: 0.42008 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 342 | TrainLoss: 0.00186 | TestLoss: 0.42178 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 343 | TrainLoss: 0.00168 | TestLoss: 0.42533 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 344 | TrainLoss: 0.00172 | TestLoss: 0.42422 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 345 | TrainLoss: 0.00170 | TestLoss: 0.42117 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 346 | TrainLoss: 0.00213 | TestLoss: 0.42341 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 347 | TrainLoss: 0.00173 | TestLoss: 0.42465 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 348 | TrainLoss: 0.00153 | TestLoss: 0.42307 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 349 | TrainLoss: 0.00166 | TestLoss: 0.42291 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 350 | TrainLoss: 0.00168 | TestLoss: 0.42383 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 351 | TrainLoss: 0.00185 | TestLoss: 0.42406 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 352 | TrainLoss: 0.00159 | TestLoss: 0.42446 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 353 | TrainLoss: 0.00148 | TestLoss: 0.42546 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 354 | TrainLoss: 0.00181 | TestLoss: 0.42548 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 355 | TrainLoss: 0.00157 | TestLoss: 0.42592 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 356 | TrainLoss: 0.00146 | TestLoss: 0.42662 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 357 | TrainLoss: 0.00178 | TestLoss: 0.42765 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 358 | TrainLoss: 0.00170 | TestLoss: 0.42940 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 359 | TrainLoss: 0.00164 | TestLoss: 0.43000 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 360 | TrainLoss: 0.00143 | TestLoss: 0.42870 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 361 | TrainLoss: 0.00120 | TestLoss: 0.42888 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 362 | TrainLoss: 0.00151 | TestLoss: 0.43068 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 363 | TrainLoss: 0.00174 | TestLoss: 0.43108 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 364 | TrainLoss: 0.00169 | TestLoss: 0.42985 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 365 | TrainLoss: 0.00139 | TestLoss: 0.43273 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 366 | TrainLoss: 0.00179 | TestLoss: 0.43322 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 367 | TrainLoss: 0.00146 | TestLoss: 0.43135 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 368 | TrainLoss: 0.00168 | TestLoss: 0.43117 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 369 | TrainLoss: 0.00129 | TestLoss: 0.43415 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 370 | TrainLoss: 0.00161 | TestLoss: 0.43502 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 371 | TrainLoss: 0.00239 | TestLoss: 0.43345 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 372 | TrainLoss: 0.00180 | TestLoss: 0.43858 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 373 | TrainLoss: 0.00183 | TestLoss: 0.43495 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 374 | TrainLoss: 0.00146 | TestLoss: 0.43293 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 375 | TrainLoss: 0.00143 | TestLoss: 0.43493 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 376 | TrainLoss: 0.00149 | TestLoss: 0.43644 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 377 | TrainLoss: 0.00139 | TestLoss: 0.43523 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 378 | TrainLoss: 0.00149 | TestLoss: 0.43431 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 379 | TrainLoss: 0.00114 | TestLoss: 0.43631 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 380 | TrainLoss: 0.00154 | TestLoss: 0.43600 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 381 | TrainLoss: 0.00179 | TestLoss: 0.43550 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 382 | TrainLoss: 0.00136 | TestLoss: 0.43983 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 383 | TrainLoss: 0.00141 | TestLoss: 0.43986 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 384 | TrainLoss: 0.00117 | TestLoss: 0.43719 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 385 | TrainLoss: 0.00161 | TestLoss: 0.43659 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 386 | TrainLoss: 0.00134 | TestLoss: 0.43855 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 387 | TrainLoss: 0.00151 | TestLoss: 0.43920 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 388 | TrainLoss: 0.00120 | TestLoss: 0.43864 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 389 | TrainLoss: 0.00150 | TestLoss: 0.43886 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 390 | TrainLoss: 0.00155 | TestLoss: 0.43994 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 391 | TrainLoss: 0.00112 | TestLoss: 0.44044 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 392 | TrainLoss: 0.00117 | TestLoss: 0.43999 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 393 | TrainLoss: 0.00116 | TestLoss: 0.44046 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 394 | TrainLoss: 0.00148 | TestLoss: 0.44150 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 395 | TrainLoss: 0.00122 | TestLoss: 0.44234 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 396 | TrainLoss: 0.00127 | TestLoss: 0.44201 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 397 | TrainLoss: 0.00133 | TestLoss: 0.44233 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 398 | TrainLoss: 0.00092 | TestLoss: 0.44240 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 399 | TrainLoss: 0.00091 | TestLoss: 0.44269 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 400 | TrainLoss: 0.00109 | TestLoss: 0.44311 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 401 | TrainLoss: 0.00127 | TestLoss: 0.44340 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 402 | TrainLoss: 0.00097 | TestLoss: 0.44371 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 403 | TrainLoss: 0.00115 | TestLoss: 0.44407 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 404 | TrainLoss: 0.00099 | TestLoss: 0.44446 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 405 | TrainLoss: 0.00107 | TestLoss: 0.44483 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 406 | TrainLoss: 0.00092 | TestLoss: 0.44519 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 407 | TrainLoss: 0.00107 | TestLoss: 0.44533 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 408 | TrainLoss: 0.00095 | TestLoss: 0.44559 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 409 | TrainLoss: 0.00088 | TestLoss: 0.44586 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 410 | TrainLoss: 0.00108 | TestLoss: 0.44628 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 411 | TrainLoss: 0.00089 | TestLoss: 0.44676 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 412 | TrainLoss: 0.00100 | TestLoss: 0.44687 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 413 | TrainLoss: 0.00132 | TestLoss: 0.44745 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 414 | TrainLoss: 0.00115 | TestLoss: 0.44772 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 415 | TrainLoss: 0.00115 | TestLoss: 0.44820 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 416 | TrainLoss: 0.00087 | TestLoss: 0.45020 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 417 | TrainLoss: 0.00089 | TestLoss: 0.45129 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 418 | TrainLoss: 0.00115 | TestLoss: 0.44959 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 419 | TrainLoss: 0.00116 | TestLoss: 0.44960 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 420 | TrainLoss: 0.00101 | TestLoss: 0.45318 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 421 | TrainLoss: 0.00156 | TestLoss: 0.45114 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 422 | TrainLoss: 0.00108 | TestLoss: 0.45134 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 423 | TrainLoss: 0.00102 | TestLoss: 0.45975 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 424 | TrainLoss: 0.00182 | TestLoss: 0.45457 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 425 | TrainLoss: 0.00130 | TestLoss: 0.45323 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 426 | TrainLoss: 0.00080 | TestLoss: 0.46185 | TestAcc: 0.89593 | TestF1: 0.89\n",
      "Epoch: 427 | TrainLoss: 0.00153 | TestLoss: 0.45815 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 428 | TrainLoss: 0.00195 | TestLoss: 0.45645 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 429 | TrainLoss: 0.00128 | TestLoss: 0.47504 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 430 | TrainLoss: 0.00367 | TestLoss: 0.45423 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 431 | TrainLoss: 0.00082 | TestLoss: 0.47069 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 432 | TrainLoss: 0.00138 | TestLoss: 0.48465 | TestAcc: 0.88688 | TestF1: 0.88\n",
      "Epoch: 433 | TrainLoss: 0.00220 | TestLoss: 0.45975 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 434 | TrainLoss: 0.00126 | TestLoss: 0.46215 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 435 | TrainLoss: 0.00090 | TestLoss: 0.48678 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 436 | TrainLoss: 0.00234 | TestLoss: 0.46966 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 437 | TrainLoss: 0.00142 | TestLoss: 0.45667 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 438 | TrainLoss: 0.00109 | TestLoss: 0.46749 | TestAcc: 0.90045 | TestF1: 0.90\n",
      "Epoch: 439 | TrainLoss: 0.00108 | TestLoss: 0.48280 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 440 | TrainLoss: 0.00230 | TestLoss: 0.46084 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 441 | TrainLoss: 0.00072 | TestLoss: 0.46020 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 442 | TrainLoss: 0.00088 | TestLoss: 0.47532 | TestAcc: 0.89140 | TestF1: 0.90\n",
      "Epoch: 443 | TrainLoss: 0.00154 | TestLoss: 0.47155 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 444 | TrainLoss: 0.00117 | TestLoss: 0.46088 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 445 | TrainLoss: 0.00094 | TestLoss: 0.46012 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 446 | TrainLoss: 0.00101 | TestLoss: 0.46609 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 447 | TrainLoss: 0.00111 | TestLoss: 0.46408 | TestAcc: 0.89593 | TestF1: 0.90\n",
      "Epoch: 448 | TrainLoss: 0.00114 | TestLoss: 0.45951 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 449 | TrainLoss: 0.00083 | TestLoss: 0.46074 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 450 | TrainLoss: 0.00084 | TestLoss: 0.46307 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 451 | TrainLoss: 0.00083 | TestLoss: 0.46337 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 452 | TrainLoss: 0.00078 | TestLoss: 0.46195 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 453 | TrainLoss: 0.00073 | TestLoss: 0.46122 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 454 | TrainLoss: 0.00083 | TestLoss: 0.46187 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 455 | TrainLoss: 0.00077 | TestLoss: 0.46244 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 456 | TrainLoss: 0.00073 | TestLoss: 0.46313 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 457 | TrainLoss: 0.00081 | TestLoss: 0.46347 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 458 | TrainLoss: 0.00080 | TestLoss: 0.46362 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 459 | TrainLoss: 0.00079 | TestLoss: 0.46433 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 460 | TrainLoss: 0.00074 | TestLoss: 0.46528 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 461 | TrainLoss: 0.00069 | TestLoss: 0.46591 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 462 | TrainLoss: 0.00063 | TestLoss: 0.46605 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 463 | TrainLoss: 0.00081 | TestLoss: 0.46592 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 464 | TrainLoss: 0.00065 | TestLoss: 0.46672 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 465 | TrainLoss: 0.00101 | TestLoss: 0.46779 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 466 | TrainLoss: 0.00072 | TestLoss: 0.46810 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 467 | TrainLoss: 0.00070 | TestLoss: 0.46792 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 468 | TrainLoss: 0.00076 | TestLoss: 0.46824 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 469 | TrainLoss: 0.00062 | TestLoss: 0.46964 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 470 | TrainLoss: 0.00075 | TestLoss: 0.46985 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 471 | TrainLoss: 0.00075 | TestLoss: 0.46982 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 472 | TrainLoss: 0.00064 | TestLoss: 0.46973 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 473 | TrainLoss: 0.00071 | TestLoss: 0.46999 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 474 | TrainLoss: 0.00056 | TestLoss: 0.47033 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 475 | TrainLoss: 0.00065 | TestLoss: 0.47066 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 476 | TrainLoss: 0.00069 | TestLoss: 0.47099 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 477 | TrainLoss: 0.00070 | TestLoss: 0.47118 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 478 | TrainLoss: 0.00073 | TestLoss: 0.47123 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 479 | TrainLoss: 0.00054 | TestLoss: 0.47202 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 480 | TrainLoss: 0.00061 | TestLoss: 0.47336 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 481 | TrainLoss: 0.00060 | TestLoss: 0.47414 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 482 | TrainLoss: 0.00064 | TestLoss: 0.47361 | TestAcc: 0.88235 | TestF1: 0.89\n",
      "Epoch: 483 | TrainLoss: 0.00053 | TestLoss: 0.47296 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 484 | TrainLoss: 0.00061 | TestLoss: 0.47253 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 485 | TrainLoss: 0.00085 | TestLoss: 0.47291 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 486 | TrainLoss: 0.00061 | TestLoss: 0.47330 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 487 | TrainLoss: 0.00066 | TestLoss: 0.47370 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 488 | TrainLoss: 0.00083 | TestLoss: 0.47347 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 489 | TrainLoss: 0.00061 | TestLoss: 0.47603 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 490 | TrainLoss: 0.00061 | TestLoss: 0.47783 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 491 | TrainLoss: 0.00060 | TestLoss: 0.47727 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 492 | TrainLoss: 0.00086 | TestLoss: 0.47473 | TestAcc: 0.87330 | TestF1: 0.88\n",
      "Epoch: 493 | TrainLoss: 0.00052 | TestLoss: 0.47566 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 494 | TrainLoss: 0.00066 | TestLoss: 0.47797 | TestAcc: 0.89140 | TestF1: 0.89\n",
      "Epoch: 495 | TrainLoss: 0.00074 | TestLoss: 0.47686 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 496 | TrainLoss: 0.00054 | TestLoss: 0.47577 | TestAcc: 0.88235 | TestF1: 0.88\n",
      "Epoch: 497 | TrainLoss: 0.00077 | TestLoss: 0.47623 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 498 | TrainLoss: 0.00061 | TestLoss: 0.47784 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Epoch: 499 | TrainLoss: 0.00070 | TestLoss: 0.47846 | TestAcc: 0.88688 | TestF1: 0.89\n",
      "Best WLoss: 0.00128 | Best Epoch: 127\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(test_data_pol.num_features, [512, 512, 512, 256, 256, 256], 1, 512, 128, 0.001).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "print(device)\n",
    "\n",
    "\n",
    "wloss = []\n",
    "weighted_loss = 0\n",
    "exp_param = 0.8\n",
    "best_test_loss = float('inf')  # Initialize with a large value\n",
    "\n",
    "# Without dropout training results\n",
    "for epoch in range(500):\n",
    "    train_loss = train(epoch)\n",
    "    test_loss, test_acc, test_f1 = test(epoch)\n",
    "    weighted_loss = exp_param * weighted_loss + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
    "  \n",
    "    wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
    "  \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss  # Update the best test loss\n",
    "\n",
    "    print(f'Epoch: {epoch:02d} | TrainLoss: {train_loss:.5f} | '\n",
    "          f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
    "\n",
    "# Print the best values\n",
    "best_wloss = min(wloss)\n",
    "best_epoch = wloss.index(best_wloss)\n",
    "print(f'Best WLoss: {best_wloss:.5f} | Best Epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc7f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
