{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "18387023",
      "metadata": {
        "id": "18387023"
      },
      "source": [
        "# Model-1\n",
        "GraphSage + Bert embeddings using default Parameters and 3 Layer MLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa6d9dc2",
      "metadata": {
        "id": "fa6d9dc2",
        "outputId": "cf2cf4b1-3fd9-48f7-e9aa-7ebab04a3054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
            "Requirement already satisfied: torch-scatter in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (2.1.1)\n",
            "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
            "Requirement already satisfied: torch-sparse in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (0.6.17)\n",
            "Requirement already satisfied: scipy in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from scipy->torch-sparse) (1.21.5)\n",
            "Requirement already satisfied: torch-geometric in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (2.4.0)\n",
            "Requirement already satisfied: numpy in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.21.5)\n",
            "Requirement already satisfied: tqdm in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: jinja2 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: scikit-learn in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: pyparsing in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (5.8.0)\n",
            "Requirement already satisfied: scipy in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: requests in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (2.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (2.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b70ca2b6",
      "metadata": {
        "id": "b70ca2b6"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import UPFD #importing the UPFD Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "272b4593",
      "metadata": {
        "id": "272b4593"
      },
      "outputs": [],
      "source": [
        "#defining the train and test split by defining the feature as bert and setting the name as Gossipcop\n",
        "test_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"bert\",split=\"test\") \n",
        "train_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"bert\", split=\"train\")\n",
        "val_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"bert\", split=\"val\")\n",
        "train_data_gos = train_data_gos + val_data_gos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fab31589",
      "metadata": {
        "id": "fab31589",
        "outputId": "9456aabe-ef2d-43a5-e1b4-6d17c9fd4089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gossipcop Dataset\n",
            "Train Samples:  1638\n",
            "Test Samples:  3826\n"
          ]
        }
      ],
      "source": [
        "print(\"Gossipcop Dataset\")\n",
        "print(\"Train Samples: \", len(train_data_gos))\n",
        "print(\"Test Samples: \", len(test_data_gos))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db557cf0",
      "metadata": {
        "id": "db557cf0",
        "outputId": "64f08b7a-aa0f-4c64-d573-a6a10e40f02a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          1, 70, 74],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
              "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
              "         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
              "         55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "         73, 74, 75]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data_gos[0].edge_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5baf8a6",
      "metadata": {
        "id": "b5baf8a6"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "train_loader = DataLoader(train_data_gos, batch_size=256, shuffle=True)\n",
        "test_loader = DataLoader(test_data_gos, batch_size=256, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5707e0c6",
      "metadata": {
        "id": "5707e0c6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import LeakyReLU, Softmax, Linear, SELU,Dropout\n",
        "from torch_geometric.nn import SAGEConv, global_max_pool, GATv2Conv, TopKPooling, global_mean_pool\n",
        "from torch_geometric.transforms import ToUndirected\n",
        "from torch.nn import LeakyReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47fa68ed",
      "metadata": {
        "id": "47fa68ed"
      },
      "outputs": [],
      "source": [
        "#defining the GraphSage Model with 3 SageConv layers and 3 unit MLP\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
        "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
        "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
        "        \n",
        "        self.full1 = Linear(hidden_channels[2],hidden_channels[3])\n",
        "        self.full2 = Linear(hidden_channels[3],hidden_channels[4])\n",
        "        self.full3 = Linear(hidden_channels[4],hidden_channels[5])\n",
        "\n",
        "        self.softmax = Linear(hidden_channels[5],out_channels)\n",
        "\n",
        "        #droupouts\n",
        "        self.dp1 = Dropout(0.2)\n",
        "        self.dp2 = Dropout(0.2)\n",
        "        self.dp3 = Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        h = self.conv1(x, edge_index).relu()\n",
        "        h = self.conv2(h, edge_index).relu()\n",
        "        h = self.conv3(h, edge_index).relu()\n",
        "\n",
        "        h = global_max_pool(h,batch)\n",
        "\n",
        "        h = self.full1(h).relu()\n",
        "        h = self.dp1(h)\n",
        "        h = self.full2(h).relu()\n",
        "        h = self.dp2(h)\n",
        "        h = self.full3(h).relu()\n",
        "        h = self.dp3(h)\n",
        "        \n",
        "        h = self.softmax(h)\n",
        "\n",
        "        return torch.sigmoid(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bed69539",
      "metadata": {
        "id": "bed69539"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import accuracy_score, f1_score "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "804f4b17",
      "metadata": {
        "id": "804f4b17",
        "outputId": "b8e91f2c-fe27-4b73-afee-011abd1b4743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "#specifying number of input features, hidden layer sizes, and number of output channels\n",
        "model = Net(test_data_gos.num_features,[512,512,512,256,256,256],1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) #setting optimiser and learning rate as defined by the paper \n",
        "lossff = torch.nn.BCELoss()\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b902c53",
      "metadata": {
        "id": "1b902c53"
      },
      "outputs": [],
      "source": [
        "#defining the train and test function for the model\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        all_preds.append(torch.reshape(out, (-1,)))\n",
        "        all_labels.append(data.y.float())\n",
        "    # print(all_preds)\n",
        "    accuracy, f1 = metrics(all_preds, all_labels)\n",
        "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
        "\n",
        "\n",
        "def metrics(preds, gts):\n",
        "    preds = torch.round(torch.cat(preds))\n",
        "    gts = torch.cat(gts)\n",
        "    # print(preds.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    return acc, f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "461c1f4e",
      "metadata": {
        "id": "461c1f4e",
        "outputId": "a16ec7b0-2392-414b-c9b5-603f6a123c1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 00 |  TrainLoss: 0.69271 | TestLoss: 0.69131 | TestAcc: 0.82593 | TestF1: 0.84\n",
            "Epoch: 01 |  TrainLoss: 0.69028 | TestLoss: 0.68640 | TestAcc: 0.87689 | TestF1: 0.88\n",
            "Epoch: 02 |  TrainLoss: 0.68323 | TestLoss: 0.67375 | TestAcc: 0.88082 | TestF1: 0.89\n",
            "Epoch: 03 |  TrainLoss: 0.66677 | TestLoss: 0.64455 | TestAcc: 0.89963 | TestF1: 0.91\n",
            "Epoch: 04 |  TrainLoss: 0.63063 | TestLoss: 0.58553 | TestAcc: 0.90669 | TestF1: 0.91\n",
            "Epoch: 05 |  TrainLoss: 0.55566 | TestLoss: 0.46683 | TestAcc: 0.93152 | TestF1: 0.93\n",
            "Epoch: 06 |  TrainLoss: 0.42451 | TestLoss: 0.30236 | TestAcc: 0.94119 | TestF1: 0.94\n",
            "Epoch: 07 |  TrainLoss: 0.26788 | TestLoss: 0.19082 | TestAcc: 0.94668 | TestF1: 0.95\n",
            "Epoch: 08 |  TrainLoss: 0.18987 | TestLoss: 0.17248 | TestAcc: 0.94590 | TestF1: 0.95\n",
            "Epoch: 09 |  TrainLoss: 0.17958 | TestLoss: 0.19811 | TestAcc: 0.94773 | TestF1: 0.95\n",
            "Epoch: 10 |  TrainLoss: 0.18190 | TestLoss: 0.16731 | TestAcc: 0.95191 | TestF1: 0.95\n",
            "Epoch: 11 |  TrainLoss: 0.16443 | TestLoss: 0.15976 | TestAcc: 0.95191 | TestF1: 0.95\n",
            "Epoch: 12 |  TrainLoss: 0.16000 | TestLoss: 0.15249 | TestAcc: 0.95217 | TestF1: 0.95\n",
            "Epoch: 13 |  TrainLoss: 0.14162 | TestLoss: 0.15397 | TestAcc: 0.94825 | TestF1: 0.95\n",
            "Epoch: 14 |  TrainLoss: 0.14150 | TestLoss: 0.14597 | TestAcc: 0.95243 | TestF1: 0.95\n",
            "Epoch: 15 |  TrainLoss: 0.13222 | TestLoss: 0.14815 | TestAcc: 0.95426 | TestF1: 0.96\n",
            "Epoch: 16 |  TrainLoss: 0.13228 | TestLoss: 0.14251 | TestAcc: 0.95531 | TestF1: 0.96\n",
            "Epoch: 17 |  TrainLoss: 0.12429 | TestLoss: 0.13953 | TestAcc: 0.95400 | TestF1: 0.95\n",
            "Epoch: 18 |  TrainLoss: 0.11593 | TestLoss: 0.14180 | TestAcc: 0.95635 | TestF1: 0.96\n",
            "Epoch: 19 |  TrainLoss: 0.11857 | TestLoss: 0.13586 | TestAcc: 0.95661 | TestF1: 0.96\n",
            "Epoch: 20 |  TrainLoss: 0.10716 | TestLoss: 0.13751 | TestAcc: 0.95740 | TestF1: 0.96\n",
            "Epoch: 21 |  TrainLoss: 0.09674 | TestLoss: 0.13532 | TestAcc: 0.95766 | TestF1: 0.96\n",
            "Epoch: 22 |  TrainLoss: 0.10067 | TestLoss: 0.13252 | TestAcc: 0.95609 | TestF1: 0.96\n",
            "Epoch: 23 |  TrainLoss: 0.09477 | TestLoss: 0.13883 | TestAcc: 0.95165 | TestF1: 0.95\n",
            "Epoch: 24 |  TrainLoss: 0.09414 | TestLoss: 0.13227 | TestAcc: 0.95504 | TestF1: 0.95\n",
            "Epoch: 25 |  TrainLoss: 0.09011 | TestLoss: 0.12997 | TestAcc: 0.95635 | TestF1: 0.96\n",
            "Epoch: 26 |  TrainLoss: 0.07855 | TestLoss: 0.14724 | TestAcc: 0.94773 | TestF1: 0.95\n",
            "Epoch: 27 |  TrainLoss: 0.08761 | TestLoss: 0.13221 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 28 |  TrainLoss: 0.08979 | TestLoss: 0.14210 | TestAcc: 0.95661 | TestF1: 0.96\n",
            "Epoch: 29 |  TrainLoss: 0.07752 | TestLoss: 0.14114 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 30 |  TrainLoss: 0.07201 | TestLoss: 0.13576 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 31 |  TrainLoss: 0.06718 | TestLoss: 0.13185 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 32 |  TrainLoss: 0.05915 | TestLoss: 0.13213 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 33 |  TrainLoss: 0.05644 | TestLoss: 0.13604 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 34 |  TrainLoss: 0.06170 | TestLoss: 0.17845 | TestAcc: 0.94694 | TestF1: 0.95\n",
            "Epoch: 35 |  TrainLoss: 0.06550 | TestLoss: 0.15482 | TestAcc: 0.95400 | TestF1: 0.96\n",
            "Epoch: 36 |  TrainLoss: 0.06227 | TestLoss: 0.13987 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 37 |  TrainLoss: 0.05732 | TestLoss: 0.14915 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 38 |  TrainLoss: 0.05612 | TestLoss: 0.12835 | TestAcc: 0.96158 | TestF1: 0.96\n",
            "Epoch: 39 |  TrainLoss: 0.05931 | TestLoss: 0.12831 | TestAcc: 0.96158 | TestF1: 0.96\n",
            "Epoch: 40 |  TrainLoss: 0.04923 | TestLoss: 0.16384 | TestAcc: 0.94825 | TestF1: 0.95\n",
            "Epoch: 41 |  TrainLoss: 0.04702 | TestLoss: 0.12983 | TestAcc: 0.96027 | TestF1: 0.96\n",
            "Epoch: 42 |  TrainLoss: 0.04098 | TestLoss: 0.13446 | TestAcc: 0.96184 | TestF1: 0.96\n",
            "Epoch: 43 |  TrainLoss: 0.03765 | TestLoss: 0.13255 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 44 |  TrainLoss: 0.03476 | TestLoss: 0.13881 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 45 |  TrainLoss: 0.03561 | TestLoss: 0.13527 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 46 |  TrainLoss: 0.03138 | TestLoss: 0.14345 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 47 |  TrainLoss: 0.03379 | TestLoss: 0.14121 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 48 |  TrainLoss: 0.02904 | TestLoss: 0.14897 | TestAcc: 0.95687 | TestF1: 0.96\n",
            "Epoch: 49 |  TrainLoss: 0.02765 | TestLoss: 0.15783 | TestAcc: 0.95321 | TestF1: 0.95\n",
            "Epoch: 50 |  TrainLoss: 0.02877 | TestLoss: 0.16810 | TestAcc: 0.94956 | TestF1: 0.95\n",
            "Epoch: 51 |  TrainLoss: 0.02790 | TestLoss: 0.15618 | TestAcc: 0.95504 | TestF1: 0.95\n",
            "Epoch: 52 |  TrainLoss: 0.01998 | TestLoss: 0.16996 | TestAcc: 0.95452 | TestF1: 0.96\n",
            "Epoch: 53 |  TrainLoss: 0.02223 | TestLoss: 0.15913 | TestAcc: 0.95766 | TestF1: 0.96\n",
            "Epoch: 54 |  TrainLoss: 0.01930 | TestLoss: 0.16042 | TestAcc: 0.95714 | TestF1: 0.96\n",
            "Epoch: 55 |  TrainLoss: 0.02529 | TestLoss: 0.16757 | TestAcc: 0.95400 | TestF1: 0.95\n",
            "Epoch: 56 |  TrainLoss: 0.01678 | TestLoss: 0.16269 | TestAcc: 0.95635 | TestF1: 0.96\n",
            "Epoch: 57 |  TrainLoss: 0.01428 | TestLoss: 0.15836 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 58 |  TrainLoss: 0.01222 | TestLoss: 0.18080 | TestAcc: 0.95504 | TestF1: 0.96\n",
            "Epoch: 59 |  TrainLoss: 0.01433 | TestLoss: 0.16254 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 60 |  TrainLoss: 0.00860 | TestLoss: 0.16553 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 61 |  TrainLoss: 0.00738 | TestLoss: 0.16622 | TestAcc: 0.95687 | TestF1: 0.96\n",
            "Epoch: 62 |  TrainLoss: 0.00677 | TestLoss: 0.18132 | TestAcc: 0.95478 | TestF1: 0.95\n",
            "Epoch: 63 |  TrainLoss: 0.00686 | TestLoss: 0.16580 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 64 |  TrainLoss: 0.00600 | TestLoss: 0.16870 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 65 |  TrainLoss: 0.00675 | TestLoss: 0.18196 | TestAcc: 0.95635 | TestF1: 0.96\n",
            "Epoch: 66 |  TrainLoss: 0.00415 | TestLoss: 0.17586 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 67 |  TrainLoss: 0.00621 | TestLoss: 0.18341 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 68 |  TrainLoss: 0.00584 | TestLoss: 0.23994 | TestAcc: 0.94642 | TestF1: 0.94\n",
            "Epoch: 69 |  TrainLoss: 0.00763 | TestLoss: 0.19284 | TestAcc: 0.95531 | TestF1: 0.95\n",
            "Epoch: 70 |  TrainLoss: 0.00279 | TestLoss: 0.18539 | TestAcc: 0.96027 | TestF1: 0.96\n",
            "Epoch: 71 |  TrainLoss: 0.00259 | TestLoss: 0.18901 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 72 |  TrainLoss: 0.00123 | TestLoss: 0.18770 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 73 |  TrainLoss: 0.00122 | TestLoss: 0.18988 | TestAcc: 0.96027 | TestF1: 0.96\n",
            "Epoch: 74 |  TrainLoss: 0.00094 | TestLoss: 0.19219 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 75 |  TrainLoss: 0.00103 | TestLoss: 0.19510 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 76 |  TrainLoss: 0.00081 | TestLoss: 0.19784 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 77 |  TrainLoss: 0.00091 | TestLoss: 0.19873 | TestAcc: 0.95975 | TestF1: 0.96\n",
            "Epoch: 78 |  TrainLoss: 0.00067 | TestLoss: 0.20007 | TestAcc: 0.95975 | TestF1: 0.96\n",
            "Epoch: 79 |  TrainLoss: 0.00061 | TestLoss: 0.20246 | TestAcc: 0.96053 | TestF1: 0.96\n",
            "Epoch: 80 |  TrainLoss: 0.00070 | TestLoss: 0.20385 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 81 |  TrainLoss: 0.00064 | TestLoss: 0.20790 | TestAcc: 0.96053 | TestF1: 0.96\n",
            "Epoch: 82 |  TrainLoss: 0.00056 | TestLoss: 0.20750 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 83 |  TrainLoss: 0.00051 | TestLoss: 0.20899 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 84 |  TrainLoss: 0.00061 | TestLoss: 0.21085 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 85 |  TrainLoss: 0.00044 | TestLoss: 0.21209 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 86 |  TrainLoss: 0.00048 | TestLoss: 0.21611 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 87 |  TrainLoss: 0.00053 | TestLoss: 0.21516 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 88 |  TrainLoss: 0.00048 | TestLoss: 0.21834 | TestAcc: 0.96027 | TestF1: 0.96\n",
            "Epoch: 89 |  TrainLoss: 0.00059 | TestLoss: 0.21819 | TestAcc: 0.96079 | TestF1: 0.96\n",
            "Epoch: 90 |  TrainLoss: 0.00038 | TestLoss: 0.21855 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 91 |  TrainLoss: 0.00036 | TestLoss: 0.21941 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 92 |  TrainLoss: 0.00029 | TestLoss: 0.22158 | TestAcc: 0.96027 | TestF1: 0.96\n",
            "Epoch: 93 |  TrainLoss: 0.00036 | TestLoss: 0.22192 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 94 |  TrainLoss: 0.00033 | TestLoss: 0.22333 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 95 |  TrainLoss: 0.00031 | TestLoss: 0.22573 | TestAcc: 0.96079 | TestF1: 0.96\n",
            "Epoch: 96 |  TrainLoss: 0.00044 | TestLoss: 0.22548 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 97 |  TrainLoss: 0.00026 | TestLoss: 0.22870 | TestAcc: 0.95975 | TestF1: 0.96\n",
            "Epoch: 98 |  TrainLoss: 0.00024 | TestLoss: 0.22837 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 99 |  TrainLoss: 0.00032 | TestLoss: 0.23060 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Best WLoss: 0.00004 | Best Epoch: 33\n"
          ]
        }
      ],
      "source": [
        "#printing out the epoch at lowest wloss. \n",
        "wloss = []\n",
        "weighted_loss = 0\n",
        "exp_param = 0.8\n",
        "best_test_loss = float('inf')  #initialize with a large value\n",
        "\n",
        "#training results \n",
        "for epoch in range(100): #setting epoch at 100\n",
        "  train_loss = train(epoch)\n",
        "  test_loss, test_acc, test_f1 = test(epoch)\n",
        "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
        "  \n",
        "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
        "  \n",
        "  if test_loss < best_test_loss:\n",
        "    best_test_loss = test_loss  #updating the best test loss\n",
        "\n",
        "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
        "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
        "\n",
        "#printing the best values\n",
        "best_wloss = min(wloss)\n",
        "best_epoch = wloss.index(best_wloss)\n",
        "print(f'Best WLoss: {best_wloss:.5f} | Best Epoch: {best_epoch}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fffc015",
      "metadata": {
        "id": "5fffc015"
      },
      "source": [
        "# Model 2: \n",
        "GraphSage + Bert Embeddings with hyperparameters as defined by the paper and 3 Layer MLP\n",
        "\n",
        "(Embedding size = 128, batch size= 128, l2 Regularization = 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6971a3f",
      "metadata": {
        "id": "b6971a3f"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, embedding_size, batch_size, l2_reg_weight): #adding in embedding size, batch size and l2 regularisation parameters \n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.batch_size = batch_size\n",
        "        self.l2_reg_weight = l2_reg_weight\n",
        "        \n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
        "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
        "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
        "        \n",
        "        self.full1 = Linear(hidden_channels[2], hidden_channels[3])\n",
        "        self.full2 = Linear(hidden_channels[3], hidden_channels[4])\n",
        "        self.full3 = Linear(hidden_channels[4], hidden_channels[5])\n",
        "\n",
        "        self.softmax = Linear(hidden_channels[5], out_channels)\n",
        "\n",
        "        # Dropouts\n",
        "        self.dp1 = Dropout(0.2)\n",
        "        self.dp2 = Dropout(0.2)\n",
        "        self.dp3 = Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        h = self.conv1(x, edge_index).relu()\n",
        "        h = self.conv2(h, edge_index).relu()\n",
        "        h = self.conv3(h, edge_index).relu()\n",
        "\n",
        "        h = global_max_pool(h, batch)\n",
        "\n",
        "        h = self.full1(h).relu()\n",
        "        h = self.dp1(h)\n",
        "        h = self.full2(h).relu()\n",
        "        h = self.dp2(h)\n",
        "        h = self.full3(h).relu()\n",
        "        h = self.dp3(h)\n",
        "        \n",
        "        h = self.softmax(h)\n",
        "\n",
        "        return torch.sigmoid(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f40d635",
      "metadata": {
        "id": "3f40d635",
        "outputId": "45b38261-d749-4046-9836-58a377e3d7e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net(test_data_gos.num_features,[512,512,512,256,256,256],1, 128, 128, 0.001).to(device) #setting embedding size=128, batch size=128 and l2 regularization weight as 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "lossff = torch.nn.BCELoss() #using Binary Cross Entropy loss function \n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5543ef89",
      "metadata": {
        "id": "5543ef89"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        all_preds.append(torch.reshape(out, (-1,)))\n",
        "        all_labels.append(data.y.float())\n",
        "    # print(all_preds)\n",
        "    accuracy, f1 = metrics(all_preds, all_labels)\n",
        "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
        "\n",
        "\n",
        "def metrics(preds, gts):\n",
        "    preds = torch.round(torch.cat(preds))\n",
        "    gts = torch.cat(gts)\n",
        "    # print(preds.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    return acc, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01bde71a",
      "metadata": {
        "id": "01bde71a",
        "outputId": "82d1518f-3d69-4520-e6f9-0c9965b2814a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 00 |  TrainLoss: 0.69249 | TestLoss: 0.69143 | TestAcc: 0.71720 | TestF1: 0.78\n",
            "Epoch: 01 |  TrainLoss: 0.69042 | TestLoss: 0.68718 | TestAcc: 0.88134 | TestF1: 0.88\n",
            "Epoch: 02 |  TrainLoss: 0.68520 | TestLoss: 0.67789 | TestAcc: 0.87167 | TestF1: 0.88\n",
            "Epoch: 03 |  TrainLoss: 0.67251 | TestLoss: 0.65598 | TestAcc: 0.89493 | TestF1: 0.89\n",
            "Epoch: 04 |  TrainLoss: 0.64574 | TestLoss: 0.60772 | TestAcc: 0.90042 | TestF1: 0.90\n",
            "Epoch: 05 |  TrainLoss: 0.58485 | TestLoss: 0.50331 | TestAcc: 0.91009 | TestF1: 0.91\n",
            "Epoch: 06 |  TrainLoss: 0.46447 | TestLoss: 0.35867 | TestAcc: 0.90408 | TestF1: 0.90\n",
            "Epoch: 07 |  TrainLoss: 0.30784 | TestLoss: 0.22962 | TestAcc: 0.92185 | TestF1: 0.92\n",
            "Epoch: 08 |  TrainLoss: 0.22559 | TestLoss: 0.18448 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 09 |  TrainLoss: 0.19111 | TestLoss: 0.17126 | TestAcc: 0.94198 | TestF1: 0.94\n",
            "Epoch: 10 |  TrainLoss: 0.17946 | TestLoss: 0.16538 | TestAcc: 0.94485 | TestF1: 0.95\n",
            "Epoch: 11 |  TrainLoss: 0.15980 | TestLoss: 0.16261 | TestAcc: 0.94198 | TestF1: 0.94\n",
            "Epoch: 12 |  TrainLoss: 0.16478 | TestLoss: 0.15863 | TestAcc: 0.94198 | TestF1: 0.94\n",
            "Epoch: 13 |  TrainLoss: 0.15264 | TestLoss: 0.15070 | TestAcc: 0.95243 | TestF1: 0.95\n",
            "Epoch: 14 |  TrainLoss: 0.14487 | TestLoss: 0.14902 | TestAcc: 0.94825 | TestF1: 0.95\n",
            "Epoch: 15 |  TrainLoss: 0.13725 | TestLoss: 0.14553 | TestAcc: 0.95269 | TestF1: 0.95\n",
            "Epoch: 16 |  TrainLoss: 0.13032 | TestLoss: 0.14329 | TestAcc: 0.95269 | TestF1: 0.95\n",
            "Epoch: 17 |  TrainLoss: 0.13259 | TestLoss: 0.14070 | TestAcc: 0.95295 | TestF1: 0.95\n",
            "Epoch: 18 |  TrainLoss: 0.12457 | TestLoss: 0.14318 | TestAcc: 0.94903 | TestF1: 0.95\n",
            "Epoch: 19 |  TrainLoss: 0.11744 | TestLoss: 0.13888 | TestAcc: 0.95086 | TestF1: 0.95\n",
            "Epoch: 20 |  TrainLoss: 0.11106 | TestLoss: 0.14295 | TestAcc: 0.94903 | TestF1: 0.95\n",
            "Epoch: 21 |  TrainLoss: 0.11216 | TestLoss: 0.13594 | TestAcc: 0.95504 | TestF1: 0.96\n",
            "Epoch: 22 |  TrainLoss: 0.10167 | TestLoss: 0.13630 | TestAcc: 0.95112 | TestF1: 0.95\n",
            "Epoch: 23 |  TrainLoss: 0.09802 | TestLoss: 0.13694 | TestAcc: 0.95034 | TestF1: 0.95\n",
            "Epoch: 24 |  TrainLoss: 0.09587 | TestLoss: 0.14653 | TestAcc: 0.94642 | TestF1: 0.95\n",
            "Epoch: 25 |  TrainLoss: 0.09294 | TestLoss: 0.23392 | TestAcc: 0.90643 | TestF1: 0.90\n",
            "Epoch: 26 |  TrainLoss: 0.12677 | TestLoss: 0.15926 | TestAcc: 0.94459 | TestF1: 0.94\n",
            "Epoch: 27 |  TrainLoss: 0.09714 | TestLoss: 0.13876 | TestAcc: 0.95086 | TestF1: 0.95\n",
            "Epoch: 28 |  TrainLoss: 0.09336 | TestLoss: 0.17454 | TestAcc: 0.94877 | TestF1: 0.95\n",
            "Epoch: 29 |  TrainLoss: 0.09631 | TestLoss: 0.12898 | TestAcc: 0.95504 | TestF1: 0.96\n",
            "Epoch: 30 |  TrainLoss: 0.08137 | TestLoss: 0.13546 | TestAcc: 0.95165 | TestF1: 0.95\n",
            "Epoch: 31 |  TrainLoss: 0.07103 | TestLoss: 0.13285 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 32 |  TrainLoss: 0.06863 | TestLoss: 0.13394 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 33 |  TrainLoss: 0.06273 | TestLoss: 0.12867 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 34 |  TrainLoss: 0.05766 | TestLoss: 0.13120 | TestAcc: 0.95400 | TestF1: 0.95\n",
            "Epoch: 35 |  TrainLoss: 0.06313 | TestLoss: 0.13338 | TestAcc: 0.95269 | TestF1: 0.95\n",
            "Epoch: 36 |  TrainLoss: 0.05744 | TestLoss: 0.14176 | TestAcc: 0.95034 | TestF1: 0.95\n",
            "Epoch: 37 |  TrainLoss: 0.05588 | TestLoss: 0.13288 | TestAcc: 0.95452 | TestF1: 0.95\n",
            "Epoch: 38 |  TrainLoss: 0.04901 | TestLoss: 0.13200 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 39 |  TrainLoss: 0.05046 | TestLoss: 0.13134 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 40 |  TrainLoss: 0.04352 | TestLoss: 0.13472 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 41 |  TrainLoss: 0.04316 | TestLoss: 0.13489 | TestAcc: 0.95557 | TestF1: 0.96\n",
            "Epoch: 42 |  TrainLoss: 0.04094 | TestLoss: 0.13784 | TestAcc: 0.95452 | TestF1: 0.95\n",
            "Epoch: 43 |  TrainLoss: 0.04348 | TestLoss: 0.15936 | TestAcc: 0.94773 | TestF1: 0.95\n",
            "Epoch: 44 |  TrainLoss: 0.04142 | TestLoss: 0.15072 | TestAcc: 0.95295 | TestF1: 0.95\n",
            "Epoch: 45 |  TrainLoss: 0.04172 | TestLoss: 0.16486 | TestAcc: 0.94825 | TestF1: 0.95\n",
            "Epoch: 46 |  TrainLoss: 0.04125 | TestLoss: 0.13806 | TestAcc: 0.95714 | TestF1: 0.96\n",
            "Epoch: 47 |  TrainLoss: 0.03976 | TestLoss: 0.14024 | TestAcc: 0.95687 | TestF1: 0.96\n",
            "Epoch: 48 |  TrainLoss: 0.03654 | TestLoss: 0.13650 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 49 |  TrainLoss: 0.02826 | TestLoss: 0.13675 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 50 |  TrainLoss: 0.02309 | TestLoss: 0.14202 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 51 |  TrainLoss: 0.02613 | TestLoss: 0.14180 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 52 |  TrainLoss: 0.02150 | TestLoss: 0.14900 | TestAcc: 0.95687 | TestF1: 0.96\n",
            "Epoch: 53 |  TrainLoss: 0.02016 | TestLoss: 0.14546 | TestAcc: 0.95766 | TestF1: 0.96\n",
            "Epoch: 54 |  TrainLoss: 0.01896 | TestLoss: 0.15173 | TestAcc: 0.95714 | TestF1: 0.96\n",
            "Epoch: 55 |  TrainLoss: 0.01825 | TestLoss: 0.15509 | TestAcc: 0.95609 | TestF1: 0.96\n",
            "Epoch: 56 |  TrainLoss: 0.01502 | TestLoss: 0.15905 | TestAcc: 0.95504 | TestF1: 0.95\n",
            "Epoch: 57 |  TrainLoss: 0.01297 | TestLoss: 0.15959 | TestAcc: 0.95609 | TestF1: 0.96\n",
            "Epoch: 58 |  TrainLoss: 0.01024 | TestLoss: 0.16290 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 59 |  TrainLoss: 0.00889 | TestLoss: 0.16862 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 60 |  TrainLoss: 0.00709 | TestLoss: 0.16782 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 61 |  TrainLoss: 0.00938 | TestLoss: 0.18055 | TestAcc: 0.95557 | TestF1: 0.95\n",
            "Epoch: 62 |  TrainLoss: 0.00465 | TestLoss: 0.17262 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 63 |  TrainLoss: 0.00471 | TestLoss: 0.17484 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 64 |  TrainLoss: 0.00343 | TestLoss: 0.17805 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 65 |  TrainLoss: 0.00224 | TestLoss: 0.18474 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 66 |  TrainLoss: 0.00222 | TestLoss: 0.18368 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 67 |  TrainLoss: 0.00163 | TestLoss: 0.18802 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 68 |  TrainLoss: 0.00128 | TestLoss: 0.19205 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 69 |  TrainLoss: 0.00149 | TestLoss: 0.19611 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 70 |  TrainLoss: 0.00116 | TestLoss: 0.19842 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 71 |  TrainLoss: 0.00098 | TestLoss: 0.19846 | TestAcc: 0.95975 | TestF1: 0.96\n",
            "Epoch: 72 |  TrainLoss: 0.00091 | TestLoss: 0.20216 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 73 |  TrainLoss: 0.00091 | TestLoss: 0.20192 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 74 |  TrainLoss: 0.00063 | TestLoss: 0.20484 | TestAcc: 0.96027 | TestF1: 0.96\n",
            "Epoch: 75 |  TrainLoss: 0.00063 | TestLoss: 0.20672 | TestAcc: 0.95975 | TestF1: 0.96\n",
            "Epoch: 76 |  TrainLoss: 0.00051 | TestLoss: 0.21027 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 77 |  TrainLoss: 0.00059 | TestLoss: 0.21045 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 78 |  TrainLoss: 0.00047 | TestLoss: 0.21250 | TestAcc: 0.96106 | TestF1: 0.96\n",
            "Epoch: 79 |  TrainLoss: 0.00050 | TestLoss: 0.21392 | TestAcc: 0.96106 | TestF1: 0.96\n",
            "Epoch: 80 |  TrainLoss: 0.00049 | TestLoss: 0.21545 | TestAcc: 0.96106 | TestF1: 0.96\n",
            "Epoch: 81 |  TrainLoss: 0.00036 | TestLoss: 0.21785 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 82 |  TrainLoss: 0.00043 | TestLoss: 0.21809 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 83 |  TrainLoss: 0.00049 | TestLoss: 0.21953 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 84 |  TrainLoss: 0.00038 | TestLoss: 0.22150 | TestAcc: 0.96053 | TestF1: 0.96\n",
            "Epoch: 85 |  TrainLoss: 0.00038 | TestLoss: 0.22176 | TestAcc: 0.96106 | TestF1: 0.96\n",
            "Epoch: 86 |  TrainLoss: 0.00025 | TestLoss: 0.22348 | TestAcc: 0.96106 | TestF1: 0.96\n",
            "Epoch: 87 |  TrainLoss: 0.00029 | TestLoss: 0.22435 | TestAcc: 0.96053 | TestF1: 0.96\n",
            "Epoch: 88 |  TrainLoss: 0.00030 | TestLoss: 0.22695 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 89 |  TrainLoss: 0.00027 | TestLoss: 0.22702 | TestAcc: 0.96053 | TestF1: 0.96\n",
            "Epoch: 90 |  TrainLoss: 0.00031 | TestLoss: 0.22790 | TestAcc: 0.96106 | TestF1: 0.96\n",
            "Epoch: 91 |  TrainLoss: 0.00026 | TestLoss: 0.22929 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 92 |  TrainLoss: 0.00027 | TestLoss: 0.23206 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 93 |  TrainLoss: 0.00024 | TestLoss: 0.23123 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 94 |  TrainLoss: 0.00022 | TestLoss: 0.23250 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 95 |  TrainLoss: 0.00019 | TestLoss: 0.23546 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 96 |  TrainLoss: 0.00031 | TestLoss: 0.23480 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 97 |  TrainLoss: 0.00019 | TestLoss: 0.23507 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 98 |  TrainLoss: 0.00015 | TestLoss: 0.23789 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 99 |  TrainLoss: 0.00027 | TestLoss: 0.23727 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Best WLoss: 0.00004 | Best Epoch: 41\n"
          ]
        }
      ],
      "source": [
        "wloss = []\n",
        "weighted_loss = 0\n",
        "exp_param = 0.8\n",
        "best_test_loss = float('inf')  #initialize with a large value\n",
        "\n",
        "#iithout dropout training results\n",
        "for epoch in range(100):\n",
        "  train_loss = train(epoch)\n",
        "  test_loss, test_acc, test_f1 = test(epoch)\n",
        "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
        "  \n",
        "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
        "  \n",
        "  if test_loss < best_test_loss:\n",
        "    best_test_loss = test_loss  #update the best test loss\n",
        "\n",
        "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
        "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
        "\n",
        "#print the best values\n",
        "best_wloss = min(wloss)\n",
        "best_epoch = wloss.index(best_wloss)\n",
        "print(f'Best WLoss: {best_wloss:.5f} | Best Epoch: {best_epoch}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "123c672d",
      "metadata": {
        "id": "123c672d"
      },
      "source": [
        "# Model 3: \n",
        "GraphSage + Bert Embeddings with 2 Layer MLP as defined by the paper. (This is the model implemented in the paper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff9b1eff",
      "metadata": {
        "id": "ff9b1eff"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, embedding_size, batch_size, l2_reg_weight):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.batch_size = batch_size\n",
        "        self.l2_reg_weight = l2_reg_weight\n",
        "        \n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
        "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
        "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
        "        \n",
        "        self.full1 = Linear(hidden_channels[2], hidden_channels[3])\n",
        "        self.full2 = Linear(hidden_channels[3], hidden_channels[4])\n",
        "        self.softmax = Linear(hidden_channels[4], out_channels)\n",
        "\n",
        "        # Dropouts\n",
        "        self.dp1 = Dropout(0.2)\n",
        "        self.dp2 = Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        h = self.conv1(x, edge_index).relu()\n",
        "        h = self.conv2(h, edge_index).relu()\n",
        "        h = self.conv3(h, edge_index).relu()\n",
        "\n",
        "        h = global_max_pool(h, batch)\n",
        "\n",
        "        h = self.full1(h).relu()\n",
        "        h = self.dp1(h)\n",
        "        h = self.full2(h).relu()\n",
        "        h = self.dp2(h)\n",
        "        \n",
        "        h = self.softmax(h)\n",
        "\n",
        "        return torch.sigmoid(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adfc56c1",
      "metadata": {
        "id": "adfc56c1",
        "outputId": "6d6e6349-5d2c-4c32-939a-6a45eaa8e47c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net(test_data_gos.num_features,[512,512,512,256,256,256],1, 128, 128, 0.001).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "lossff = torch.nn.BCELoss()\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0839056",
      "metadata": {
        "id": "d0839056"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        all_preds.append(torch.reshape(out, (-1,)))\n",
        "        all_labels.append(data.y.float())\n",
        "    # print(all_preds)\n",
        "    accuracy, f1 = metrics(all_preds, all_labels)\n",
        "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
        "\n",
        "\n",
        "def metrics(preds, gts):\n",
        "    preds = torch.round(torch.cat(preds))\n",
        "    gts = torch.cat(gts)\n",
        "    # print(preds.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    return acc, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "920e5727",
      "metadata": {
        "id": "920e5727",
        "outputId": "f113d628-f424-429b-a2a2-69e9980a3909"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 00 |  TrainLoss: 0.69129 | TestLoss: 0.68761 | TestAcc: 0.80136 | TestF1: 0.77\n",
            "Epoch: 01 |  TrainLoss: 0.68454 | TestLoss: 0.67688 | TestAcc: 0.69420 | TestF1: 0.57\n",
            "Epoch: 02 |  TrainLoss: 0.66988 | TestLoss: 0.65339 | TestAcc: 0.77757 | TestF1: 0.72\n",
            "Epoch: 03 |  TrainLoss: 0.63928 | TestLoss: 0.61159 | TestAcc: 0.81234 | TestF1: 0.78\n",
            "Epoch: 04 |  TrainLoss: 0.59011 | TestLoss: 0.53668 | TestAcc: 0.88761 | TestF1: 0.89\n",
            "Epoch: 05 |  TrainLoss: 0.49988 | TestLoss: 0.43406 | TestAcc: 0.88369 | TestF1: 0.89\n",
            "Epoch: 06 |  TrainLoss: 0.38927 | TestLoss: 0.31651 | TestAcc: 0.89388 | TestF1: 0.90\n",
            "Epoch: 07 |  TrainLoss: 0.29827 | TestLoss: 0.25255 | TestAcc: 0.90538 | TestF1: 0.91\n",
            "Epoch: 08 |  TrainLoss: 0.23997 | TestLoss: 0.21457 | TestAcc: 0.92211 | TestF1: 0.92\n",
            "Epoch: 09 |  TrainLoss: 0.21290 | TestLoss: 0.18328 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 10 |  TrainLoss: 0.18359 | TestLoss: 0.16632 | TestAcc: 0.94407 | TestF1: 0.94\n",
            "Epoch: 11 |  TrainLoss: 0.16350 | TestLoss: 0.16125 | TestAcc: 0.95112 | TestF1: 0.95\n",
            "Epoch: 12 |  TrainLoss: 0.15862 | TestLoss: 0.15627 | TestAcc: 0.95034 | TestF1: 0.95\n",
            "Epoch: 13 |  TrainLoss: 0.15962 | TestLoss: 0.15188 | TestAcc: 0.95139 | TestF1: 0.95\n",
            "Epoch: 14 |  TrainLoss: 0.14216 | TestLoss: 0.15737 | TestAcc: 0.95217 | TestF1: 0.95\n",
            "Epoch: 15 |  TrainLoss: 0.14457 | TestLoss: 0.14937 | TestAcc: 0.95060 | TestF1: 0.95\n",
            "Epoch: 16 |  TrainLoss: 0.13789 | TestLoss: 0.14452 | TestAcc: 0.95243 | TestF1: 0.95\n",
            "Epoch: 17 |  TrainLoss: 0.13124 | TestLoss: 0.15180 | TestAcc: 0.94616 | TestF1: 0.95\n",
            "Epoch: 18 |  TrainLoss: 0.12999 | TestLoss: 0.14015 | TestAcc: 0.95426 | TestF1: 0.95\n",
            "Epoch: 19 |  TrainLoss: 0.12105 | TestLoss: 0.14290 | TestAcc: 0.95531 | TestF1: 0.96\n",
            "Epoch: 20 |  TrainLoss: 0.12320 | TestLoss: 0.14105 | TestAcc: 0.95583 | TestF1: 0.96\n",
            "Epoch: 21 |  TrainLoss: 0.11647 | TestLoss: 0.13678 | TestAcc: 0.95557 | TestF1: 0.96\n",
            "Epoch: 22 |  TrainLoss: 0.10869 | TestLoss: 0.13318 | TestAcc: 0.95583 | TestF1: 0.96\n",
            "Epoch: 23 |  TrainLoss: 0.10415 | TestLoss: 0.13529 | TestAcc: 0.95687 | TestF1: 0.96\n",
            "Epoch: 24 |  TrainLoss: 0.10092 | TestLoss: 0.13227 | TestAcc: 0.95766 | TestF1: 0.96\n",
            "Epoch: 25 |  TrainLoss: 0.09662 | TestLoss: 0.13896 | TestAcc: 0.95635 | TestF1: 0.96\n",
            "Epoch: 26 |  TrainLoss: 0.09662 | TestLoss: 0.13654 | TestAcc: 0.95766 | TestF1: 0.96\n",
            "Epoch: 27 |  TrainLoss: 0.09530 | TestLoss: 0.13116 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 28 |  TrainLoss: 0.09641 | TestLoss: 0.13888 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 29 |  TrainLoss: 0.10756 | TestLoss: 0.15016 | TestAcc: 0.95426 | TestF1: 0.96\n",
            "Epoch: 30 |  TrainLoss: 0.09514 | TestLoss: 0.12436 | TestAcc: 0.95766 | TestF1: 0.96\n",
            "Epoch: 31 |  TrainLoss: 0.07825 | TestLoss: 0.12779 | TestAcc: 0.95321 | TestF1: 0.95\n",
            "Epoch: 32 |  TrainLoss: 0.07974 | TestLoss: 0.13462 | TestAcc: 0.95139 | TestF1: 0.95\n",
            "Epoch: 33 |  TrainLoss: 0.06814 | TestLoss: 0.12416 | TestAcc: 0.95714 | TestF1: 0.96\n",
            "Epoch: 34 |  TrainLoss: 0.06622 | TestLoss: 0.12457 | TestAcc: 0.95609 | TestF1: 0.96\n",
            "Epoch: 35 |  TrainLoss: 0.06214 | TestLoss: 0.12722 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 36 |  TrainLoss: 0.06179 | TestLoss: 0.14354 | TestAcc: 0.95557 | TestF1: 0.96\n",
            "Epoch: 37 |  TrainLoss: 0.06486 | TestLoss: 0.15254 | TestAcc: 0.95295 | TestF1: 0.95\n",
            "Epoch: 38 |  TrainLoss: 0.06513 | TestLoss: 0.13857 | TestAcc: 0.95714 | TestF1: 0.96\n",
            "Epoch: 39 |  TrainLoss: 0.06204 | TestLoss: 0.12234 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 40 |  TrainLoss: 0.05544 | TestLoss: 0.14281 | TestAcc: 0.95217 | TestF1: 0.95\n",
            "Epoch: 41 |  TrainLoss: 0.05919 | TestLoss: 0.15618 | TestAcc: 0.94720 | TestF1: 0.95\n",
            "Epoch: 42 |  TrainLoss: 0.06019 | TestLoss: 0.12950 | TestAcc: 0.96027 | TestF1: 0.96\n",
            "Epoch: 43 |  TrainLoss: 0.04448 | TestLoss: 0.13017 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 44 |  TrainLoss: 0.04374 | TestLoss: 0.12471 | TestAcc: 0.95766 | TestF1: 0.96\n",
            "Epoch: 45 |  TrainLoss: 0.05000 | TestLoss: 0.12674 | TestAcc: 0.95661 | TestF1: 0.96\n",
            "Epoch: 46 |  TrainLoss: 0.04325 | TestLoss: 0.13267 | TestAcc: 0.95531 | TestF1: 0.95\n",
            "Epoch: 47 |  TrainLoss: 0.03681 | TestLoss: 0.12667 | TestAcc: 0.95687 | TestF1: 0.96\n",
            "Epoch: 48 |  TrainLoss: 0.03475 | TestLoss: 0.14054 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 49 |  TrainLoss: 0.03521 | TestLoss: 0.13411 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 50 |  TrainLoss: 0.03316 | TestLoss: 0.13006 | TestAcc: 0.95714 | TestF1: 0.96\n",
            "Epoch: 51 |  TrainLoss: 0.02630 | TestLoss: 0.13200 | TestAcc: 0.95635 | TestF1: 0.96\n",
            "Epoch: 52 |  TrainLoss: 0.02407 | TestLoss: 0.13459 | TestAcc: 0.95635 | TestF1: 0.96\n",
            "Epoch: 53 |  TrainLoss: 0.02073 | TestLoss: 0.13656 | TestAcc: 0.95714 | TestF1: 0.96\n",
            "Epoch: 54 |  TrainLoss: 0.01917 | TestLoss: 0.13741 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 55 |  TrainLoss: 0.01977 | TestLoss: 0.13978 | TestAcc: 0.95635 | TestF1: 0.96\n",
            "Epoch: 56 |  TrainLoss: 0.01581 | TestLoss: 0.15386 | TestAcc: 0.95374 | TestF1: 0.95\n",
            "Epoch: 57 |  TrainLoss: 0.01814 | TestLoss: 0.14480 | TestAcc: 0.95661 | TestF1: 0.96\n",
            "Epoch: 58 |  TrainLoss: 0.01710 | TestLoss: 0.15097 | TestAcc: 0.95661 | TestF1: 0.96\n",
            "Epoch: 59 |  TrainLoss: 0.00977 | TestLoss: 0.14905 | TestAcc: 0.95687 | TestF1: 0.96\n",
            "Epoch: 60 |  TrainLoss: 0.00832 | TestLoss: 0.14928 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 61 |  TrainLoss: 0.00757 | TestLoss: 0.15280 | TestAcc: 0.95661 | TestF1: 0.96\n",
            "Epoch: 62 |  TrainLoss: 0.00520 | TestLoss: 0.15315 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 63 |  TrainLoss: 0.00471 | TestLoss: 0.15693 | TestAcc: 0.95766 | TestF1: 0.96\n",
            "Epoch: 64 |  TrainLoss: 0.00505 | TestLoss: 0.15816 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 65 |  TrainLoss: 0.00430 | TestLoss: 0.17377 | TestAcc: 0.95583 | TestF1: 0.96\n",
            "Epoch: 66 |  TrainLoss: 0.00480 | TestLoss: 0.16242 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 67 |  TrainLoss: 0.00303 | TestLoss: 0.16586 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 68 |  TrainLoss: 0.00256 | TestLoss: 0.16685 | TestAcc: 0.95766 | TestF1: 0.96\n",
            "Epoch: 69 |  TrainLoss: 0.00265 | TestLoss: 0.16863 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 70 |  TrainLoss: 0.00216 | TestLoss: 0.17132 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 71 |  TrainLoss: 0.00180 | TestLoss: 0.17228 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 72 |  TrainLoss: 0.00152 | TestLoss: 0.17422 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 73 |  TrainLoss: 0.00150 | TestLoss: 0.17606 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 74 |  TrainLoss: 0.00152 | TestLoss: 0.17713 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 75 |  TrainLoss: 0.00151 | TestLoss: 0.17844 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 76 |  TrainLoss: 0.00135 | TestLoss: 0.17973 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 77 |  TrainLoss: 0.00160 | TestLoss: 0.18099 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 78 |  TrainLoss: 0.00134 | TestLoss: 0.18264 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 79 |  TrainLoss: 0.00113 | TestLoss: 0.18406 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 80 |  TrainLoss: 0.00099 | TestLoss: 0.18544 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 81 |  TrainLoss: 0.00099 | TestLoss: 0.18719 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 82 |  TrainLoss: 0.00095 | TestLoss: 0.18761 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 83 |  TrainLoss: 0.00087 | TestLoss: 0.19054 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 84 |  TrainLoss: 0.00081 | TestLoss: 0.19068 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 85 |  TrainLoss: 0.00085 | TestLoss: 0.19323 | TestAcc: 0.96027 | TestF1: 0.96\n",
            "Epoch: 86 |  TrainLoss: 0.00085 | TestLoss: 0.19283 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 87 |  TrainLoss: 0.00096 | TestLoss: 0.19787 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 88 |  TrainLoss: 0.00087 | TestLoss: 0.19694 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 89 |  TrainLoss: 0.00082 | TestLoss: 0.19916 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 90 |  TrainLoss: 0.00086 | TestLoss: 0.19809 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 91 |  TrainLoss: 0.00057 | TestLoss: 0.19912 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 92 |  TrainLoss: 0.00056 | TestLoss: 0.19907 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 93 |  TrainLoss: 0.00047 | TestLoss: 0.20028 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 94 |  TrainLoss: 0.00050 | TestLoss: 0.20111 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 95 |  TrainLoss: 0.00044 | TestLoss: 0.20182 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 96 |  TrainLoss: 0.00048 | TestLoss: 0.20274 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 97 |  TrainLoss: 0.00041 | TestLoss: 0.20506 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 98 |  TrainLoss: 0.00039 | TestLoss: 0.20500 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 99 |  TrainLoss: 0.00038 | TestLoss: 0.20607 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Best WLoss: 0.00003 | Best Epoch: 35\n"
          ]
        }
      ],
      "source": [
        "wloss = []\n",
        "weighted_loss = 0\n",
        "exp_param = 0.8\n",
        "best_test_loss = float('inf')  #initialize with a large value\n",
        "\n",
        "#without dropout training results\n",
        "for epoch in range(100):\n",
        "  train_loss = train(epoch)\n",
        "  test_loss, test_acc, test_f1 = test(epoch)\n",
        "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
        "  \n",
        "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
        "  \n",
        "  if test_loss < best_test_loss:\n",
        "    best_test_loss = test_loss  #update the best test loss\n",
        "\n",
        "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
        "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
        "\n",
        "#print the best values\n",
        "best_wloss = min(wloss)\n",
        "best_epoch = wloss.index(best_wloss)\n",
        "print(f'Best WLoss: {best_wloss:.5f} | Best Epoch: {best_epoch}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b2230f",
      "metadata": {
        "id": "19b2230f"
      },
      "source": [
        "# Code 4: \n",
        "GraphSage + Bert Embedding with hyperparameters as defined in the paper and replacemenent of 1 MLP layer with 1 RNN Layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "320093fd",
      "metadata": {
        "id": "320093fd"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, embedding_size, batch_size, l2_reg_weight):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.batch_size = batch_size\n",
        "        self.l2_reg_weight = l2_reg_weight\n",
        "        \n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
        "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
        "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
        "        \n",
        "        self.rnn = nn.RNN(embedding_size, hidden_channels[3], batch_first=True)\n",
        "        self.full1 = nn.Linear(hidden_channels[3], hidden_channels[4])\n",
        "        self.softmax = nn.Linear(hidden_channels[4], out_channels)\n",
        "\n",
        "        # Dropouts\n",
        "        self.dp1 = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        h = self.conv1(x, edge_index).relu()\n",
        "        h = self.conv2(h, edge_index).relu()\n",
        "        h = self.conv3(h, edge_index).relu()\n",
        "\n",
        "        h = global_max_pool(h, batch)\n",
        "        \n",
        "        # Reshape the input tensor for RNN\n",
        "        h = h.unsqueeze(0)  # Add a time dimension\n",
        "        h = self.dp1(h)\n",
        "        \n",
        "        # Apply RNN\n",
        "        h, _ = self.rnn(h)\n",
        "        h = h.squeeze(0)  # Remove the time dimension\n",
        "        \n",
        "        h = self.full1(h).relu()\n",
        "        h = self.softmax(h)\n",
        "\n",
        "        return torch.sigmoid(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ad01364",
      "metadata": {
        "id": "2ad01364"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        all_preds.append(torch.reshape(out, (-1,)))\n",
        "        all_labels.append(data.y.float())\n",
        "    # print(all_preds)\n",
        "    accuracy, f1 = metrics(all_preds, all_labels)\n",
        "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
        "\n",
        "\n",
        "def metrics(preds, gts):\n",
        "    preds = torch.round(torch.cat(preds))\n",
        "    gts = torch.cat(gts)\n",
        "    # print(preds.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    return acc, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc94d9b0",
      "metadata": {
        "id": "dc94d9b0",
        "outputId": "21fbd36d-7676-4968-ff82-7650eb5180e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 00 |  TrainLoss: 0.00033 | TestLoss: 0.20748 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 01 |  TrainLoss: 0.00034 | TestLoss: 0.20776 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 02 |  TrainLoss: 0.00035 | TestLoss: 0.20856 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 03 |  TrainLoss: 0.00031 | TestLoss: 0.21071 | TestAcc: 0.95975 | TestF1: 0.96\n",
            "Epoch: 04 |  TrainLoss: 0.00034 | TestLoss: 0.21048 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 05 |  TrainLoss: 0.00042 | TestLoss: 0.21112 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 06 |  TrainLoss: 0.00034 | TestLoss: 0.21320 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 07 |  TrainLoss: 0.00027 | TestLoss: 0.21239 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 08 |  TrainLoss: 0.00027 | TestLoss: 0.21396 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 09 |  TrainLoss: 0.00029 | TestLoss: 0.21412 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 10 |  TrainLoss: 0.00031 | TestLoss: 0.21541 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 11 |  TrainLoss: 0.00025 | TestLoss: 0.21589 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 12 |  TrainLoss: 0.00026 | TestLoss: 0.21717 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 13 |  TrainLoss: 0.00025 | TestLoss: 0.21741 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 14 |  TrainLoss: 0.00024 | TestLoss: 0.21784 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 15 |  TrainLoss: 0.00022 | TestLoss: 0.21913 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 16 |  TrainLoss: 0.00027 | TestLoss: 0.21938 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 17 |  TrainLoss: 0.00021 | TestLoss: 0.22071 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 18 |  TrainLoss: 0.00020 | TestLoss: 0.22085 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 19 |  TrainLoss: 0.00021 | TestLoss: 0.22162 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 20 |  TrainLoss: 0.00021 | TestLoss: 0.22258 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 21 |  TrainLoss: 0.00020 | TestLoss: 0.22298 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 22 |  TrainLoss: 0.00020 | TestLoss: 0.22384 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 23 |  TrainLoss: 0.00025 | TestLoss: 0.22455 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 24 |  TrainLoss: 0.00019 | TestLoss: 0.22497 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 25 |  TrainLoss: 0.00017 | TestLoss: 0.22675 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 26 |  TrainLoss: 0.00021 | TestLoss: 0.22639 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 27 |  TrainLoss: 0.00014 | TestLoss: 0.22701 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 28 |  TrainLoss: 0.00017 | TestLoss: 0.22785 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 29 |  TrainLoss: 0.00017 | TestLoss: 0.22810 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 30 |  TrainLoss: 0.00015 | TestLoss: 0.22865 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 31 |  TrainLoss: 0.00014 | TestLoss: 0.22993 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 32 |  TrainLoss: 0.00015 | TestLoss: 0.23009 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 33 |  TrainLoss: 0.00012 | TestLoss: 0.23060 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 34 |  TrainLoss: 0.00012 | TestLoss: 0.23145 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 35 |  TrainLoss: 0.00012 | TestLoss: 0.23201 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 36 |  TrainLoss: 0.00011 | TestLoss: 0.23240 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 37 |  TrainLoss: 0.00014 | TestLoss: 0.23305 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 38 |  TrainLoss: 0.00011 | TestLoss: 0.23422 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 39 |  TrainLoss: 0.00012 | TestLoss: 0.23414 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 40 |  TrainLoss: 0.00013 | TestLoss: 0.23446 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 41 |  TrainLoss: 0.00012 | TestLoss: 0.23513 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 42 |  TrainLoss: 0.00010 | TestLoss: 0.23600 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 43 |  TrainLoss: 0.00011 | TestLoss: 0.23647 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 44 |  TrainLoss: 0.00011 | TestLoss: 0.23695 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 45 |  TrainLoss: 0.00012 | TestLoss: 0.23765 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 46 |  TrainLoss: 0.00011 | TestLoss: 0.23765 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 47 |  TrainLoss: 0.00010 | TestLoss: 0.23816 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 48 |  TrainLoss: 0.00011 | TestLoss: 0.23905 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 49 |  TrainLoss: 0.00011 | TestLoss: 0.23910 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 50 |  TrainLoss: 0.00009 | TestLoss: 0.23964 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 51 |  TrainLoss: 0.00010 | TestLoss: 0.24062 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 52 |  TrainLoss: 0.00009 | TestLoss: 0.24080 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 53 |  TrainLoss: 0.00008 | TestLoss: 0.24131 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 54 |  TrainLoss: 0.00009 | TestLoss: 0.24189 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 55 |  TrainLoss: 0.00008 | TestLoss: 0.24232 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 56 |  TrainLoss: 0.00008 | TestLoss: 0.24283 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 57 |  TrainLoss: 0.00007 | TestLoss: 0.24333 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 58 |  TrainLoss: 0.00007 | TestLoss: 0.24386 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 59 |  TrainLoss: 0.00008 | TestLoss: 0.24446 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 60 |  TrainLoss: 0.00008 | TestLoss: 0.24474 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 61 |  TrainLoss: 0.00007 | TestLoss: 0.24509 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 62 |  TrainLoss: 0.00007 | TestLoss: 0.24546 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 63 |  TrainLoss: 0.00007 | TestLoss: 0.24632 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 64 |  TrainLoss: 0.00008 | TestLoss: 0.24644 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 65 |  TrainLoss: 0.00008 | TestLoss: 0.24695 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 66 |  TrainLoss: 0.00007 | TestLoss: 0.24757 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 67 |  TrainLoss: 0.00006 | TestLoss: 0.24817 | TestAcc: 0.95766 | TestF1: 0.96\n",
            "Epoch: 68 |  TrainLoss: 0.00007 | TestLoss: 0.24861 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 69 |  TrainLoss: 0.00007 | TestLoss: 0.24928 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 70 |  TrainLoss: 0.00005 | TestLoss: 0.24962 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 71 |  TrainLoss: 0.00006 | TestLoss: 0.25003 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 72 |  TrainLoss: 0.00006 | TestLoss: 0.25048 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 73 |  TrainLoss: 0.00006 | TestLoss: 0.25141 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 74 |  TrainLoss: 0.00005 | TestLoss: 0.25143 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 75 |  TrainLoss: 0.00005 | TestLoss: 0.25180 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 76 |  TrainLoss: 0.00008 | TestLoss: 0.25273 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 77 |  TrainLoss: 0.00005 | TestLoss: 0.25262 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 78 |  TrainLoss: 0.00006 | TestLoss: 0.25322 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 79 |  TrainLoss: 0.00007 | TestLoss: 0.25362 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 80 |  TrainLoss: 0.00006 | TestLoss: 0.25393 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 81 |  TrainLoss: 0.00005 | TestLoss: 0.25448 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 82 |  TrainLoss: 0.00005 | TestLoss: 0.25478 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 83 |  TrainLoss: 0.00005 | TestLoss: 0.25520 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 84 |  TrainLoss: 0.00005 | TestLoss: 0.25561 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 85 |  TrainLoss: 0.00005 | TestLoss: 0.25586 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 86 |  TrainLoss: 0.00005 | TestLoss: 0.25645 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 87 |  TrainLoss: 0.00004 | TestLoss: 0.25678 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 88 |  TrainLoss: 0.00004 | TestLoss: 0.25742 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 89 |  TrainLoss: 0.00005 | TestLoss: 0.25778 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 90 |  TrainLoss: 0.00005 | TestLoss: 0.25773 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 91 |  TrainLoss: 0.00005 | TestLoss: 0.25815 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 92 |  TrainLoss: 0.00005 | TestLoss: 0.25841 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 93 |  TrainLoss: 0.00005 | TestLoss: 0.25873 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 94 |  TrainLoss: 0.00004 | TestLoss: 0.25976 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 95 |  TrainLoss: 0.00005 | TestLoss: 0.25955 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 96 |  TrainLoss: 0.00004 | TestLoss: 0.26004 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 97 |  TrainLoss: 0.00004 | TestLoss: 0.26062 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 98 |  TrainLoss: 0.00005 | TestLoss: 0.26100 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 99 |  TrainLoss: 0.00004 | TestLoss: 0.26148 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Best WLoss: 0.00005 | Best Epoch: 0\n"
          ]
        }
      ],
      "source": [
        "wloss = []\n",
        "weighted_loss = 0\n",
        "exp_param = 0.8\n",
        "best_test_loss = float('inf')  # Initialize with a large value\n",
        "\n",
        "# Without dropout training results\n",
        "for epoch in range(100):\n",
        "  train_loss = train(epoch)\n",
        "  test_loss, test_acc, test_f1 = test(epoch)\n",
        "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
        "  \n",
        "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
        "  \n",
        "  if test_loss < best_test_loss:\n",
        "    best_test_loss = test_loss  # Update the best test loss\n",
        "\n",
        "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
        "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
        "\n",
        "# Print the best values\n",
        "best_wloss = min(wloss)\n",
        "best_epoch = wloss.index(best_wloss)\n",
        "print(f'Best WLoss: {best_wloss:.5f} | Best Epoch: {best_epoch}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f820ec1",
      "metadata": {
        "id": "4f820ec1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}